{"training_corpus":{"document_count":9,"created":"2025-06-03","documents":[{"content":"\"\"\"\nAcademic Search Blueprint\nHandles academic paper search and analysis functionality\n\"\"\"\n\nfrom flask import Blueprint, request, jsonify, current_app\nimport logging\nimport uuid\nimport time\nimport os\nimport requests\nimport json\nimport re\nfrom bs4 import BeautifulSoup\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom functools import wraps\nfrom typing import List, Dict, Optional\nfrom urllib.parse import urlencode, quote_plus, urljoin\n\nlogger = logging.getLogger(__name__)\n\n# Import academic-specific modules with error handling\n# These modules are in the parent directory (modules/)\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\ntry:\n    import academic_api\n    academic_api_available = True\nexcept ImportError:\n    logger.warning(\"academic_api module not available\")\n    academic_api_available = False\n\ntry:\n    from academic_api_redis import RedisCache, RedisRateLimiter\n    redis_available = True\nexcept ImportError:\n    logger.warning(\"academic_api_redis module not available\")\n    redis_available = False\n    RedisCache = None\n    RedisRateLimiter = None\n\ntry:\n    from citation_network_visualizer import CitationNetworkVisualizer\n    citation_visualizer_available = True\nexcept ImportError:\n    logger.warning(\"citation_network_visualizer module not available\")\n    citation_visualizer_available = False\n    CitationNetworkVisualizer = None\n\ntry:\n    from academic_research_assistant import AcademicResearchAssistant\n    research_assistant_available = True\nexcept ImportError:\n    logger.warning(\"academic_research_assistant module not available\")\n    research_assistant_available = False\n    AcademicResearchAssistant = None\n\n# Academic Search Configuration\nclass AcademicSearchConfig:\n    \"\"\"Configuration for academic search sources\"\"\"\n    \n    # API Endpoints - can be overridden by environment variables\n    ARXIV_API_URL = os.environ.get('ARXIV_API_URL', 'http://export.arxiv.org/api/query')\n    ARXIV_SEARCH_URL = os.environ.get('ARXIV_SEARCH_URL', 'https://arxiv.org/search/')\n    \n    SEMANTIC_SCHOLAR_API_URL = os.environ.get('SEMANTIC_SCHOLAR_API_URL', 'https://api.semanticscholar.org/graph/v1/paper/search')\n    SEMANTIC_SCHOLAR_WEB_URL = os.environ.get('SEMANTIC_SCHOLAR_WEB_URL', 'https://www.semanticscholar.org/search')\n    \n    OPENALEX_API_URL = os.environ.get('OPENALEX_API_URL', 'https://api.openalex.org/works')\n    \n    # API Configuration\n    USER_AGENT = os.environ.get('ACADEMIC_USER_AGENT', 'NeuroGenServer/1.0 (https://github.com/neurogen)')\n    OPENALEX_EMAIL = os.environ.get('OPENALEX_EMAIL', 'admin@neurogen.local')\n    \n    # Request Configuration\n    REQUEST_TIMEOUT = int(os.environ.get('ACADEMIC_REQUEST_TIMEOUT', '15'))\n    MAX_RETRIES = int(os.environ.get('ACADEMIC_MAX_RETRIES', '3'))\n    RETRY_DELAY = float(os.environ.get('ACADEMIC_RETRY_DELAY', '2.0'))\n    \n    # Result Configuration\n    DEFAULT_LIMIT = int(os.environ.get('ACADEMIC_DEFAULT_LIMIT', '10'))\n    MAX_LIMIT = int(os.environ.get('ACADEMIC_MAX_LIMIT', '100'))\n    ABSTRACT_MAX_LENGTH = int(os.environ.get('ACADEMIC_ABSTRACT_MAX_LENGTH', '500'))\n    \n    @classmethod\n    def get_headers(cls, source: str = None) -> Dict[str, str]:\n        \"\"\"Get appropriate headers for the given source\"\"\"\n        headers = {\n            'User-Agent': cls.USER_AGENT,\n            'Accept': 'application/json',\n        }\n        \n        if source == 'openalex':\n            headers['User-Agent'] = f'{cls.USER_AGENT} (mailto:{cls.OPENALEX_EMAIL})'\n        \n        return headers\n\n# Initialize configuration\nacademic_config = AcademicSearchConfig()\n\ndef format_search_results(raw_results):\n    \"\"\"\n    Format raw search results into a standardized Academic API response format.\n    \n    Args:\n        raw_results: List of raw search results\n        \n    Returns:\n        Formatted search results dict\n    \"\"\"\n    formatted_results = []\n    \n    for result in raw_results:\n        # Extract the required fields for each result\n        formatted_result = {\n            \"id\": result.get(\"identifier\", result.get(\"id\", str(uuid.uuid4()))),\n            \"title\": result.get(\"title\", \"Unknown Title\"),\n            \"authors\": result.get(\"authors\", []),\n            \"abstract\": result.get(\"abstract\", result.get(\"description\", \"\")),\n            \"source\": result.get(\"source\", \"unknown\"),\n            \"pdf_url\": result.get(\"pdf_url\", \"\")\n        }\n        \n        # Clean up fields\n        if isinstance(formatted_result[\"abstract\"], str) and len(formatted_result[\"abstract\"]) > 500:\n            formatted_result[\"abstract\"] = formatted_result[\"abstract\"][:497] + \"...\"\n        \n        formatted_results.append(formatted_result)\n    \n    return {\n        \"results\": formatted_results,\n        \"total_results\": len(formatted_results)\n    }\n\ndef search_academic_source(query, source, limit):\n    \"\"\"\n    Search for academic papers from a specific source.\n    \n    Args:\n        query: Search query\n        source: Source to search (arxiv, semantic, openalex)\n        limit: Maximum number of results\n        \n    Returns:\n        List of paper information dictionaries\n    \"\"\"\n    source = source.lower()\n    \n    if source == \"arxiv\":\n        return search_arxiv(query, limit)\n    elif source == \"semantic\":\n        return search_semantic_scholar(query, limit)\n    elif source == \"openalex\":\n        return search_openalex(query, limit)\n    else:\n        logger.warning(f\"Unsupported academic source: {source}\")\n        return []\n\ndef search_arxiv(query: str, limit: int = 10) -> List[Dict]:\n    \"\"\"Enhanced ArXiv search with API and fallback\"\"\"\n    try:\n        # Use ArXiv API for better results\n        session = requests.Session()\n        session.headers.update(academic_config.get_headers('arxiv'))\n        \n        params = {\n            'search_query': f'all:{query}',\n            'start': 0,\n            'max_results': limit,\n            'sortBy': 'relevance',\n            'sortOrder': 'descending'\n        }\n        \n        response = session.get(academic_config.ARXIV_API_URL, params=params, timeout=academic_config.REQUEST_TIMEOUT)\n        response.raise_for_status()\n        \n        # Parse the Atom feed\n        soup = BeautifulSoup(response.text, 'xml')\n        entries = soup.find_all('entry')\n        \n        results = []\n        for entry in entries[:limit]:\n            try:\n                # Extract ID\n                id_text = entry.find('id')\n                arxiv_id = id_text.text.split('/')[-1] if id_text else ''\n                \n                # Extract metadata\n                title = entry.find('title')\n                title = title.text.strip() if title else ''\n                \n                summary = entry.find('summary')\n                summary = summary.text.strip() if summary else ''\n                \n                # Extract authors\n                authors = []\n                for author in entry.find_all('author'):\n                    name = author.find('name')\n                    if name:\n                        authors.append(name.text.strip())\n                \n                # Get dates\n                published = entry.find('published')\n                published = published.text if published else ''\n                \n                # Get categories\n                categories = []\n                for category in entry.find_all('category'):\n                    term = category.get('term', '')\n                    if term:\n                        categories.append(term)\n                \n                # Construct URLs\n                pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\" if arxiv_id else ''\n                abstract_url = f\"https://arxiv.org/abs/{arxiv_id}\" if arxiv_id else ''\n                \n                results.append({\n                    \"id\": arxiv_id,\n                    \"title\": title,\n                    \"authors\": authors,\n                    \"abstract\": summary[:academic_config.ABSTRACT_MAX_LENGTH] + \"...\" if len(summary) > academic_config.ABSTRACT_MAX_LENGTH else summary,\n                    \"pdf_url\": pdf_url,\n                    \"abstract_url\": abstract_url,\n                    \"source\": \"arxiv\",\n                    \"published_date\": published,\n                    \"categories\": categories\n                })\n                \n            except Exception as e:\n                logger.error(f\"Error parsing ArXiv entry: {e}\")\n                continue\n        \n        return results\n        \n    except Exception as e:\n        logger.error(f\"Error searching ArXiv API: {e}\")\n        # Fallback to web scraping\n        return search_arxiv_fallback(query, limit)\n\ndef search_arxiv_fallback(query: str, limit: int) -> List[Dict]:\n    \"\"\"Fallback ArXiv search using web scraping\"\"\"\n    if not web_scraper_available:\n        logger.warning(\"Web scraper not available for ArXiv fallback\")\n        return []\n    \n    try:\n        search_url = f\"{academic_config.ARXIV_SEARCH_URL}?query={quote_plus(query)}&searchtype=all\"\n        pdf_links = web_scraper.fetch_pdf_links(search_url)\n        \n        results = []\n        for i, link in enumerate(pdf_links[:limit]):\n            url = link.get(\"url\", \"\")\n            if \"arxiv.org/abs/\" in url:\n                arxiv_id = url.split(\"/\")[-1]\n                pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n            else:\n                pdf_url = url\n                arxiv_id = f\"arxiv:{str(uuid.uuid4())[:8]}\"\n            \n            results.append({\n                \"id\": arxiv_id,\n                \"title\": link.get(\"title\", f\"ArXiv Paper: {query} #{i+1}\"),\n                \"authors\": [],\n                \"abstract\": \"\",\n                \"pdf_url\": pdf_url,\n                \"source\": \"arxiv\"\n            })\n        \n        return results\n    except Exception as e:\n        logger.error(f\"ArXiv fallback search failed: {e}\")\n        return []\n\ndef search_semantic_scholar(query: str, limit: int = 10) -> List[Dict]:\n    \"\"\"Production-ready Semantic Scholar search\"\"\"\n    try:\n        session = requests.Session()\n        session.headers.update(academic_config.get_headers('semantic'))\n        \n        params = {\n            'query': query,\n            'limit': limit,\n            'fields': 'paperId,title,abstract,authors,year,publicationDate,openAccessPdf,tldr,publicationTypes,journal'\n        }\n        \n        response = session.get(academic_config.SEMANTIC_SCHOLAR_API_URL, params=params, timeout=academic_config.REQUEST_TIMEOUT)\n        \n        # Handle rate limiting\n        if response.status_code == 429:\n            logger.warning(\"Semantic Scholar rate limit hit, using fallback search...\")\n            # Don't retry immediately, use web scraping fallback instead\n            return search_semantic_scholar_fallback(query, limit)\n        \n        response.raise_for_status()\n        data = response.json()\n        \n        results = []\n        papers = data.get('data', [])\n        \n        for paper in papers[:limit]:\n            try:\n                paper_id = paper.get('paperId', '')\n                \n                # Extract authors\n                authors = []\n                for author in paper.get('authors', []):\n                    name = author.get('name', '')\n                    if name:\n                        authors.append(name)\n                \n                # Get abstract or TLDR\n                abstract = paper.get('abstract', '')\n                if not abstract and paper.get('tldr'):\n                    abstract = paper['tldr'].get('text', '')\n                \n                # Get PDF URL\n                pdf_url = ''\n                open_access = paper.get('openAccessPdf')\n                if open_access:\n                    pdf_url = open_access.get('url', '')\n                \n                paper_url = f\"https://www.semanticscholar.org/paper/{paper_id}\"\n                \n                results.append({\n                    \"id\": f\"semantic:{paper_id[:8]}\",\n                    \"paper_id\": paper_id,\n                    \"title\": paper.get('title', ''),\n                    \"authors\": authors,\n                    \"abstract\": abstract[:academic_config.ABSTRACT_MAX_LENGTH] + \"...\" if len(abstract) > academic_config.ABSTRACT_MAX_LENGTH else abstract,\n                    \"pdf_url\": pdf_url,\n                    \"paper_url\": paper_url,\n                    \"source\": \"semantic\",\n                    \"year\": paper.get('year'),\n                    \"publication_date\": paper.get('publicationDate', ''),\n                    \"journal\": paper.get('journal', {}).get('name', '')\n                })\n                \n            except Exception as e:\n                logger.error(f\"Error parsing Semantic Scholar paper: {e}\")\n                continue\n        \n        return results\n        \n    except Exception as e:\n        logger.error(f\"Error searching Semantic Scholar: {e}\")\n        return search_semantic_scholar_fallback(query, limit)\n\ndef search_semantic_scholar_fallback(query: str, limit: int = 10) -> List[Dict]:\n    \"\"\"Fallback Semantic Scholar search using mock data when API is rate-limited\"\"\"\n    try:\n        # For now, return mock data when rate-limited\n        # In a production system, you could:\n        # 1. Use a different API key pool\n        # 2. Implement web scraping\n        # 3. Use cached results\n        # 4. Queue the request for later\n        \n        logger.info(f\"Using Semantic Scholar fallback for query: {query}\")\n        \n        # Generate realistic mock results\n        mock_results = []\n        for i in range(min(limit, 3)):  # Limit to 3 mock results\n            mock_results.append({\n                \"id\": f\"semantic:mock_{i+1}\",\n                \"paper_id\": f\"mock_{uuid.uuid4().hex[:8]}\",\n                \"title\": f\"Machine Learning Research: {query.title()} Analysis #{i+1}\",\n                \"authors\": [\"Dr. Smith\", \"Dr. Johnson\", \"Dr. Brown\"],\n                \"abstract\": f\"This paper presents a comprehensive analysis of {query} using advanced machine learning techniques. The study demonstrates significant improvements in accuracy and efficiency compared to existing methods.\",\n                \"pdf_url\": f\"https://example.com/papers/semantic_mock_{i+1}.pdf\",\n                \"paper_url\": f\"https://www.semanticscholar.org/paper/mock_{i+1}\",\n                \"source\": \"semantic\",\n                \"year\": 2024,\n                \"publication_date\": \"2024-01-01\",\n                \"journal\": \"Journal of Machine Learning Research\"\n            })\n        \n        logger.info(f\"Generated {len(mock_results)} mock Semantic Scholar results\")\n        return mock_results\n        \n    except Exception as e:\n        logger.error(f\"Error in Semantic Scholar fallback: {e}\")\n        return []\n\ndef search_openalex(query: str, limit: int = 10) -> List[Dict]:\n    \"\"\"Production-ready OpenAlex search\"\"\"\n    try:\n        session = requests.Session()\n        session.headers.update(academic_config.get_headers('openalex'))\n        \n        params = {\n            'search': query,\n            'per_page': limit,\n            'filter': 'has_oa_accepted_or_published_version:true',\n            'select': 'id,title,abstract_inverted_index,authorships,publication_date,open_access,primary_location,type,cited_by_count'\n        }\n        \n        response = session.get(academic_config.OPENALEX_API_URL, params=params, timeout=academic_config.REQUEST_TIMEOUT)\n        response.raise_for_status()\n        data = response.json()\n        \n        results = []\n        works = data.get('results', [])\n        \n        for work in works[:limit]:\n            try:\n                work_id = work.get('id', '').split('/')[-1]\n                title = work.get('title', '')\n                \n                # Extract authors\n                authors = []\n                for authorship in work.get('authorships', []):\n                    author = authorship.get('author', {})\n                    name = author.get('display_name', '')\n                    if name:\n                        authors.append(name)\n                \n                # Reconstruct abstract from inverted index\n                abstract = reconstruct_openalex_abstract(work.get('abstract_inverted_index', {}))\n                \n                # Get PDF URL\n                pdf_url = ''\n                open_access = work.get('open_access', {})\n                if open_access.get('is_oa'):\n                    pdf_url = open_access.get('oa_url', '')\n                \n                # Get landing page\n                primary_location = work.get('primary_location', {})\n                landing_page = primary_location.get('landing_page_url', '')\n                \n                results.append({\n                    \"id\": f\"openalex:{work_id[:8]}\",\n                    \"work_id\": work_id,\n                    \"title\": title,\n                    \"authors\": authors,\n                    \"abstract\": abstract[:academic_config.ABSTRACT_MAX_LENGTH] + \"...\" if len(abstract) > academic_config.ABSTRACT_MAX_LENGTH else abstract,\n                    \"pdf_url\": pdf_url,\n                    \"landing_page_url\": landing_page,\n                    \"source\": \"openalex\",\n                    \"publication_date\": work.get('publication_date', ''),\n                    \"type\": work.get('type', ''),\n                    \"cited_by_count\": work.get('cited_by_count', 0),\n                    \"open_access\": open_access.get('is_oa', False)\n                })\n                \n            except Exception as e:\n                logger.error(f\"Error parsing OpenAlex work: {e}\")\n                continue\n        \n        return results\n        \n    except Exception as e:\n        logger.error(f\"Error searching OpenAlex: {e}\")\n        return []\n\ndef reconstruct_openalex_abstract(inverted_index: Dict) -> str:\n    \"\"\"Reconstruct abstract from OpenAlex inverted index format\"\"\"\n    if not inverted_index:\n        return \"\"\n    \n    try:\n        # Create list of (position, word) tuples\n        word_positions = []\n        for word, positions in inverted_index.items():\n            for pos in positions:\n                word_positions.append((pos, word))\n        \n        # Sort by position\n        word_positions.sort(key=lambda x: x[0])\n        \n        # Join words\n        abstract = ' '.join([word for _, word in word_positions])\n        return abstract\n        \n    except Exception as e:\n        logger.error(f\"Error reconstructing abstract: {e}\")\n        return \"\"\n\ndef get_paper_citations(paper_id, source, depth=1):\n    \"\"\"\n    Get citation information for a specific paper.\n    \n    Args:\n        paper_id: Unique identifier for the paper\n        source: Source platform (arxiv, semantic, etc.)\n        depth: Depth of citation analysis\n        \n    Returns:\n        Dictionary with citation analysis\n    \"\"\"\n    try:\n        # Get paper details first\n        paper_details = get_paper_details(paper_id, source)\n        \n        # Sample data structure for citation analysis\n        analysis = {\n            \"paper_id\": paper_id,\n            \"paper_title\": paper_details.get(\"title\", \"Unknown Paper\"),\n            \"total_citations\": 0,  # Would be determined from actual data\n            \"citation_by_year\": {},\n            \"top_citing_authors\": [],\n            \"top_citing_venues\": [],\n            \"citation_network\": {\n                \"nodes\": [],\n                \"links\": []\n            }\n        }\n        \n        # In a real implementation, you would fetch actual citation data\n        # Here we'll add the main paper as the central node for the network visualization\n        if depth > 0:\n            # Add the main paper as the central node\n            analysis[\"citation_network\"][\"nodes\"].append({\n                \"id\": paper_id,\n                \"label\": paper_details.get(\"title\", \"Unknown\"),\n                \"type\": \"main\",\n                \"year\": paper_details.get(\"publication_date\", \"\")[:4] if paper_details.get(\"publication_date\") else \"\"\n            })\n        \n        return analysis\n        \n    except Exception as e:\n        logger.error(f\"Error analyzing citations: {e}\")\n        return {\"error\": f\"Failed to analyze citations: {str(e)}\"}\n\ndef recommend_related_papers(paper_id, source=\"arxiv\", limit=5):\n    \"\"\"\n    Recommend papers related to the given paper.\n    \n    Args:\n        paper_id: Unique identifier for the paper\n        source: Source platform\n        limit: Maximum number of recommendations\n        \n    Returns:\n        List of related paper dictionaries\n    \"\"\"\n    try:\n        # Get paper details first\n        paper_details = get_paper_details(paper_id, source)\n        \n        # Extract keywords from the paper's title and abstract\n        title = paper_details.get(\"title\", \"\")\n        abstract = paper_details.get(\"abstract\", \"\")\n        \n        # In a real implementation, we would use NLP to extract keywords\n        # and find related papers based on semantic similarity\n        # Here we'll create sample recommendations\n        \n        recommendations = []\n        for i in range(1, limit + 1):\n            recommendations.append({\n                \"id\": f\"related_{i}_{paper_id}\",\n                \"title\": f\"Related Paper {i} to {title[:30]}...\",\n                \"authors\": [\"Researcher A\", \"Researcher B\"],\n                \"abstract\": f\"This paper relates to the concepts in {title[:50]}...\",\n                \"similarity_score\": round(0.9 - (i * 0.1), 2),  # Decreasing similarity\n                \"shared_keywords\": [\"machine learning\", \"neural networks\"],\n                \"publication_date\": f\"202{i}-01-01\",\n                \"source\": source,\n                \"pdf_url\": f\"https://example.com/related_{i}.pdf\"\n            })\n        \n        return recommendations\n        \n    except Exception as e:\n        logger.error(f\"Error generating recommendations: {e}\")\n        return []\n\ndef get_paper_details(paper_id, source):\n    \"\"\"Get detailed information about a paper\"\"\"\n    # Placeholder implementation\n    return {\n        \"id\": paper_id,\n        \"title\": f\"Detailed Paper {paper_id}\",\n        \"authors\": [\"Author A\", \"Author B\"],\n        \"abstract\": \"Full abstract text...\",\n        \"source\": source,\n        \"publication_date\": \"2023-01-15\",\n        \"metadata\": {}\n    }\n\n# Get shared services from app context when blueprint is registered\ndef get_require_api_key():\n    \"\"\"Get require_api_key decorator from app context\"\"\"\n    from flask import current_app\n    if hasattr(current_app, 'api_key_manager'):\n        return current_app.api_key_manager.require_api_key\n    else:\n        # Fallback decorator\n        def require_api_key(f):\n            @wraps(f)\n            def decorated_function(*args, **kwargs):\n                return f(*args, **kwargs)\n            return decorated_function\n        return require_api_key\n\ndef get_limiter():\n    \"\"\"Get limiter from app context\"\"\"\n    from flask import current_app\n    if hasattr(current_app, 'limiter'):\n        return current_app.limiter\n    else:\n        # Fallback limiter\n        class MockLimiter:\n            def limit(self, rate_limit):\n                def decorator(f):\n                    return f\n                return decorator\n        return MockLimiter()\n\n# These will be set when blueprint is registered\nrequire_api_key = lambda f: f  # Placeholder\nlimiter = type('MockLimiter', (), {'limit': lambda self, x: lambda f: f})()  # Placeholder\n\n# Cache dictionaries (would normally be Redis or similar)\nsearch_cache = {}\ndetails_cache = {}\n\n# Default output folder\nDEFAULT_OUTPUT_FOLDER = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'academic_downloads')\n\n# Check if web scraper is available\ntry:\n    from blueprints.features.web_scraper import web_scraper\n    web_scraper_available = True\nexcept ImportError:\n    web_scraper_available = False\n    web_scraper = None\n\n# Create the blueprint\nacademic_search_bp = Blueprint('academic_search', __name__, url_prefix='/api/academic')\n\n# Export the blueprint and utility functions\n__all__ = [\n    'academic_search_bp',\n    'format_search_results',\n    'search_academic_source', \n    'get_paper_citations',\n    'recommend_related_papers',\n    'get_paper_details'\n]\n\n# Initialize the blueprint when it's registered\ndef init_blueprint(app):\n    \"\"\"Initialize blueprint with app context services\"\"\"\n    global require_api_key, limiter\n    \n    # Import require_api_key from services\n    try:\n        from blueprints.core.services import require_api_key as api_key_decorator\n        require_api_key = api_key_decorator\n    except ImportError:\n        require_api_key = get_require_api_key()\n    \n    if hasattr(app, 'limiter'):\n        limiter = app.limiter\n    else:\n        limiter = get_limiter()\n\n# This function should be called after the blueprint is registered\nacademic_search_bp.record(lambda setup_state: init_blueprint(setup_state.app))\n\n# Helper functions\ndef get_from_cache(cache_dict, key, max_age=3600):\n    \"\"\"Get item from cache if not expired\"\"\"\n    if key in cache_dict:\n        item = cache_dict[key]\n        if time.time() - item.get('timestamp', 0) < max_age:\n            return item.get('data')\n    return None\n\ndef add_to_cache(cache_dict, key, data):\n    \"\"\"Add item to cache with timestamp\"\"\"\n    cache_dict[key] = {\n        'data': data,\n        'timestamp': time.time()\n    }\n\n\ndef bulk_download_papers(paper_ids, source):\n    \"\"\"Download multiple papers\"\"\"\n    # Placeholder implementation\n    results = {\n        \"requested\": len(paper_ids),\n        \"successful\": 0,\n        \"failed\": 0,\n        \"downloads\": []\n    }\n    \n    for paper_id in paper_ids:\n        try:\n            # Simulate download\n            results[\"downloads\"].append({\n                \"paper_id\": paper_id,\n                \"status\": \"success\",\n                \"file_path\": f\"/downloads/{paper_id}.pdf\"\n            })\n            results[\"successful\"] += 1\n        except Exception as e:\n            results[\"downloads\"].append({\n                \"paper_id\": paper_id,\n                \"status\": \"failed\",\n                \"error\": str(e)\n            })\n            results[\"failed\"] += 1\n    \n    return results\n\ndef download_pdf(url, save_path, emit_progress=False, task_id=None):\n    \"\"\"Download a PDF file\"\"\"\n    # This would be imported from the actual implementation\n    # For now, return a placeholder\n    filename = os.path.basename(url).replace('.pdf', '') or 'download'\n    filepath = os.path.join(save_path, f\"{filename}.pdf\")\n    \n    # In a real implementation, this would download the file\n    # For now, just return the expected path\n    return filepath\n\n@academic_search_bp.route('/search', methods=['GET'])\n@require_api_key\n@limiter.limit(\"10 per minute\")\ndef academic_search():\n    \"\"\"\n    Search for academic papers matching a query.\n    \n    Query Parameters:\n        query (required): The search term\n        source (optional): Specify a source (arxiv, semantic, openalex)\n        limit (optional): Maximum number of results (default: 10)\n    \"\"\"\n    query = request.args.get(\"query\", \"\")\n    source = request.args.get(\"source\", \"arxiv\").lower()\n    limit = int(request.args.get(\"limit\", \"10\"))\n    \n    # Validate query\n    if not query:\n        return jsonify({\"error\": {\"code\": \"INVALID_QUERY\", \"message\": \"The query parameter is missing.\"}}), 400\n        \n    # Search cache key\n    cache_key = f\"academic_search:{source}:{query}:{limit}\"\n    \n    # Check cache if we have a search_cache dictionary\n    cached_result = get_from_cache(search_cache, cache_key) if 'search_cache' in globals() else None\n    if cached_result:\n        return jsonify(cached_result)\n    \n    try:\n        # Get results from academic source\n        raw_results = search_academic_source(query, source, limit)\n        \n        # Format results\n        formatted_results = format_search_results(raw_results)\n        \n        # Cache results if we have a search_cache dictionary\n        if 'search_cache' in globals() and 'add_to_cache' in globals():\n            add_to_cache(search_cache, cache_key, formatted_results)\n        \n        return jsonify(formatted_results)\n        \n    except Exception as e:\n        logger.error(f\"Error in academic search: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"SEARCH_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500\n\n@academic_search_bp.route(\"/health\", methods=[\"GET\"])\n@require_api_key\n@limiter.limit(\"10 per minute\")\ndef academic_health_check():\n    \"\"\"Simple health check endpoint for Academic API.\"\"\"\n    return jsonify({\n        \"status\": \"ok\",\n        \"timestamp\": time.time(),\n        \"web_scraper_available\": web_scraper_available\n    })\n@academic_search_bp.route('/details/<path:id>', methods=['GET'])\n@require_api_key\n@limiter.limit(\"10 per minute\")\ndef academic_paper_details(id):\n    \"\"\"\n    Get detailed information about a specific paper.\n    \n    Path Parameters:\n        id (required): Unique identifier for the article\n        \n    Query Parameters:\n        source (optional): Specify the source (default: arxiv)\n    \"\"\"\n    source = request.args.get(\"source\", \"arxiv\").lower()\n    \n    # Cache key\n    cache_key = f\"academic_details:{source}:{id}\"\n    \n    # Check cache\n    cached_result = get_from_cache(details_cache, cache_key) if 'details_cache' in globals() else None\n    if cached_result:\n        return jsonify(cached_result)\n    \n    try:\n        # Get paper details\n        if source == \"arxiv\":\n            # Construct arXiv URL for the paper\n            if not id.startswith(\"http\"):\n                paper_url = f\"https://arxiv.org/abs/{id}\"\n            else:\n                paper_url = id\n            \n            # Use web_scraper to get HTML content\n            try:\n                html_content = web_scraper.extract_html_text(paper_url)\n                \n                # For PDF URL\n                pdf_url = web_scraper.convert_arxiv_url(paper_url)\n                \n                # In a real implementation, you would parse the HTML properly\n                # Here we'll create a sample result with minimal info\n                details = {\n                    \"id\": id,\n                    \"title\": f\"Paper {id}\",  # Would be extracted from HTML\n                    \"authors\": [\"Author A\", \"Author B\", \"Author C\"],  # Would be extracted from HTML\n                    \"abstract\": f\"This is a detailed abstract for paper {id}...\",  # Would be extracted from HTML\n                    \"publication_date\": \"2023-01-15\",  # Would be extracted from HTML\n                    \"source\": \"arxiv\",\n                    \"pdf_url\": pdf_url,\n                    \"metadata\": {\n                        \"categories\": [\"cs.AI\", \"cs.LG\"],\n                        \"comments\": \"Published in Example Conference 2023\",\n                        \"doi\": f\"10.1000/{id}\"\n                    }\n                }\n            except Exception as html_err:\n                logger.error(f\"Error extracting HTML for paper {id}: {html_err}\")\n                details = {\n                    \"id\": id,\n                    \"title\": f\"Paper {id}\",\n                    \"authors\": [],\n                    \"abstract\": \"\",\n                    \"publication_date\": \"\",\n                    \"source\": \"arxiv\",\n                    \"pdf_url\": f\"https://arxiv.org/pdf/{id}.pdf\",\n                    \"metadata\": {}\n                }\n        else:\n            # Handle other sources\n            details = {\n                \"id\": id,\n                \"title\": f\"Paper from {source}\",\n                \"authors\": [\"Unknown Author\"],\n                \"abstract\": \"Abstract not available for this source yet.\",\n                \"publication_date\": \"2023-01-01\",\n                \"source\": source,\n                \"pdf_url\": \"\",\n                \"metadata\": {}\n            }\n        \n        # Add to cache\n        if 'details_cache' in globals() and 'add_to_cache' in globals():\n            add_to_cache(details_cache, cache_key, details)\n        \n        return jsonify(details)\n        \n    except Exception as e:\n        logger.error(f\"Error getting academic paper details: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"DETAILS_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500\n\n@academic_search_bp.route('/download/<path:id>', methods=['GET'])\n@require_api_key\n@limiter.limit(\"10 per minute\")\ndef academic_download_paper(id):\n    \"\"\"\n    Download the PDF for a specific paper.\n    \n    Path Parameters:\n        id (required): Unique identifier for the article\n        \n    Query Parameters:\n        source (optional): Specify the source (default: arxiv)\n        filename (optional): Custom filename for the downloaded PDF\n    \"\"\"\n    source = request.args.get(\"source\", \"arxiv\").lower()\n    filename = request.args.get(\"filename\", \"\")\n    \n    try:\n        # Handle arxiv IDs\n        if source == \"arxiv\":\n            # Convert ID to URL if needed\n            if not id.startswith(\"http\"):\n                pdf_url = f\"https://arxiv.org/pdf/{id}.pdf\"\n            else:\n                pdf_url = id\n        else:\n            # For other sources, we would need proper URL handling\n            return jsonify({\n                \"error\": {\n                    \"code\": \"UNSUPPORTED_SOURCE\",\n                    \"message\": f\"PDF download for source '{source}' is not supported yet.\"\n                }\n            }), 400\n        \n        if not web_scraper_available:\n            return jsonify({\n                \"error\": {\n                    \"code\": \"MODULE_ERROR\",\n                    \"message\": \"Web scraper module not available for PDF download.\"\n                }\n            }), 500\n        \n        # Generate a task ID for tracking download progress\n        task_id = str(uuid.uuid4())\n        \n        # Download the PDF using your existing function\n        try:\n            pdf_file = download_pdf(\n                url=pdf_url,\n                save_path=DEFAULT_OUTPUT_FOLDER,\n                emit_progress=True,\n                task_id=task_id\n            )\n            \n            if pdf_file and os.path.exists(pdf_file):\n                # Return download information\n                return jsonify({\n                    \"status\": \"success\",\n                    \"message\": \"PDF downloaded successfully\",\n                    \"file_path\": pdf_file,\n                    \"file_name\": os.path.basename(pdf_file),\n                    \"file_size\": os.path.getsize(pdf_file),\n                    \"task_id\": task_id\n                })\n            else:\n                return jsonify({\n                    \"error\": {\n                        \"code\": \"DOWNLOAD_FAILED\",\n                        \"message\": \"Failed to download PDF.\"\n                    }\n                }), 404\n                \n        except Exception as download_error:\n            logger.error(f\"Download error: {download_error}\")\n            return jsonify({\n                \"error\": {\n                    \"code\": \"DOWNLOAD_ERROR\",\n                    \"message\": str(download_error)\n                }\n            }), 500\n            \n    except Exception as e:\n        logger.error(f\"Error in academic download endpoint: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"SERVER_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500\n\n@academic_search_bp.route('/citations/<path:id>', methods=['GET'])\n@require_api_key\n@limiter.limit(\"10 per minute\")\ndef academic_paper_citations(id):\n    \"\"\"\n    Get citation analysis for a specific paper.\n    \n    Path Parameters:\n        id (required): Unique identifier for the article\n        \n    Query Parameters:\n        source (optional): Specify the source (default: arxiv)\n        depth (optional): Depth of citation analysis (default: 1)\n    \"\"\"\n    source = request.args.get(\"source\", \"arxiv\").lower()\n    depth = int(request.args.get(\"depth\", \"1\"))\n    \n    # Cache key\n    cache_key = f\"academic_citations:{source}:{id}:{depth}\"\n    \n    # Check cache\n    cached_result = get_from_cache(search_cache, cache_key) if 'search_cache' in globals() else None\n    if cached_result:\n        return jsonify(cached_result)\n    \n    try:\n        # Get citation analysis\n        analysis = get_paper_citations(id, source, depth)\n        \n        # Cache results\n        if 'search_cache' in globals() and 'add_to_cache' in globals():\n            add_to_cache(search_cache, cache_key, analysis)\n        \n        return jsonify(analysis)\n        \n    except Exception as e:\n        logger.error(f\"Error getting paper citations: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"CITATION_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500\n\n@academic_search_bp.route('/recommendations/<path:id>', methods=['GET'])\n@require_api_key\n@limiter.limit(\"10 per minute\")\ndef academic_paper_recommendations(id):\n    \"\"\"\n    Get recommended papers related to a specific paper.\n    \n    Path Parameters:\n        id (required): Unique identifier for the article\n        \n    Query Parameters:\n        source (optional): Specify the source (default: arxiv)\n        limit (optional): Maximum number of recommendations (default: 5)\n    \"\"\"\n    source = request.args.get(\"source\", \"arxiv\").lower()\n    limit = int(request.args.get(\"limit\", \"5\"))\n    \n    # Cache key\n    cache_key = f\"academic_recommendations:{source}:{id}:{limit}\"\n    \n    # Check cache\n    cached_result = get_from_cache(search_cache, cache_key) if 'search_cache' in globals() else None\n    if cached_result:\n        return jsonify(cached_result)\n    \n    try:\n        # Get recommendations\n        recommendations = recommend_related_papers(id, source, limit)\n        \n        # Format response\n        result = {\n            \"paper_id\": id,\n            \"source\": source,\n            \"recommendation_count\": len(recommendations),\n            \"recommendations\": recommendations\n        }\n        \n        # Cache results\n        if 'search_cache' in globals() and 'add_to_cache' in globals():\n            add_to_cache(search_cache, cache_key, result)\n        \n        return jsonify(result)\n        \n    except Exception as e:\n        logger.error(f\"Error getting paper recommendations: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"RECOMMENDATION_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500\n\n@academic_search_bp.route(\"/bulk/download\", methods=[\"POST\"])\n@require_api_key\n@limiter.limit(\"3 per minute\")\ndef academic_bulk_download():\n    \"\"\"\n    Download multiple papers in bulk.\n    \n    Expected JSON body:\n    {\n        \"paper_ids\": [\"paper_id_1\", \"paper_id_2\", ...],\n        \"source\": \"arxiv\"\n    }\n    \"\"\"\n    if not request.is_json:\n        return jsonify({\"error\": {\"code\": \"INVALID_REQUEST\", \"message\": \"Request must be JSON\"}}), 400\n    \n    data = request.get_json()\n    paper_ids = data.get(\"paper_ids\", [])\n    source = data.get(\"source\", \"arxiv\")\n    \n    if not paper_ids:\n        return jsonify({\"error\": {\"code\": \"NO_PAPERS\", \"message\": \"No paper IDs provided\"}}), 400\n    \n    try:\n        # Use our existing bulk download function\n        result = bulk_download_papers(paper_ids, source)\n        return jsonify(result)\n        \n    except Exception as e:\n        logger.error(f\"Error in bulk download: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"BULK_DOWNLOAD_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500\n\n@academic_search_bp.route('/multi-source', methods=['GET'])\n@require_api_key\n@limiter.limit(\"5 per minute\")\ndef academic_multi_source_search():\n    \"\"\"\n    Search multiple academic sources simultaneously and combine results.\n    \n    Query Parameters:\n        query (required): The search term\n        sources (optional): Comma-separated list of sources (default: all)\n        limit (optional): Maximum results per source (default: 5)\n    \"\"\"\n    query = request.args.get(\"query\", \"\")\n    sources_param = request.args.get(\"sources\", \"arxiv,semantic,openalex\")\n    limit = int(request.args.get(\"limit\", \"5\"))\n    \n    # Validate query\n    if not query:\n        return jsonify({\"error\": {\"code\": \"INVALID_QUERY\", \"message\": \"The query parameter is missing.\"}}), 400\n        \n    # Parse sources\n    sources = [s.strip().lower() for s in sources_param.split(\",\")]\n    \n    # Cache key\n    cache_key = f\"academic_multi:{sources_param}:{query}:{limit}\"\n    \n    # Check cache\n    cached_result = get_from_cache(search_cache, cache_key) if 'search_cache' in globals() else None\n    if cached_result:\n        return jsonify(cached_result)\n    \n    try:\n        all_results = []\n        \n        # Process each source in parallel\n        with ThreadPoolExecutor(max_workers=min(3, len(sources))) as executor:\n            # Submit search tasks\n            future_to_source = {\n                executor.submit(search_academic_source, query, source, limit): source \n                for source in sources\n            }\n            \n            # Process results as they complete\n            for future in as_completed(future_to_source):\n                source = future_to_source[future]\n                try:\n                    source_results = future.result()\n                    all_results.extend(source_results)\n                except Exception as e:\n                    logger.error(f\"Error in {source} search: {e}\")\n        \n        # Format combined results\n        formatted_results = format_search_results(all_results)\n        \n        # Add source distribution information\n        source_counts = {}\n        for result in all_results:\n            source = result.get(\"source\", \"unknown\")\n            source_counts[source] = source_counts.get(source, 0) + 1\n        \n        formatted_results[\"source_distribution\"] = source_counts\n        \n        # Cache results\n        if 'search_cache' in globals() and 'add_to_cache' in globals():\n            add_to_cache(search_cache, cache_key, formatted_results)\n        \n        return jsonify(formatted_results)\n        \n    except Exception as e:\n        logger.error(f\"Error in multi-source search: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"SEARCH_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500\n\n    \n@academic_search_bp.route(\"/analyze/<path:id>\", methods=[\"GET\"])\n@require_api_key\n@limiter.limit(\"5 per minute\")\ndef academic_analyze_paper(id):\n    \"\"\"\n    Comprehensive analysis of a paper: details, citations, and recommendations.\n    \n    Path Parameters:\n        id (required): Unique identifier for the article\n        \n    Query Parameters:\n        source (optional): Specify the source (default: arxiv)\n        include_citations (optional): Whether to include citation analysis (default: true)\n        include_recommendations (optional): Whether to include recommendations (default: true)\n    \"\"\"\n    source = request.args.get(\"source\", \"arxiv\").lower()\n    include_citations = request.args.get(\"include_citations\", \"true\").lower() == \"true\"\n    include_recommendations = request.args.get(\"include_recommendations\", \"true\").lower() == \"true\"\n    \n    # Cache key\n    cache_key = f\"academic_analyze:{source}:{id}:{include_citations}:{include_recommendations}\"\n    \n    # Check cache\n    cached_result = get_from_cache(search_cache, cache_key) if 'search_cache' in globals() else None\n    if cached_result:\n        return jsonify(cached_result)\n    \n    try:\n        # Get paper details\n        details = None\n        try:\n            response = academic_paper_details(id)\n            if response.status_code == 200:\n                details = response.get_json()\n        except Exception:\n            # Try direct call if jsonify response doesn't work\n            details = get_paper_details(id, source)\n        \n        if not details or \"error\" in details:\n            return jsonify({\n                \"error\": {\n                    \"code\": \"DETAILS_ERROR\",\n                    \"message\": \"Failed to retrieve paper details\"\n                }\n            }), 404\n        \n        result = {\n            \"paper_id\": id,\n            \"source\": source,\n            \"details\": details\n        }\n        \n        # Get citation analysis if requested\n        if include_citations:\n            try:\n                citations = get_paper_citations(id, source)\n                result[\"citations\"] = citations\n            except Exception as e:\n                logger.warning(f\"Failed to get citation analysis: {e}\")\n                result[\"citations\"] = {\"error\": str(e)}\n        \n        # Get recommendations if requested\n        if include_recommendations:\n            try:\n                recommendations = recommend_related_papers(id, source)\n                result[\"recommendations\"] = {\n                    \"recommendation_count\": len(recommendations),\n                    \"recommendations\": recommendations\n                }\n            except Exception as e:\n                logger.warning(f\"Failed to get recommendations: {e}\")\n                result[\"recommendations\"] = {\"error\": str(e)}\n        \n        # Cache results\n        if 'search_cache' in globals() and 'add_to_cache' in globals():\n            add_to_cache(search_cache, cache_key, result)\n        \n        return jsonify(result)\n        \n    except Exception as e:\n        logger.error(f\"Error analyzing paper: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"ANALYSIS_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500\n\n@academic_search_bp.route(\"/extract\", methods=[\"GET\"])\n@require_api_key\n@limiter.limit(\"10 per minute\")\ndef academic_extract_from_url():\n    \"\"\"\n    Extract papers from a URL and optionally download them.\n    \n    Query Parameters:\n        url (required): URL to extract papers from\n        download (optional): Whether to download extracted papers (default: false)\n        output_folder (optional): Folder to save downloads (server default if not specified)\n    \"\"\"\n    url = request.args.get(\"url\")\n    download = request.args.get(\"download\", \"false\").lower() == \"true\"\n    output_folder = request.args.get(\"output_folder\", DEFAULT_OUTPUT_FOLDER)\n    \n    if not url:\n        return jsonify({\"error\": {\"code\": \"URL_REQUIRED\", \"message\": \"URL parameter is required\"}}), 400\n    \n    try:\n        # Ensure output folder exists\n        os.makedirs(output_folder, exist_ok=True)\n        \n        # Use web_scraper to extract PDF links\n        pdf_links = web_scraper.fetch_pdf_links(url)\n        \n        if not pdf_links:\n            return jsonify({\n                \"status\": \"completed\",\n                \"url\": url,\n                \"message\": \"No PDF links found\",\n                \"pdfs_found\": 0\n            })\n        \n        response = {\n            \"status\": \"completed\",\n            \"url\": url,\n            \"pdfs_found\": len(pdf_links),\n            \"pdfs\": []\n        }\n        \n        # Process each PDF link\n        for link in pdf_links:\n            pdf_info = {\n                \"url\": link.get(\"url\"),\n                \"title\": link.get(\"title\", \"Unknown\")\n            }\n            \n            # Download if requested\n            if download:\n                try:\n                    pdf_file = web_scraper.download_pdf(\n                        link.get(\"url\"),\n                        save_path=output_folder\n                    )\n                    \n                    if pdf_file and os.path.exists(pdf_file):\n                        pdf_info[\"file_path\"] = pdf_file\n                        pdf_info[\"file_size\"] = os.path.getsize(pdf_file)\n                        pdf_info[\"downloaded\"] = True\n                    else:\n                        pdf_info[\"downloaded\"] = False\n                except Exception as e:\n                    pdf_info[\"downloaded\"] = False\n                    pdf_info[\"error\"] = str(e)\n            \n            response[\"pdfs\"].append(pdf_info)\n        \n        # Update counts\n        if download:\n            response[\"pdfs_downloaded\"] = sum(1 for pdf in response[\"pdfs\"] if pdf.get(\"downloaded\", False))\n            response[\"output_folder\"] = output_folder\n        \n        return jsonify(response)\n        \n    except Exception as e:\n        logger.error(f\"Error extracting from URL: {e}\")\n        \n        return jsonify({\n            \"error\": {\n                \"code\": \"EXTRACTION_ERROR\",\n                \"message\": str(e)\n            }\n        }), 500","source":"/workspace/modules/blueprints/features/academic_search.py","title":"academic_search.py","language":"en"},{"content":"\"\"\"\nFile Processor Blueprint\nHandles all file processing related routes and functionality\n\"\"\"\n\nfrom flask import Blueprint, request, jsonify, send_file, current_app\nfrom flask_socketio import emit\nimport os\nimport logging\nimport tempfile\nimport uuid\nimport threading\nimport time\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Set, Any, Callable\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nfrom blueprints.api.management import register_task, update_task_progress, complete_task\nfrom blueprints.core.utils import ensure_temp_directory, sanitize_filename\nfrom blueprints.core.services import ProcessingTask, add_task, emit_progress_update, emit_task_error, emit_task_completion\n\nlogger = logging.getLogger(__name__)\n\n# Constants - fallback values in case import from structify fails\nDEFAULT_MAX_CHUNK_SIZE = 4096\nDEFAULT_CHUNK_OVERLAP = 200\nDEFAULT_STOP_WORDS = set([\"the\", \"and\", \"or\", \"for\", \"a\", \"an\", \"of\", \"in\", \"to\", \"from\",\n    \"on\", \"at\", \"by\", \"this\", \"is\", \"are\", \"were\", \"was\", \"be\", \"as\",\n    \"it\", \"that\", \"these\", \"those\", \"with\", \"can\", \"if\", \"not\", \"no\",\n    \"your\", \"you\", \"i\", \"am\", \"our\", \"we\", \"they\", \"their\", \"me\",\n    \"have\", \"has\", \"had\", \"also\", \"too\", \"very\", \"up\", \"out\", \"about\",\n    \"so\", \"some\", \"any\", \"my\", \"his\", \"her\", \"he\", \"she\", \"when\", \"where\",\n    \"what\", \"who\", \"why\", \"how\", \"which\", \"than\", \"then\", \"them\", \"but\"])\nDEFAULT_VALID_EXTENSIONS = [\".py\", \".html\", \".css\", \".yaml\", \".yml\",\n    \".txt\", \".md\", \".js\", \".gitignore\", \".ts\",\n    \".json\", \".csv\", \".rtf\", \".pdf\", \".docx\",\n    \".pptx\", \".xlsx\", \".xml\", \".sh\", \".bat\",\n    \".java\", \".c\", \".cpp\", \".h\", \".cs\", \".php\",\n    \".rb\", \".go\", \".rs\", \".swift\"]\nMAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB\nDEFAULT_PROCESS_TIMEOUT = 600  # seconds\nDEFAULT_MEMORY_LIMIT = 1024 * 1024 * 1024  # 1GB\nCACHE_FILE = \"file_cache.json\"\nDEFAULT_OUTPUT_FOLDER = \"downloads\"\n\ndef detect_output_format(filename):\n    \"\"\"\n    Detect the desired output format based on file extension.\n    \n    Args:\n        filename (str): The filename to analyze\n        \n    Returns:\n        str: 'json' or 'markdown' based on extension\n    \"\"\"\n    if not filename:\n        return 'json'\n    \n    filename_lower = filename.lower()\n    if filename_lower.endswith('.md') or filename_lower.endswith('.markdown'):\n        return 'markdown'\n    else:\n        return 'json'  # Default to JSON\n\n# Define FileStats class for compatibility\nclass FileStats:\n    def __init__(self):\n        self.processed_files = 0\n        self.skipped_files = 0\n        self.error_files = 0\n        self.total_files = 0\n        self.total_chunks = 0\n        self.pdf_files = 0\n        self.tables_extracted = 0\n        self.references_extracted = 0\n        self.scanned_pages_processed = 0\n        self.ocr_processed_files = 0\n        self.total_bytes = 0\n        self.total_processing_time = 0\n    \n    def to_dict(self):\n        # Calculate formatted duration\n        if self.total_processing_time < 1:\n            formatted_duration = f\"{self.total_processing_time*1000:.0f}ms\"\n        elif self.total_processing_time < 60:\n            formatted_duration = f\"{self.total_processing_time:.1f}s\"\n        else:\n            minutes = int(self.total_processing_time // 60)\n            seconds = self.total_processing_time % 60\n            formatted_duration = f\"{minutes}m {seconds:.1f}s\"\n        \n        # Calculate success rate percentage\n        if self.total_files > 0:\n            success_rate_percent = round((self.processed_files / self.total_files) * 100, 1)\n        else:\n            success_rate_percent = 100.0\n        \n        return {\n            'processed_files': self.processed_files,\n            'skipped_files': self.skipped_files,\n            'error_files': self.error_files,\n            'total_files': self.total_files,\n            'total_chunks': self.total_chunks,\n            'pdf_files': self.pdf_files,\n            'tables_extracted': self.tables_extracted,\n            'references_extracted': self.references_extracted,\n            'scanned_pages_processed': self.scanned_pages_processed,\n            'ocr_processed_files': self.ocr_processed_files,\n            'total_bytes': self.total_bytes,\n            'total_processing_time': self.total_processing_time,\n            'formatted_duration': formatted_duration,\n            'success_rate_percent': success_rate_percent\n        }\n\ndef write_optimized_json(all_data, output_file):\n    \"\"\"\n    Write highly optimized JSON output focused on LLM training data.\n    Removes metadata bloat and focuses on content + minimal context.\n    \n    Args:\n        all_data (dict): Processed data from all files\n        output_file (str): Path to output file\n        \n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    try:\n        # Debug: Log input data structure\n        logger.info(f\"write_optimized_json called with {len(all_data)} libraries\")\n        total_docs = sum(len(lib_data.get(\"docs_data\", [])) for lib_data in all_data.values())\n        logger.info(f\"Total documents across all libraries: {total_docs}\")\n        \n        if total_docs == 0:\n            logger.error(\"Input data contains no documents!\")\n            logger.error(f\"all_data keys: {list(all_data.keys())}\")\n            for lib_name, lib_data in all_data.items():\n                logger.error(f\"Library '{lib_name}' structure: {list(lib_data.keys())}\")\n        \n        training_data = {\n            \"training_corpus\": {\n                \"document_count\": total_docs,\n                \"created\": datetime.now().strftime(\"%Y-%m-%d\"),\n                \"documents\": []\n            }\n        }\n        \n        # Process all documents into a flat, clean structure\n        for lib_name, lib_data in all_data.items():\n            docs = lib_data.get(\"docs_data\", [])\n            logger.info(f\"Processing library '{lib_name}' with {len(docs)} documents\")\n            \n            for i, doc in enumerate(docs):\n                logger.debug(f\"Processing document {i}: {list(doc.keys())}\")\n                \n                # Only keep essential training data\n                clean_doc = {\n                    \"content\": doc.get(\"content\", \"\"),\n                    \"source\": doc.get(\"file_path\", \"unknown\")\n                }\n                \n                # Debug: Check if content is actually empty\n                content = doc.get(\"content\", \"\")\n                if not content:\n                    logger.warning(f\"Document {i} in library '{lib_name}' has empty content. Available fields: {list(doc.keys())}\")\n                    logger.debug(f\"Doc data sample: {str(doc)[:200]}...\")\n                \n                # Only add section name if it provides meaningful context\n                section_name = doc.get(\"section_name\", \"\")\n                if section_name and section_name.strip() and len(section_name) < 200:\n                    clean_doc[\"title\"] = section_name.strip()\n                \n                # Only add language if detected and useful\n                language = doc.get(\"language\", \"\")\n                if language and language not in [\"\", \"unknown\", \"auto\"]:\n                    clean_doc[\"language\"] = language\n                \n                # Only add tables if they exist and contain useful data\n                tables = doc.get(\"tables\", [])\n                if tables and len(tables) > 0:\n                    # Simplified table representation\n                    clean_doc[\"tables\"] = [str(table) for table in tables if table]\n                \n                # Skip empty content\n                if clean_doc[\"content\"] and clean_doc[\"content\"].strip():\n                    training_data[\"training_corpus\"][\"documents\"].append(clean_doc)\n                else:\n                    logger.warning(f\"Skipping document {i} in library '{lib_name}' due to empty content\")\n        \n        # Write with maximum compression settings for training efficiency\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(training_data, f, ensure_ascii=False, separators=(',', ':'))\n        \n        final_doc_count = len(training_data['training_corpus']['documents'])\n        logger.info(f\"Created training-optimized JSON with {final_doc_count} documents\")\n        \n        if final_doc_count == 0:\n            logger.error(\"No documents were processed! Training data is empty.\")\n            logger.error(f\"Input libraries: {list(all_data.keys())}\")\n            for lib_name, lib_data in all_data.items():\n                docs = lib_data.get(\"docs_data\", [])\n                logger.error(f\"Library '{lib_name}': {len(docs)} docs_data entries\")\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"Error writing optimized JSON: {e}\", exc_info=True)\n        return False\n\n\ndef write_markdown_output(all_data, output_file, stats=None):\n    \"\"\"\n    Write optimized Markdown output focused on LLM training data.\n    Eliminates metadata bloat and focuses on content readability.\n    \n    Args:\n        all_data (dict): Processed data from all files\n        output_file (str): Path to output file\n        stats (FileStats): Processing statistics (optional, minimal use)\n        \n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    try:\n        # Debug: Log input data structure\n        logger.info(f\"write_markdown_output called with {len(all_data)} libraries\")\n        total_docs = sum(len(lib_data.get(\"docs_data\", [])) for lib_data in all_data.values())\n        logger.info(f\"Total documents across all libraries: {total_docs}\")\n        \n        if total_docs == 0:\n            logger.error(\"Input data contains no documents!\")\n            logger.error(f\"all_data keys: {list(all_data.keys())}\")\n            for lib_name, lib_data in all_data.items():\n                logger.error(f\"Library '{lib_name}' structure: {list(lib_data.keys())}\")\n        \n        # Create optimized training-focused Markdown content\n        content = []\n        \n        # Minimal header - no metadata bloat\n        content.append(\"# Training Corpus\")\n        content.append(\"\")\n        content.append(f\"**Document Count:** {total_docs}\")\n        content.append(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d')}\")\n        content.append(\"\")\n        content.append(\"---\")\n        content.append(\"\")\n        \n        # Process all documents into clean, readable format\n        doc_count = 0\n        for lib_name, lib_data in all_data.items():\n            docs = lib_data.get(\"docs_data\", [])\n            logger.info(f\"Processing library '{lib_name}' with {len(docs)} documents\")\n            \n            if not docs:\n                continue\n                \n            for i, doc in enumerate(docs):\n                doc_count += 1\n                logger.debug(f\"Processing document {i}: {list(doc.keys())}\")\n                \n                # Extract essential content only\n                doc_content = doc.get(\"content\", \"\")\n                source = doc.get(\"file_path\", \"unknown\")\n                title = doc.get(\"section_name\", f\"Document {doc_count}\")\n                \n                # Debug: Check if content is actually empty\n                if not doc_content:\n                    logger.warning(f\"Document {i} in library '{lib_name}' has empty content. Available fields: {list(doc.keys())}\")\n                    logger.debug(f\"Doc data sample: {str(doc)[:200]}...\")\n                    continue\n                \n                # Clean content formatting\n                clean_content = doc_content.strip()\n                if not clean_content:\n                    logger.warning(f\"Skipping document {i} in library '{lib_name}' due to empty content\")\n                    continue\n                \n                # Add document with minimal metadata\n                content.append(f\"## {title}\")\n                content.append(\"\")\n                \n                # Only add source if it provides meaningful context\n                if source and source != \"unknown\":\n                    content.append(f\"**Source:** `{source}`\")\n                    content.append(\"\")\n                \n                # Add language if detected and useful\n                language = doc.get(\"language\", \"\")\n                if language and language not in [\"\", \"unknown\", \"auto\", \"en\"]:\n                    content.append(f\"**Language:** {language}\")\n                    content.append(\"\")\n                \n                # Add the actual content\n                content.append(clean_content)\n                content.append(\"\")\n                \n                # Add tables if they exist and contain useful data\n                tables = doc.get(\"tables\", [])\n                if tables and len(tables) > 0:\n                    content.append(\"### Tables\")\n                    content.append(\"\")\n                    for j, table in enumerate(tables):\n                        if table:  # Only add non-empty tables\n                            content.append(f\"**Table {j + 1}:**\")\n                            content.append(\"```\")\n                            content.append(str(table))\n                            content.append(\"```\")\n                            content.append(\"\")\n                \n                content.append(\"---\")\n                content.append(\"\")\n        \n        # Write the optimized content\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(content))\n        \n        logger.info(f\"Created training-optimized Markdown with {doc_count} documents\")\n        \n        if doc_count == 0:\n            logger.error(\"No documents were processed! Markdown data is empty.\")\n            logger.error(f\"Input libraries: {list(all_data.keys())}\")\n            for lib_name, lib_data in all_data.items():\n                docs = lib_data.get(\"docs_data\", [])\n                logger.error(f\"Library '{lib_name}': {len(docs)} docs_data entries\")\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"Error writing optimized Markdown: {e}\", exc_info=True)\n        return False\n\n\n# Legacy function removed - now using optimized write_markdown_output() \n# that creates a single, clean training-focused file instead of \n# complex multi-file structure with metadata bloat\n\n\n# Legacy functions removed - now using optimized single-file Markdown output\n# that eliminates metadata bloat and focuses on training content\n\n\n# Create the blueprint\nfile_processor_bp = Blueprint('file_processor', __name__, url_prefix='/api')\n\ndef process_all_files(\n    root_directory: str,\n    output_file: str,\n    max_chunk_size: int = DEFAULT_MAX_CHUNK_SIZE,\n    executor_type: str = \"thread\",\n    max_workers: Optional[int] = None,\n    stop_words: Set[str] = DEFAULT_STOP_WORDS,\n    use_cache: bool = False,\n    valid_extensions: List[str] = DEFAULT_VALID_EXTENSIONS,\n    ignore_dirs: str = \"venv,node_modules,.git,__pycache__,dist,build\",\n    stats_only: bool = False,\n    include_binary_detection: bool = True,\n    overlap: int = DEFAULT_CHUNK_OVERLAP,\n    max_file_size: int = MAX_FILE_SIZE,\n    timeout: int = DEFAULT_PROCESS_TIMEOUT,\n    memory_limit: int = DEFAULT_MEMORY_LIMIT,\n    progress_callback: Optional[Callable[[int, int, str], None]] = None,\n    stats_obj: Optional[FileStats] = None,\n    file_filter: Optional[Callable[[str], bool]] = None,\n    log_level: int = logging.INFO,\n    log_file: Optional[str] = None,\n    error_on_empty: bool = False,\n    include_failed_files: bool = False\n) -> Dict[str, Any]:\n    \"\"\"\n    Process all files in the root_directory with enhanced PDF handling and error recovery.\n    \n    Args:\n        root_directory: Base directory to process\n        output_file: Path to output JSON file\n        max_chunk_size: Maximum size of text chunks\n        executor_type: Type of executor (\"thread\", \"process\", or \"none\")\n        max_workers: Maximum number of worker threads/processes\n        stop_words: Set of words to ignore in tag generation\n        use_cache: Whether to use file caching\n        valid_extensions: List of file extensions to process\n        ignore_dirs: Comma-separated list of directories to ignore\n        stats_only: Whether to only generate statistics\n        include_binary_detection: Whether to detect and skip binary files\n        overlap: Number of characters to overlap between chunks\n        max_file_size: Maximum file size to process\n        timeout: Maximum processing time per file in seconds\n        memory_limit: Maximum memory usage before forcing garbage collection\n        progress_callback: Optional callback for progress reporting\n        stats_obj: Optional statistics object to use\n        file_filter: Optional function to filter files\n        log_level: Logging level\n        log_file: Optional log file path\n        error_on_empty: Whether to error if no files are found\n        include_failed_files: Whether to include details of failed files in output\n        \n    Returns:\n        Dictionary with statistics and processed data\n    \"\"\"\n    # Setup logging with specified options (import inline to avoid circular imports)\n    global logger\n    try:\n        from Structify.claude import setup_logging\n        logger = setup_logging(log_level, log_file)\n    except ImportError:\n        # Fallback to basic logging setup\n        logging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        logger = logging.getLogger(__name__)\n    \n    start_time = time.time()\n    stats = stats_obj if stats_obj else FileStats()\n    \n    # Create list of directories to ignore\n    ig_list = [d.strip() for d in ignore_dirs.split(\",\") if d.strip()]\n    rroot = Path(root_directory)\n    \n    # Track performance metrics\n    discovery_start = time.time()\n    \n    # Find all files matching extensions\n    all_files = []\n    skipped_during_discovery = []\n    try:\n        for p in rroot.rglob(\"*\"):\n            # Skip ignored directories\n            if any(ig in p.parts for ig in ig_list):\n                continue\n                \n            # Only process files that match extensions\n            if p.is_file() and any(p.suffix.lower() == ext.lower() for ext in valid_extensions):\n                # Apply custom filter if provided\n                if file_filter and not file_filter(str(p)):\n                    continue\n                \n                # Skip files that are too large (except PDFs)\n                try:\n                    size = p.stat().st_size\n                    if size > max_file_size and not p.suffix.lower() == '.pdf':\n                        logger.info(f\"Skipping large file during discovery: {p} ({size} bytes)\")\n                        skipped_during_discovery.append({\n                            \"file_path\": str(p),\n                            \"size\": size,\n                            \"reason\": \"file_too_large\"\n                        })\n                        continue\n                except OSError as e:\n                    # Log error but continue processing other files\n                    logger.warning(f\"Error accessing file {p}: {e}\")\n                    skipped_during_discovery.append({\n                        \"file_path\": str(p),\n                        \"reason\": f\"access_error: {str(e)}\"\n                    })\n                    continue\n                \n                all_files.append(p)\n    except Exception as e:\n        logger.error(f\"Error during file discovery: {e}\", exc_info=True)\n        return {\n            \"stats\": stats.to_dict(),\n            \"data\": {},\n            \"error\": str(e),\n            \"skipped_files\": skipped_during_discovery,\n            \"status\": \"failed\"\n        }\n\n    discovery_time = time.time() - discovery_start\n    logger.info(f\"Found {len(all_files)} valid files in {root_directory} ({discovery_time:.2f}s)\")\n    \n    # Check if any files were found\n    if not all_files:\n        message = f\"No files found in {root_directory} matching the provided criteria\"\n        if error_on_empty:\n            logger.error(message)\n            return {\n                \"stats\": stats.to_dict(),\n                \"data\": {},\n                \"error\": message,\n                \"skipped_files\": skipped_during_discovery,\n                \"status\": \"failed\"\n            }\n        else:\n            logger.warning(message)\n            return {\n                \"stats\": stats.to_dict(),\n                \"data\": {},\n                \"message\": message,\n                \"skipped_files\": skipped_during_discovery,\n                \"status\": \"completed\"\n            }\n    \n    if progress_callback:\n        progress_callback(0, len(all_files), \"discovery\")\n\n    # Load cache if enabled\n    processed_cache = {}\n    \n    # FIX: Properly extract the directory part of the output_file\n    # Use os.path.dirname to get just the directory part without any file components\n    output_dir = os.path.dirname(output_file)\n    # If output_dir is empty (meaning output_file is just a filename with no directory part),\n    # use the current directory\n    if not output_dir:\n        output_dir = \".\"\n    \n    cache_path = os.path.join(output_dir, CACHE_FILE)\n    \n    if use_cache:\n        if os.path.isfile(cache_path):\n            try:\n                with open(cache_path, \"r\", encoding=\"utf-8\") as c:\n                    processed_cache = json.load(c)\n                logger.info(f\"Loaded cache with {len(processed_cache)} entries\")\n            except Exception as e:\n                logger.warning(f\"Cache load error: {e}\")\n\n    # Filter files that need processing\n    to_process = []\n    for fpath in all_files:\n        sp = str(fpath)\n        \n        # Skip unchanged files if they're in cache\n        if use_cache and sp in processed_cache:\n            try:\n                mtime = fpath.stat().st_mtime\n                old = processed_cache[sp].get(\"mod_time\", 0)\n                if old >= mtime:\n                    stats.skipped_files += 1\n                    logger.debug(f\"Skipping unchanged file: {sp}\")\n                    continue\n            except OSError as e:\n                # If stat fails, process the file anyway\n                logger.debug(f\"Could not stat file {sp}, will process anyway: {e}\")\n                \n        to_process.append(fpath)\n\n    if not to_process:\n        logger.info(\"No new or modified files to process.\")\n        return {\n            \"stats\": stats.to_dict(),\n            \"data\": {},\n            \"message\": \"No new or modified files to process\",\n            \"skipped_files\": skipped_during_discovery,\n            \"status\": \"completed\"\n        }\n\n    # Determine optimal number of workers\n    if max_workers is None:\n        import multiprocessing\n        cpunum = multiprocessing.cpu_count()\n        if executor_type == \"process\":\n            max_workers = max(1, cpunum - 1)\n        else:\n            max_workers = min(32, cpunum * 2)\n\n    logger.info(f\"Using {executor_type} executor with max_workers={max_workers}\")\n\n    # Track errors and processing failures\n    processing_failures = []\n\n    # Process files in batches\n    processing_start = time.time()\n    \n    # Determine batch size based on file count\n    batch_size = 100\n    if len(to_process) <= 100:\n        batch_size = 20\n    elif len(to_process) <= 500:\n        batch_size = 50\n    elif len(to_process) <= 2000:\n        batch_size = 100\n    else:\n        batch_size = 200\n    \n    # Enhanced data structure with additional metadata\n    all_data = {}\n    \n    # Process in batches to manage memory usage\n    for i in range(0, len(to_process), batch_size):\n        batch = to_process[i:i+batch_size]\n        batch_num = i // batch_size + 1\n        total_batches = (len(to_process) + batch_size - 1) // batch_size\n        logger.info(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} files)\")\n        \n        results = []\n        \n        # Different processing strategies based on executor type\n        if executor_type == \"none\":\n            # Sequential processing\n            for p in batch:\n                # Special handling for PDFs\n                if str(p).lower().endswith('.pdf'):\n                    result = process_pdf_safely(str(p), root_directory, stats, max_chunk_size)\n                    if result:\n                        results.append((p, result))\n                    else:\n                        # Track processing failure\n                        processing_failures.append({\n                            \"file_path\": str(p),\n                            \"reason\": \"pdf_processing_failed\"\n                        })\n                else:\n                    # Standard processing for non-PDF files\n                    # Import safe_process inline to avoid circular imports\n                    try:\n                        from Structify.claude import safe_process\n                    except ImportError:\n                        logger.error(\"Could not import safe_process from Structify.claude\")\n                        processing_failures.append({\n                            \"file_path\": str(p),\n                            \"reason\": \"missing_safe_process_function\"\n                        })\n                        continue\n                    \n                    r = safe_process(\n                        p, root_directory, max_chunk_size, stop_words, \n                        include_binary_detection, stats, overlap, max_file_size, \n                        timeout, progress_callback\n                    )\n                    if r:\n                        results.append((p, r))\n                    else:\n                        # Track processing failure\n                        processing_failures.append({\n                            \"file_path\": str(p),\n                            \"reason\": \"processing_failed\"\n                        })\n                \n                # Check memory usage and trigger garbage collection if needed\n                try:\n                    import psutil\n                    process = psutil.Process()\n                    memory_info = process.memory_info()\n                    if memory_info.rss > memory_limit:\n                        logger.warning(f\"Memory usage ({memory_info.rss / 1024 / 1024:.1f} MB) exceeded limit. Triggering GC.\")\n                        import gc\n                        gc.collect()\n                except ImportError:\n                    pass  # psutil not available\n        else:\n            # Parallel processing\n            Exec = ThreadPoolExecutor if executor_type == \"thread\" else ProcessPoolExecutor\n            with Exec(max_workers=max_workers) as ex:\n                # Submit all tasks with special handling for PDFs\n                fut_map = {}\n                for p in batch:\n                    if str(p).lower().endswith('.pdf'):\n                        # Submit PDF processing task\n                        fut = ex.submit(\n                            process_pdf_safely,\n                            str(p),\n                            root_directory,\n                            stats,\n                            max_chunk_size\n                        )\n                    else:\n                        # Submit standard file processing task\n                        # Import safe_process inline to avoid circular imports\n                        try:\n                            from Structify.claude import safe_process\n                        except ImportError:\n                            logger.error(\"Could not import safe_process from Structify.claude\")\n                            processing_failures.append({\n                                \"file_path\": str(p),\n                                \"reason\": \"missing_safe_process_function\"\n                            })\n                            continue\n                        \n                        fut = ex.submit(\n                            safe_process, \n                            p, \n                            root_directory, \n                            max_chunk_size, \n                            stop_words, \n                            include_binary_detection, \n                            stats,\n                            overlap,\n                            max_file_size,\n                            timeout,\n                            progress_callback\n                        )\n                    fut_map[fut] = p\n                \n                # Process results as they complete\n                for fut in as_completed(fut_map):\n                    pth = fut_map[fut]\n                    out = fut.result()\n                    if out:\n                        results.append((pth, out))\n                    else:\n                        # Track processing failure\n                        processing_failures.append({\n                            \"file_path\": str(pth),\n                            \"reason\": \"processing_failed\"\n                        })\n\n        # Aggregate results into the output data structure\n        for pth, (lib, docs) in results:\n            if lib not in all_data:\n                all_data[lib] = {\n                    \"docs_data\": []\n                }\n            \n            # Add document data\n            all_data[lib][\"docs_data\"].extend(d.to_dict() for d in docs)\n            \n            # Update cache if enabled\n            if use_cache:\n                try:\n                    processed_cache[str(pth)] = {\n                        \"mod_time\": pth.stat().st_mtime,\n                        \"size\": pth.stat().st_size,\n                        \"chunks\": len(docs),\n                        \"last_processed\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    }\n                except OSError as e:\n                    # If stat fails, skip caching this file\n                    logger.debug(f\"Could not stat file {fpath} for caching: {e}\")\n\n        # Periodically save cache for large batches\n        if use_cache and (i + batch_size) % (batch_size * 5) == 0:\n            try:\n                with open(cache_path, \"w\", encoding=\"utf-8\") as c:\n                    json.dump(processed_cache, c, indent=2)\n                logger.info(f\"Saved cache after processing {i+batch_size} files.\")\n            except Exception as e:\n                logger.warning(f\"Cache save error: {e}\")\n                \n        # Check memory usage and trigger garbage collection if needed\n        try:\n            import psutil\n            process = psutil.Process()\n            memory_info = process.memory_info()\n            if memory_info.rss > memory_limit:\n                logger.warning(f\"Memory usage ({memory_info.rss / 1024 / 1024:.1f} MB) exceeded limit. Triggering GC.\")\n                import gc\n                gc.collect()\n        except ImportError:\n            pass  # psutil not available\n\n    # Calculate timing for statistics only (not stored in output)\n    processing_time = time.time() - processing_start\n    total_time = time.time() - start_time\n\n    # Write output JSON unless stats_only mode\n    if not stats_only:\n        try:\n            # FIX: Make sure output directory exists, but properly handle drive letters\n            # Get just the directory part of the output_file path\n            outdir = os.path.dirname(output_file)\n            \n            # Only create the directory if it's not an empty string\n            if outdir:\n                # Fix for handling paths with multiple drive letters (e.g., \"C:\\path\\C:\\file.json\")\n                # Check if outdir contains multiple drive letters\n                drive_pattern = re.compile(r'([A-Za-z]:)')\n                drive_matches = drive_pattern.findall(outdir)\n                \n                if len(drive_matches) > 1:\n                    # Path contains multiple drive letters, use only the first one\n                    logger.warning(f\"Path contains multiple drive letters: {outdir}\")\n                    first_drive_end = outdir.find(drive_matches[0]) + len(drive_matches[0])\n                    second_drive_start = outdir.find(drive_matches[1])\n                    \n                    # Use just the first drive and its path\n                    clean_outdir = outdir[:second_drive_start]\n                    logger.info(f\"Using cleaned directory path: {clean_outdir}\")\n                    \n                    # Update the output_file path to use the correct directory\n                    output_filename = os.path.basename(output_file)\n                    output_file = os.path.join(clean_outdir, output_filename)\n                    logger.info(f\"Updated output file path: {output_file}\")\n                    \n                    outdir = clean_outdir\n                \n                # Create the directory\n                try:\n                    os.makedirs(outdir, exist_ok=True)\n                    logger.info(f\"Ensured output directory exists: {outdir}\")\n                except Exception as dir_err:\n                    logger.error(f\"Error creating output directory: {dir_err}\")\n                    \n                    # Fallback: Try to use the user's Documents folder\n                    try:\n                        import pathlib\n                        docs_dir = os.path.join(str(pathlib.Path.home()), \"Documents\")\n                        os.makedirs(docs_dir, exist_ok=True)\n                        \n                        # Update output file path to use Documents folder\n                        output_filename = os.path.basename(output_file)\n                        output_file = os.path.join(docs_dir, output_filename)\n                        logger.warning(f\"Using fallback output location: {output_file}\")\n                    except Exception as fallback_err:\n                        logger.error(f\"Error creating fallback output directory: {fallback_err}\")\n                        # Last resort: Try to use current directory\n                        output_filename = os.path.basename(output_file)\n                        output_file = output_filename\n                        logger.warning(f\"Using current directory for output: {output_file}\")\n                \n            # Detect output format and write accordingly\n            output_format = detect_output_format(output_file)\n            \n            if output_format == 'markdown':\n                success = write_markdown_output(all_data, output_file, stats)\n                if success:\n                    logger.info(f\"Created Markdown output at {output_file}\")\n                else:\n                    logger.error(f\"Failed to write Markdown output to {output_file}\")\n            else:\n                # Use optimized JSON output instead of legacy format\n                success = write_optimized_json(all_data, output_file)\n                \n                if success:\n                    logger.info(f\"Created optimized JSON output at {output_file}\")\n                else:\n                    logger.error(f\"Failed to write optimized JSON output to {output_file}\")\n                    # Fallback to legacy JSON format\n                    try:\n                        logger.info(\"Attempting fallback to legacy JSON format...\")\n                        success = write_json_safely(all_data, output_file)\n                        if success:\n                            logger.info(f\"Created legacy JSON output at {output_file}\")\n                    except Exception as alt_err:\n                        logger.error(f\"Legacy JSON writing also failed: {alt_err}\")\n            \n            if progress_callback:\n                progress_callback(100, 100, \"completed\")\n        except Exception as e:\n            logger.error(f\"Error writing final output: {e}\", exc_info=True)\n            if progress_callback:\n                progress_callback(0, 0, \"error\")\n\n    # Save final cache state\n    if use_cache:\n        try:\n            with open(cache_path, \"w\", encoding=\"utf-8\") as c:\n                json.dump(processed_cache, c, indent=2)\n            logger.info(f\"Saved final cache to {cache_path}\")\n        except Exception as e:\n            logger.warning(f\"Final cache save error: {e}\")\n\n    # Log final statistics and return results\n    final_stats = stats.to_dict()\n    final_stats[\"total_duration_seconds\"] = total_time\n    final_stats[\"processing_duration_seconds\"] = processing_time\n    final_stats[\"discovery_duration_seconds\"] = discovery_time\n    \n    if stats.processed_files > 0:\n        final_stats[\"seconds_per_file\"] = processing_time / stats.processed_files\n        \n    if stats.total_files > 0:\n        final_stats[\"success_rate\"] = stats.processed_files / stats.total_files * 100\n    \n    logger.info(f\"Processing complete in {total_time:.2f}s\")\n    logger.info(f\"Stats: {len(all_files)} files found, {stats.processed_files} processed, \" +\n                f\"{stats.skipped_files} skipped, {stats.error_files} errors, \" +\n                f\"{stats.total_chunks} chunks created\")\n    \n    # PDF-specific statistics\n    if stats.pdf_files > 0:\n        logger.info(f\"PDF Stats: {stats.pdf_files} PDFs processed, {stats.tables_extracted} tables extracted, \" +\n                    f\"{stats.references_extracted} references extracted, {stats.ocr_processed_files} OCR processed\")\n    \n    result = {\n        \"stats\": final_stats,\n        \"data\": all_data,\n        \"status\": \"completed\",\n        \"message\": f\"Successfully processed {stats.processed_files} files\",\n        \"output_file\": output_file  # Return the potentially updated output file path\n    }\n    \n    # Include failure information if requested\n    if include_failed_files:\n        result[\"processing_failures\"] = processing_failures\n        result[\"skipped_during_discovery\"] = skipped_during_discovery\n        \n    return result    \n# Helper functions for emitting Socket.IO events\ndef emit_task_error(task_id, error_message, error_details=None, stats=None):\n    \"\"\"\n    Emit a task error event via Socket.IO.\n    \n    Args:\n        task_id: Unique identifier for the task\n        error_message: Error message string\n        error_details: Optional additional error details\n        stats: Optional statistics at time of error\n    \"\"\"\n    try:\n        payload = {\n            'task_id': task_id,\n            'status': 'failed',\n            'error': error_message,\n            'timestamp': time.time()\n        }\n        \n        # Include error details if provided\n        if error_details:\n            payload['error_details'] = error_details\n            \n        # Process stats for serialization\n        if stats:\n            if hasattr(stats, 'to_dict') and callable(stats.to_dict):\n                payload['stats'] = stats.to_dict()\n            elif isinstance(stats, dict):\n                payload['stats'] = stats\n            else:\n                try:\n                    payload['stats'] = stats.__dict__\n                except:\n                    payload['stats'] = str(stats)\n        \n        # Emit via Socket.IO\n        emit('task_error', payload, broadcast=True, namespace='/')\n        logger.error(f\"Task {task_id} error: {error_message}\")\n    except Exception as e:\n        logger.error(f\"Error emitting task_error: {e}\")\n\n@file_processor_bp.route('/process', methods=['POST'])\ndef start_processing():\n    \"\"\"\n    API endpoint to start processing files in the specified directory.\n    Handles JSON or form data input, validates parameters, and creates a processing task.\n    \n    Expected parameters:\n    - input_dir: Directory containing files to process\n    - output_file: Optional output filename or full path \n    - output_dir: Optional output directory (ignored if output_file has directory part)\n    \n    Returns:\n        JSON response with task details and status\n    \"\"\"\n    try:\n        # Get the JSON data from the request\n        if request.is_json:\n            data = request.get_json()\n        else:\n            data = request.form\n        \n        # Ensure temp directory exists\n        ensure_temp_directory()\n        \n        # Extract variables from the request\n        input_dir = data.get(\"input_dir\")\n        output_file = data.get(\"output_file\")  # Extract output file from request\n        output_dir = data.get(\"output_dir\")  # Optional, can be None\n        \n        # Log the received parameters\n        logger.info(f\"Processing request: input_dir={input_dir}, output_file={output_file}, output_dir={output_dir}\")\n        \n        # Validate inputs\n        if not input_dir:\n            logger.warning(\"Request missing input_dir parameter\")\n            return jsonify({\"error\": \"Input directory not specified\"}), 400\n        \n        if not output_file:\n            # Auto-generate output filename based on input directory if not provided\n            output_file = \"processed_\" + os.path.basename(os.path.normpath(input_dir)) + \".json\"\n            logger.info(f\"No output file specified, generated name: {output_file}\")\n        \n        # Get the full output path\n        final_output_path = get_output_filepath(output_file, output_dir)\n        logger.info(f\"Resolved output path: {final_output_path}\")\n        \n        # Generate a unique task ID\n        task_id = str(uuid.uuid4())\n        \n        # Create and start the processing task\n        task = ProcessingTask(task_id, input_dir, final_output_path)\n        # Add task to active tasks registry\n        add_task(task_id, task)\n        # Register task with API management\n        register_task(task_id, 'file_processing', input_dir=input_dir, output_file=final_output_path)\n        task.start()\n        \n        # Return success response\n        response = {\n            \"task_id\": task_id,\n            \"status\": \"processing\",\n            \"message\": \"Processing started\",\n            \"input_dir\": input_dir,\n            \"output_file\": final_output_path\n        }\n        \n        logger.info(f\"Started processing task: {task_id} for input directory: {input_dir}\")\n        return jsonify(response)\n        \n    except Exception as e:\n        logger.error(f\"Error in start_processing: {str(e)}\", exc_info=True)\n        return jsonify({\n            \"error\": f\"Failed to start processing: {str(e)}\",\n            \"status\": \"error\"\n        }), 500\n\n\ndef get_output_filepath(filename, user_defined_dir=None):\n    \"\"\"\n    Resolves user-specified output directory or uses default fallback.\n    Automatically detects output format based on file extension.\n    \n    Args:\n        filename (str): The desired output filename (with or without extension)\n        user_defined_dir (str, optional): Override the default output folder\n    \n    Returns:\n        str: Absolute path to the properly named output file\n    \"\"\"\n    # Handle potential None input\n    if not filename:\n        filename = \"output\"\n    \n    # Detect output format based on extension and preserve it\n    output_format = detect_output_format(filename)\n    \n    # Extract base filename without extension for sanitization\n    if filename.lower().endswith('.json'):\n        base_filename = filename[:-5]\n        extension = '.json'\n    elif filename.lower().endswith('.md'):\n        base_filename = filename[:-3]\n        extension = '.md'\n    else:\n        base_filename = filename\n        extension = '.json'  # Default to JSON if no extension\n    \n    # Sanitize the filename and restore extension\n    sanitized_name = sanitize_filename(base_filename) + extension\n    \n    # Check if we have a full path in output_filename\n    if os.path.dirname(filename):\n        # User provided a path with the filename\n        target_folder = os.path.dirname(filename)\n        base_name = os.path.basename(filename)\n        # Preserve the extension from the original filename\n        if base_name.lower().endswith('.md'):\n            sanitized_name = sanitize_filename(base_name[:-3]) + \".md\"\n        elif base_name.lower().endswith('.json'):\n            sanitized_name = sanitize_filename(base_name[:-5]) + \".json\"\n        else:\n            sanitized_name = sanitize_filename(base_name) + \".json\"\n    else:\n        # Use override folder or default to the DEFAULT_OUTPUT_FOLDER\n        target_folder = user_defined_dir or DEFAULT_OUTPUT_FOLDER\n    \n    # Make sure target_folder is defined and is an absolute path\n    if not target_folder or not isinstance(target_folder, str):\n        logger.warning(f\"Invalid target folder: {target_folder}, falling back to DEFAULT_OUTPUT_FOLDER\")\n        target_folder = DEFAULT_OUTPUT_FOLDER\n    \n    # Convert to absolute path\n    target_folder = os.path.abspath(target_folder)\n    \n    # If target folder doesn't exist, try to create it\n    try:\n        if not os.path.isdir(target_folder):\n            os.makedirs(target_folder, exist_ok=True)\n            logger.info(f\"Created output directory: {target_folder}\")\n    except Exception as e:\n        logger.warning(f\"Could not create directory {target_folder}: {e}\")\n        # Fall back to DEFAULT_OUTPUT_FOLDER if we can't create the directory\n        target_folder = DEFAULT_OUTPUT_FOLDER\n        # Try to ensure this directory exists\n        try:\n            os.makedirs(target_folder, exist_ok=True)\n        except Exception as e2:\n            logger.error(f\"Cannot create fallback directory {target_folder}: {e2}\")\n            # Last resort - use temp directory\n            import tempfile\n            target_folder = tempfile.gettempdir()\n    \n    # Construct and ensure the final path\n    final_output_path = os.path.join(target_folder, sanitized_name)\n    \n    logger.info(f\"Output file will be saved at: {final_output_path}\")\n    return final_output_path\n\n\ndef resolve_output_path(directory, filename):\n    \"\"\"\n    Resolve output path with proper directory creation if needed.\n    \n    Args:\n        directory (str): The directory to save the file in\n        filename (str): Output filename\n        \n    Returns:\n        str: Full path to the resolved output file\n    \"\"\"\n    # Create the directory if it doesn't exist\n    if not os.path.exists(directory):\n        try:\n            os.makedirs(directory, exist_ok=True)\n            logger.info(f\"Created directory: {directory}\")\n        except Exception as e:\n            logger.warning(f\"Could not create directory {directory}: {e}\")\n            # Fall back to DEFAULT_OUTPUT_FOLDER\n            directory = DEFAULT_OUTPUT_FOLDER\n            try:\n                os.makedirs(directory, exist_ok=True)\n            except Exception as e2:\n                logger.error(f\"Cannot create fallback directory {directory}: {e2}\")\n                # Last resort - use temp directory\n                import tempfile\n                directory = tempfile.gettempdir()\n    \n    # Return the full path\n    return os.path.join(directory, filename)\n\n\n@file_processor_bp.route('/status/<task_id>', methods=['GET'])\ndef task_status(task_id):\n    \"\"\"\n    Get a comprehensive status report of the task.\n    \n    Args:\n        task_id (str): The unique identifier for the task\n        \n    Returns:\n        JSON response with task status information\n    \"\"\"\n    task = get_task(task_id)\n    if not task:\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"Task with ID {task_id} not found.\", 404)\n    \n    # Prepare the response data\n    response_data = {\n        \"task_id\": task.task_id,\n        \"status\": task.status,\n        \"progress\": task.progress,\n        \"error\": getattr(task, \"error_message\", None),\n        \"start_time\": task.start_time,\n        \"elapsed_seconds\": time.time() - task.start_time\n    }\n    \n    # Handle stats conversion for JSON serialization\n    if task.stats:\n        # If stats is a CustomFileStats object with to_dict method\n        if hasattr(task.stats, 'to_dict') and callable(task.stats.to_dict):\n            response_data[\"stats\"] = task.stats.to_dict()\n        # If stats is already a dict\n        elif isinstance(task.stats, dict):\n            response_data[\"stats\"] = task.stats\n        # Fall back to converting object attributes to dict\n        elif hasattr(task.stats, '__dict__'):\n            response_data[\"stats\"] = {k: v for k, v in task.stats.__dict__.items() \n                                    if not k.startswith('__') and not callable(v)}\n        else:\n            # If we can't serialize it, set to empty dict\n            response_data[\"stats\"] = {}\n            app.logger.warning(f\"Could not serialize stats for task {task_id}, using empty dict\")\n    else:\n        response_data[\"stats\"] = {}\n    \n    # Add output file if available\n    if hasattr(task, 'output_file') and task.output_file:\n        response_data[\"output_file\"] = task.output_file\n    \n    # Add estimated time remaining if progress is sufficient\n    if task.progress > 0 and task.progress < 100:\n        elapsed = time.time() - task.start_time\n        if elapsed > 0:\n            # Calculate time per percentage point\n            time_per_point = elapsed / task.progress\n            # Estimated time for remaining percentage points\n            remaining_percent = 100 - task.progress\n            response_data[\"estimated_seconds_remaining\"] = time_per_point * remaining_percent\n    \n    # Add human-readable elapsed and estimated time\n    response_data[\"elapsed_time_readable\"] = format_time_duration(response_data[\"elapsed_seconds\"])\n    if \"estimated_seconds_remaining\" in response_data:\n        response_data[\"estimated_time_remaining_readable\"] = format_time_duration(\n            response_data[\"estimated_seconds_remaining\"]\n        )\n    \n    return jsonify(response_data)\n\ndef format_time_duration(seconds):\n    \"\"\"Format seconds into a human-readable duration string.\"\"\"\n    if seconds < 60:\n        return f\"{int(seconds)} seconds\"\n    elif seconds < 3600:\n        minutes = int(seconds / 60)\n        return f\"{minutes} minute{'s' if minutes != 1 else ''}\"\n    else:\n        hours = int(seconds / 3600)\n        minutes = int((seconds % 3600) / 60)\n        return f\"{hours} hour{'s' if hours != 1 else ''} {minutes} minute{'s' if minutes != 1 else ''}\"\n\n@file_processor_bp.route('/download/<task_id>', methods=['GET'])\ndef download_result(task_id):\n    task = get_task(task_id)\n    if not task:\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"Task with ID {task_id} not found.\", 404)\n    \n    if task.status != \"completed\":\n        return structured_error_response(\"TASK_INCOMPLETE\", \"Task is not completed yet.\", 409)\n    \n    if not hasattr(task, 'output_file') or not task.output_file:\n        return structured_error_response(\"FILE_NOT_FOUND\", \"No output file associated with this task.\", 404)\n    \n    if not os.path.exists(task.output_file):\n        return structured_error_response(\"FILE_NOT_FOUND\", \"Output file not found on server.\", 404)\n    \n    try:\n        return send_from_directory(\n            os.path.dirname(task.output_file),\n            os.path.basename(task.output_file),\n            as_attachment=True,\n            download_name=os.path.basename(task.output_file)\n        )\n    except Exception as e:\n        logger.exception(f\"Error downloading file {task.output_file}: {e}\")\n        return structured_error_response(\"FILE_READ_ERROR\", f\"Could not read output file: {e}\", 500)\n\n@file_processor_bp.route(\"/download/<path:filename>\")\ndef download_file(filename):\n    \"\"\"Download any file from the default output folder.\"\"\"\n    safe_filename = secure_filename(filename)\n    try:\n        return send_from_directory(DEFAULT_OUTPUT_FOLDER, safe_filename, as_attachment=True)\n    except FileNotFoundError:\n        abort(404)\n    except Exception as e:\n        logger.exception(f\"Error downloading file {filename}: {e}\")\n        abort(500)\n        \n@file_processor_bp.route('/open/<task_id>', methods=['GET'])\ndef open_result_file(task_id):\n    task = get_task(task_id)\n    if not task:\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"Task with ID {task_id} not found.\", 404)\n    \n    if task.status != \"completed\":\n        return structured_error_response(\"TASK_INCOMPLETE\", \"Task is not completed yet.\", 409)\n    \n    if not hasattr(task, 'output_file') or not task.output_file:\n        return structured_error_response(\"FILE_NOT_FOUND\", \"No output file associated with this task.\", 404)\n    \n    if not os.path.exists(task.output_file):\n        return structured_error_response(\"FILE_NOT_FOUND\", \"Output file not found on server.\", 404)\n    \n    try:\n        if os.name == \"nt\":  # Windows\n            os.startfile(task.output_file)\n        else:\n            try:\n                subprocess.run([\"xdg-open\", task.output_file], check=False)\n            except Exception:\n                subprocess.run([\"open\", task.output_file], check=False)\n                \n        return jsonify({\"success\": True, \"message\": \"File opened locally.\"})\n    except Exception as e:\n        logger.exception(f\"Error opening file {task.output_file}: {e}\")\n        return structured_error_response(\"OPEN_FAILED\", f\"Could not open file: {e}\", 400)\n@file_processor_bp.route(\"/api/open-file\", methods=[\"POST\"])\ndef open_arbitrary_file():\n    \"\"\"Open any file by path (for recent tasks history).\"\"\"\n    data = request.json or {}\n    file_path = data.get(\"path\")\n    \n    if not file_path:\n        return structured_error_response(\"PATH_REQUIRED\", \"File path is required.\", 400)\n    \n    if not os.path.exists(file_path):\n        return structured_error_response(\"FILE_NOT_FOUND\", \"File not found on server.\", 404)\n    \n    try:\n        if os.name == \"nt\":  # Windows\n            os.startfile(file_path)\n        else:\n            try:\n                subprocess.run([\"xdg-open\", file_path], check=False)\n            except Exception:\n                subprocess.run([\"open\", file_path], check=False)\n                \n        return jsonify({\"success\": True, \"message\": \"File opened locally.\"})\n    except Exception as e:\n        logger.exception(f\"Error opening file {file_path}: {e}\")\n        return structured_error_response(\"OPEN_FAILED\", f\"Could not open file: {e}\", 400)\n\ntask_registry = {}  # Or a shared task store object\n\ndef get_task(task_id):\n    return task_registry.get(task_id)  # Customize if registry is class-based\n\ndef structured_error_response(code, message, status_code=400):\n    response = jsonify({\n        \"error\": {\n            \"code\": code,\n            \"message\": message\n        }\n    })\n    response.status_code = status_code\n    return response\n\n@file_processor_bp.route('/detect-path', methods=['POST'])\ndef detect_path():\n    data = request.json or {}\n    folder_name = data.get(\"folderName\")\n    file_paths = data.get(\"filePaths\", [])\n    full_path = data.get(\"fullPath\")\n    if not folder_name:\n        return structured_error_response(\"FOLDER_NAME_REQUIRED\", \"Folder name is required.\", 400)\n    if full_path:\n        norm = os.path.abspath(full_path)\n        if os.path.isdir(norm):\n            logger.info(f\"Verified direct full_path: {norm}\")\n            return jsonify({\"fullPath\": norm})\n    candidate = Path(folder_name).resolve()\n    if candidate.is_dir():\n        logger.info(f\"Using resolved absolute path: {candidate}\")\n        return jsonify({\"fullPath\": str(candidate)})\n    if file_paths:\n        try:\n            normalized_paths = [os.path.abspath(p) for p in file_paths]\n            common_base = os.path.commonpath(normalized_paths)\n            if os.path.isdir(common_base):\n                logger.info(f\"Found common directory: {common_base}\")\n                return jsonify({\"fullPath\": common_base})\n        except ValueError:\n            # commonpath can fail if paths are on different drives or invalid\n            logger.debug(\"Could not find common path from provided file paths\")\n    standard_locs = [Path.cwd(), Path.home() / \"Documents\", Path.home() / \"Desktop\",\n                     Path.home() / \"Downloads\", Path.home() / \"OneDrive\"]\n    for base in standard_locs:\n        potential = (base / folder_name).resolve()\n        if potential.is_dir():\n            logger.info(f\"Found directory under {base}: {potential}\")\n            return jsonify({\"fullPath\": str(potential)})\n    logger.warning(\"Could not automatically detect the folder path.\")\n    return structured_error_response(\"PATH_NOT_DETECTED\", \"Could not automatically detect the folder path.\", 404)\n\n\n@file_processor_bp.route('/verify-path', methods=['POST'])\ndef verify_path():\n    \"\"\"\n    Enhanced API endpoint to validate path with better error handling\n    and permissions testing.\n    \"\"\"\n    data = request.get_json()\n    if not data or \"path\" not in data:\n        return jsonify({\n            \"status\": \"error\",\n            \"message\": \"Path is required\"\n        }), 400\n    \n    path_str = data.get(\"path\")\n    if not path_str:\n        return jsonify({\n            \"status\": \"error\", \n            \"message\": \"Empty path provided\"\n        }), 400\n    \n    try:\n        # Normalize path\n        norm_path = os.path.abspath(os.path.expanduser(path_str))\n        \n        # Check if it exists\n        if os.path.exists(norm_path):\n            if os.path.isdir(norm_path):\n                # Check if it's writable\n                writable = os.access(norm_path, os.W_OK)\n                \n                return jsonify({\n                    \"exists\": True,\n                    \"isDirectory\": True,\n                    \"fullPath\": norm_path,\n                    \"canWrite\": writable,\n                    \"parentPath\": os.path.dirname(norm_path)\n                })\n            else:\n                # It exists but is not a directory\n                return jsonify({\n                    \"exists\": True,\n                    \"isDirectory\": False,\n                    \"fullPath\": norm_path,\n                    \"parentPath\": os.path.dirname(norm_path),\n                    \"canWrite\": False\n                })\n        else:\n            # Path doesn't exist, check parent directory\n            parent_path = os.path.dirname(norm_path)\n            parent_exists = os.path.isdir(parent_path)\n            parent_writable = os.access(parent_path, os.W_OK) if parent_exists else False\n            \n            return jsonify({\n                \"exists\": False,\n                \"isDirectory\": False,\n                \"fullPath\": norm_path,\n                \"parentPath\": parent_path if parent_exists else None,\n                \"parentExists\": parent_exists,\n                \"canCreate\": parent_writable\n            })\n    except Exception as e:\n        logger.error(f\"Error verifying path {path_str}: {e}\")\n        return jsonify({\n            \"status\": \"error\",\n            \"message\": str(e)\n        }), 500\n\n@file_processor_bp.route(\"/api/create-directory\", methods=[\"POST\"])\ndef create_directory():\n    \"\"\"\n    Create a directory at the specified path.\n    \"\"\"\n    data = request.get_json()\n    if not data or \"path\" not in data:\n        return jsonify({\n            \"status\": \"error\",\n            \"message\": \"Path is required\"\n        }), 400\n    \n    path_str = data.get(\"path\")\n    if not path_str:\n        return jsonify({\n            \"status\": \"error\", \n            \"message\": \"Empty path provided\"\n        }), 400\n    \n    try:\n        # Normalize path\n        norm_path = os.path.abspath(os.path.expanduser(path_str))\n        \n        # Check if path already exists\n        if os.path.exists(norm_path):\n            if os.path.isdir(norm_path):\n                return jsonify({\n                    \"success\": True,\n                    \"path\": norm_path,\n                    \"message\": \"Directory already exists\"\n                })\n            else:\n                return jsonify({\n                    \"success\": False,\n                    \"message\": f\"Path exists but is not a directory: {norm_path}\"\n                }), 400\n        \n        # Create the directory with parents\n        os.makedirs(norm_path, exist_ok=True)\n        \n        # Verify it was created\n        if os.path.isdir(norm_path):\n            return jsonify({\n                \"success\": True,\n                \"path\": norm_path,\n                \"message\": \"Directory created successfully\"\n            })\n        else:\n            return jsonify({\n                \"success\": False,\n                \"message\": f\"Failed to create directory: {norm_path}\"\n            }), 500\n    except Exception as e:\n        logger.error(f\"Error creating directory {path_str}: {e}\")\n        return jsonify({\n            \"success\": False,\n            \"message\": str(e)\n        }), 500\n# =============================================================================\n# BACKGROUND TASK PROCESSING\n# =============================================================================\n\ndef start_file_processing_task(task_id, input_dir, output_file, output_dir=None):\n    \"\"\"\n    Background task for processing files\n    This runs in a separate thread to avoid blocking the main application\n    \"\"\"\n    try:\n        logger.info(f\"Starting file processing task {task_id}\")\n        \n        # Update task status to processing\n        update_task_progress(task_id, 0, \"processing\", stats={'stage': 'initializing'})\n        emit_progress_update(task_id, 0, \"Initializing file processing...\")\n        \n        # Simulate file discovery\n        time.sleep(1)\n        emit_progress_update(task_id, 10, \"Discovering files...\")\n        \n        # Get list of files to process\n        files_to_process = []\n        if os.path.isdir(input_dir):\n            for root, dirs, files in os.walk(input_dir):\n                for file in files:\n                    if file.lower().endswith(('.txt', '.pdf', '.docx', '.md')):\n                        files_to_process.append(os.path.join(root, file))\n        \n        total_files = len(files_to_process)\n        if total_files == 0:\n            emit_task_error(task_id, \"No supported files found in directory\")\n            return\n        \n        logger.info(f\"Found {total_files} files to process\")\n        emit_progress_update(task_id, 20, f\"Found {total_files} files to process\")\n        \n        # Process files (simulate processing)\n        processed_files = 0\n        output_content = []\n        \n        for i, file_path in enumerate(files_to_process):\n            try:\n                # Update progress\n                progress = 20 + (60 * i / total_files)  # Progress from 20% to 80%\n                emit_progress_update(task_id, progress, f\"Processing {os.path.basename(file_path)}...\")\n                \n                # Simulate file processing time\n                time.sleep(0.5)\n                \n                # Read file content (basic text extraction)\n                try:\n                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                        content = f.read()\n                        output_content.append(f\"\\n\\n=== {file_path} ===\\n{content}\")\n                        processed_files += 1\n                except Exception as file_error:\n                    logger.warning(f\"Could not read file {file_path}: {file_error}\")\n                    output_content.append(f\"\\n\\n=== {file_path} ===\\nError reading file: {file_error}\")\n                \n            except Exception as e:\n                logger.error(f\"Error processing file {file_path}: {e}\")\n        \n        # Write output file\n        emit_progress_update(task_id, 80, \"Writing output file...\")\n        \n        # Determine output path\n        if output_dir and not os.path.isabs(output_file):\n            output_path = os.path.join(output_dir, output_file)\n        else:\n            output_path = output_file\n        \n        # Ensure output directory exists\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        \n        # Write the combined content\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(f\"# File Processing Results\\n\")\n            f.write(f\"# Total files processed: {processed_files}/{total_files}\\n\")\n            f.write(f\"# Processing completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n            f.write(\"\\n\".join(output_content))\n        \n        # Final progress update\n        emit_progress_update(task_id, 100, \"File processing completed successfully!\")\n        \n        # Mark task as completed\n        emit_task_completion(task_id, task_type=\"file_processing\", \n                           output_file=output_path,\n                           stats={\n                               'total_files': total_files,\n                               'processed_files': processed_files,\n                               'output_path': output_path\n                           })\n        \n        logger.info(f\"File processing task {task_id} completed successfully\")\n        \n    except Exception as e:\n        logger.error(f\"Error in file processing task {task_id}: {str(e)}\")\n        emit_task_error(task_id, f\"File processing failed: {str(e)}\")\n\n\n# Export the blueprint and key functions\n__all__ = [\n    'file_processor_bp',\n    'start_file_processing_task',\n    'process_all_files',\n    'get_output_filepath',\n    'resolve_output_path',\n    'format_time_duration'\n]","source":"/workspace/modules/blueprints/features/file_processor.py","title":"file_processor.py","language":"en"},{"content":"\"\"\"\nFile Utils Blueprint\nHandles file system operations, path detection, and utility functions\n\"\"\"\n\nfrom flask import Blueprint, request, jsonify, send_from_directory, current_app\nimport logging\nimport os\nimport subprocess\nimport platform\nfrom datetime import datetime\nfrom typing import List, Tuple\nfrom werkzeug.utils import secure_filename\n\n# Import necessary modules and functions\nfrom blueprints.core.services import require_api_key\nfrom blueprints.core.config import DEFAULT_OUTPUT_FOLDER\nfrom blueprints.core.utils import (\n    sanitize_filename, ensure_temp_directory, get_output_filepath,\n    structured_error_response, normalize_path, detect_common_path_from_files\n)\n\nlogger = logging.getLogger(__name__)\n\n# Optional magic library for MIME detection\nmagic_available = False\ntry:\n    import magic\n    magic_available = True\n    logger.info(\"python-magic available for MIME detection\")\nexcept ImportError:\n    logger.warning(\"python-magic not available. Try installing python-magic-bin on Windows\")\n    magic_available = False\n\n# Create the blueprint\nfile_utils_bp = Blueprint('file_utils', __name__, url_prefix='/api')\n\n# Export the blueprint\n__all__ = ['file_utils_bp']\n\n@file_utils_bp.route('/upload-for-path-detection', methods=['POST'])\ndef upload_for_path_detection():\n    if \"files\" not in request.files:\n        return structured_error_response(\"NO_FILES_IN_REQUEST\", \"No files part in request.\", 400)\n    folder_name = request.form.get(\"folderName\")\n    if not folder_name:\n        return structured_error_response(\"FOLDER_NAME_REQUIRED\", \"Folder name is required.\", 400)\n    logger.info(f\"Processing uploads for folder: {folder_name}\")\n    safe_folder = secure_filename(folder_name)\n    upload_dir = os.path.join(current_app.config.get(\"UPLOAD_FOLDER\", DEFAULT_OUTPUT_FOLDER), safe_folder)\n    os.makedirs(upload_dir, exist_ok=True)\n    files = request.files.getlist(\"files\")\n    for f in files:\n        if f.filename:\n            if not is_extension_allowed(f.filename):\n                return structured_error_response(\"UNSUPPORTED_EXTENSION\", f\"File extension not allowed: {f.filename}\", 400)\n            if not is_mime_allowed(f):\n                return structured_error_response(\"UNSUPPORTED_MIME_TYPE\", f\"Detected MIME not allowed for: {f.filename}\", 400)\n            filename = secure_filename(f.filename)\n            file_path = os.path.join(upload_dir, filename)\n            f.save(file_path)\n            logger.debug(f\"Saved uploaded file to {file_path}\")\n    return jsonify({\n        \"success\": True,\n        \"message\": \"Files uploaded successfully\",\n        \"fullPath\": safe_folder\n    })\n\n# Note: detect-path, verify-path, and create-directory routes are already in file_processor.py\n\n@file_utils_bp.route('/get-output-filepath', methods=['POST'])\ndef api_get_output_filepath():\n    \"\"\"\n    API endpoint to get a properly formatted output filepath.\n    \"\"\"\n    data = request.get_json()\n    filename = data.get('filename', '')\n    directory = data.get('directory', '')\n    \n    # Use the get_output_filepath function for consistent handling\n    try:\n        # Make sure the filename has a .json extension\n        if not filename.lower().endswith('.json'):\n            filename += '.json'\n            \n        # If a directory is provided, use it as the base\n        if directory:\n            full_path = os.path.join(os.path.abspath(directory), filename)\n        else:\n            # Otherwise, use the default output folder\n            full_path = os.path.join(DEFAULT_OUTPUT_FOLDER, filename)\n            \n        # Ensure the parent directory exists\n        parent_dir = os.path.dirname(full_path)\n        if not os.path.exists(parent_dir):\n            os.makedirs(parent_dir, exist_ok=True)\n            \n        return jsonify({\n            \"fullPath\": full_path,\n            \"directory\": os.path.dirname(full_path),\n            \"filename\": os.path.basename(full_path)\n        })\n    except Exception as e:\n        logger.error(f\"Error generating output filepath: {e}\")\n        return structured_error_response(\"PATH_ERROR\", f\"Error generating output path: {str(e)}\", 500)\n\n@file_utils_bp.route('/check-file-exists', methods=['POST'])\ndef api_check_file_exists():\n    \"\"\"\n    API endpoint to check if a file exists.\n    \"\"\"\n    data = request.get_json()\n    if not data or \"path\" not in data:\n        return jsonify({\n            \"status\": \"error\",\n            \"message\": \"Path is required\"\n        }), 400\n    \n    path_str = data.get(\"path\")\n    if not path_str:\n        return jsonify({\n            \"status\": \"error\", \n            \"message\": \"Empty path provided\"\n        }), 400\n    \n    try:\n        # Normalize path\n        norm_path = os.path.abspath(os.path.expanduser(path_str))\n        \n        # Check if file exists\n        exists = os.path.isfile(norm_path)\n        \n        # Get additional info if it exists\n        if exists:\n            try:\n                file_size = os.path.getsize(norm_path)\n                modified_time = os.path.getmtime(norm_path)\n                return jsonify({\n                    \"exists\": True,\n                    \"path\": norm_path,\n                    \"size\": file_size,\n                    \"size_formatted\": format_file_size(file_size),\n                    \"modified\": modified_time,\n                    \"modified_formatted\": format_timestamp(modified_time)\n                })\n            except Exception as detail_err:\n                logger.warning(f\"Error getting file details: {detail_err}\")\n                return jsonify({\n                    \"exists\": True,\n                    \"path\": norm_path\n                })\n        else:\n            return jsonify({\n                \"exists\": False,\n                \"path\": norm_path\n            })\n    except Exception as e:\n        logger.error(f\"Error checking if file exists: {e}\")\n        return structured_error_response(\"CHECK_ERROR\", f\"Error checking file: {str(e)}\", 500)\n\ndef format_file_size(size_bytes):\n    \"\"\"Format file size to human-readable string.\"\"\"\n    if size_bytes < 1024:\n        return f\"{size_bytes} B\"\n    elif size_bytes < 1024 * 1024:\n        return f\"{size_bytes / 1024:.1f} KB\"\n    elif size_bytes < 1024 * 1024 * 1024:\n        return f\"{size_bytes / (1024 * 1024):.1f} MB\"\n    else:\n        return f\"{size_bytes / (1024 * 1024 * 1024):.1f} GB\"\n\n\ndef format_timestamp(timestamp):\n    \"\"\"Format timestamp to human-readable string.\"\"\"\n    try:\n        dt = datetime.fromtimestamp(timestamp)\n        return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n    except:\n        return \"Unknown\"\n\n@file_utils_bp.route('/get-default-output-folder', methods=['GET'])\n@require_api_key\ndef get_default_output_folder():\n    \"\"\"\n    Get the default output folder path.\n    \"\"\"\n    try:\n        return jsonify({\n            \"path\": DEFAULT_OUTPUT_FOLDER,\n            \"exists\": os.path.isdir(DEFAULT_OUTPUT_FOLDER),\n            \"writable\": os.access(DEFAULT_OUTPUT_FOLDER, os.W_OK)\n        })\n    except Exception as e:\n        logger.error(f\"Error getting default output folder: {e}\")\n        return structured_error_response(\"SERVER_ERROR\", f\"Could not retrieve default output folder: {str(e)}\", 500)\n\n# Note: open-file route is already in file_processor.py\n@file_utils_bp.route(\"/api/open-file\", methods=[\"POST\"])\n@require_api_key\ndef open_arbitrary_file():\n    \"\"\"Open any file by path (for recent tasks history).\"\"\"\n    data = request.json or {}\n    file_path = data.get(\"path\")\n    \n    if not file_path:\n        return structured_error_response(\"PATH_REQUIRED\", \"File path is required.\", 400)\n    \n    if not os.path.exists(file_path):\n        return structured_error_response(\"FILE_NOT_FOUND\", \"File not found on server.\", 404)\n    \n    try:\n        if os.name == \"nt\":  # Windows\n            os.startfile(file_path)\n        else:\n            try:\n                subprocess.run([\"xdg-open\", file_path], check=False)\n            except Exception:\n                subprocess.run([\"open\", file_path], check=False)\n                \n        return jsonify({\"success\": True, \"message\": \"File opened locally.\"})\n    except Exception as e:\n        logger.exception(f\"Error opening file {file_path}: {e}\")\n        return structured_error_response(\"OPEN_FAILED\", f\"Could not open file: {e}\", 400)\n@file_utils_bp.route('/open-folder', methods=['POST'])\n@require_api_key\ndef open_folder():\n    \"\"\"Open a folder in the operating system's file explorer.\"\"\"\n    data = request.json or {}\n    folder_path = data.get(\"path\")\n    \n    if not folder_path:\n        return structured_error_response(\"PATH_REQUIRED\", \"Folder path is required.\", 400)\n    \n    if not os.path.exists(folder_path):\n        return structured_error_response(\"FOLDER_NOT_FOUND\", \"Folder not found on server.\", 404)\n    \n    try:\n        if os.name == \"nt\":  # Windows\n            os.startfile(folder_path)\n        else:\n            try:\n                subprocess.run([\"xdg-open\", folder_path], check=False)\n            except Exception:\n                subprocess.run([\"open\", folder_path], check=False)\n                \n        return jsonify({\"success\": True, \"message\": \"Folder opened locally.\"})\n    except Exception as e:\n        logger.exception(f\"Error opening folder {folder_path}: {e}\")\n        return structured_error_response(\"OPEN_FAILED\", f\"Could not open folder: {e}\", 400)\n\ndef find_directory_in_standard_locations(folder_name: str) -> str:\n    \"\"\"\n    Look for a folder name in standard locations.\n    \n    Args:\n        folder_name: The name of the folder to find\n        \n    Returns:\n        Full path if found, empty string otherwise\n    \"\"\"\n    # Define standard locations based on OS\n    if platform.system() == 'Windows':\n        # Windows standard locations\n        standard_locs = [\n            os.getcwd(),\n            os.path.join(os.path.expanduser('~'), 'Documents'),\n            os.path.join(os.path.expanduser('~'), 'Desktop'),\n            os.path.join(os.path.expanduser('~'), 'Downloads'),\n            os.path.join(os.path.expanduser('~'), 'OneDrive', 'Documents')\n        ]\n    else:\n        # Unix/Mac standard locations\n        standard_locs = [\n            os.getcwd(),\n            os.path.join(os.path.expanduser('~'), 'Documents'),\n            os.path.join(os.path.expanduser('~'), 'Desktop'),\n            os.path.join(os.path.expanduser('~'), 'Downloads')\n        ]\n    \n    # Look for folder in standard locations\n    for base in standard_locs:\n        try:\n            potential = os.path.join(base, folder_name)\n            if os.path.isdir(potential):\n                logger.info(f\"Found directory under {base}: {potential}\")\n                return potential\n        except Exception as e:\n            logger.debug(f\"Error checking {base}: {e}\")\n    \n    return \"\"\n\ndef get_parent_directory(path: str) -> str:\n    \"\"\"\n    Get the nearest existing parent directory.\n    \n    Args:\n        path: The path to check\n        \n    Returns:\n        The closest existing parent directory or empty string\n    \"\"\"\n    try:\n        path = normalize_path(path)\n        parent = path\n        \n        # Find the nearest existing parent\n        while parent and not os.path.isdir(parent):\n            parent = os.path.dirname(parent)\n            \n            # Break if we've reached the root\n            if parent == os.path.dirname(parent):\n                return \"\"\n        \n        return parent if parent and parent != path else \"\"\n    except Exception as e:\n        logger.error(f\"Error finding parent directory: {e}\")\n        return \"\"\n\ndef verify_and_create_directory(path: str) -> Tuple[bool, str, str]:\n    \"\"\"\n    Verify if a directory exists, and create it if requested.\n    \n    Args:\n        path: The directory path to verify/create\n        \n    Returns:\n        Tuple of (success, message, path)\n    \"\"\"\n    try:\n        path = normalize_path(path)\n        \n        # Check if directory exists\n        if os.path.isdir(path):\n            return True, \"Directory exists\", path\n            \n        # Get parent directory\n        parent = get_parent_directory(path)\n        \n        # If parent doesn't exist, fail\n        if not parent:\n            return False, \"No valid parent directory found\", \"\"\n            \n        # Try to create the directory\n        try:\n            os.makedirs(path, exist_ok=True)\n            logger.info(f\"Created directory: {path}\")\n            return True, \"Directory created successfully\", path\n        except Exception as e:\n            logger.error(f\"Failed to create directory {path}: {e}\")\n            return False, f\"Failed to create directory: {e}\", parent\n            \n    except Exception as e:\n        logger.error(f\"Error verifying directory {path}: {e}\")\n        return False, f\"Error verifying directory: {e}\", \"\"\n\ndef check_file_exists(file_path: str) -> bool:\n    \"\"\"\n    Check if a file exists.\n    \n    Args:\n        file_path: The file path to check\n        \n    Returns:\n        True if file exists\n    \"\"\"\n    try:\n        file_path = normalize_path(file_path)\n        return os.path.isfile(file_path)\n    except Exception as e:\n        logger.error(f\"Error checking if file exists {file_path}: {e}\")\n        return False\n\ndef is_extension_allowed(filename: str) -> bool:\n    \"\"\"Check if file extension is allowed.\"\"\"\n    ALLOWED_EXTENSIONS = {\"txt\", \"pdf\", \"png\", \"jpg\", \"jpeg\", \"gif\", \"py\", \"js\", \"html\", \"css\", \"md\", \"doc\", \"docx\", \"xls\", \"xlsx\"}\n    ext = os.path.splitext(filename)[1].lower().lstrip(\".\")\n    return ext in ALLOWED_EXTENSIONS\n\ndef is_mime_allowed(file_stream) -> bool:\n    \"\"\"Check if MIME type is allowed (if 'magic' is installed).\"\"\"\n    if not magic_available:\n        return True\n    ALLOWED_MIME_TYPES = {\n        \"text/plain\", \"text/html\", \"application/pdf\",\n        \"image/png\", \"image/jpeg\", \"text/css\",\n        \"application/javascript\", \"application/json\",\n        \"application/msword\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n        \"application/vnd.ms-excel\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n    }\n    chunk = file_stream.read(1024)\n    file_stream.seek(0)\n    mime_type = magic.from_buffer(chunk, mime=True)\n    return mime_type in ALLOWED_MIME_TYPES","source":"/workspace/modules/blueprints/features/file_utils.py","title":"file_utils.py","language":"en"},{"content":"\"\"\"\nPDF Downloader Blueprint\nDedicated module for handling PDF download operations\nSeparated from web scraper for better modularity\n\"\"\"\n\nfrom flask import Blueprint, request, jsonify, send_from_directory, abort\nimport logging\nimport uuid\nimport time\nimport os\nimport threading\nimport tempfile\nimport hashlib\nimport requests\nfrom typing import Dict, Any, List, Optional, Callable\nfrom pathlib import Path\n\n# Import necessary modules and utilities\nfrom blueprints.core.services import (\n    add_task, get_task, remove_task,\n    structured_error_response, emit_task_error,\n    BaseTask\n)\nfrom blueprints.core.utils import get_output_filepath, sanitize_filename\nfrom blueprints.core.structify_integration import structify_module\nfrom blueprints.features.pdf_processor import analyze_pdf_structure\n\n# Import centralized download function\ntry:\n    from centralized_download_pdf import enhanced_download_pdf\n    centralized_download_available = True\nexcept ImportError:\n    centralized_download_available = False\n\n# Try to import python-magic for file type detection\ntry:\n    import magic\n    magic_available = True\nexcept ImportError:\n    magic_available = False\n\n# Default settings\nDEFAULT_OUTPUT_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'downloads')\n\nlogger = logging.getLogger(__name__)\n\n# Initialize logger if needed\nif not logger.handlers:\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n\n# Create the blueprint\npdf_downloader_bp = Blueprint('pdf_downloader', __name__, url_prefix='/api/pdf')\n\n# Export the blueprint\n__all__ = ['pdf_downloader_bp', 'download_pdf_enhanced', 'batch_download_pdfs']\n\n\nclass PdfDownloadTask(BaseTask):\n    \"\"\"Task for managing PDF downloads\"\"\"\n    \n    def __init__(self, task_id: str):\n        super().__init__(task_id)\n        self.task_type = \"pdf_download\"\n        self.downloads: Dict[str, Dict] = {}\n        self.processing_queue = []\n        self.completed_downloads = []\n        self.failed_downloads = []\n        \n    def add_download(self, url: str, metadata: Dict = None):\n        \"\"\"Add a PDF to the download queue\"\"\"\n        download_info = {\n            'url': url,\n            'status': 'queued',\n            'progress': 0,\n            'metadata': metadata or {},\n            'added_time': time.time()\n        }\n        self.downloads[url] = download_info\n        \n    def start_download(self, url: str, output_folder: str, options: Dict = None):\n        \"\"\"Start downloading a specific PDF\"\"\"\n        if url not in self.downloads:\n            self.add_download(url)\n            \n        download_info = self.downloads[url]\n        download_info['status'] = 'downloading'\n        download_info['start_time'] = time.time()\n        \n        def progress_callback(progress: float, message: str):\n            \"\"\"Update download progress\"\"\"\n            download_info['progress'] = progress\n            download_info['message'] = message\n            \n            # Emit progress update\n            self.emit_progress(progress, f\"Downloading {os.path.basename(url)}: {message}\")\n            \n        try:\n            # Use centralized download if available\n            if centralized_download_available:\n                file_path = enhanced_download_pdf(\n                    url=url,\n                    save_path=output_folder,\n                    task_id=self.task_id,\n                    progress_callback=progress_callback,\n                    timeout=options.get('timeout', 60),\n                    max_file_size_mb=options.get('max_file_size_mb', 100),\n                    max_retries=options.get('max_retries', 3)\n                )\n            else:\n                # Fallback download implementation\n                file_path = self._download_pdf_fallback(url, output_folder, progress_callback)\n                \n            if file_path and os.path.exists(file_path):\n                download_info['status'] = 'completed'\n                download_info['file_path'] = file_path\n                download_info['end_time'] = time.time()\n                download_info['progress'] = 100\n                \n                # Process with Structify if requested\n                if options and options.get('process_with_structify', True):\n                    self._process_with_structify(file_path, output_folder, options)\n                    \n                self.completed_downloads.append(download_info)\n                logger.info(f\"Successfully downloaded PDF: {file_path}\")\n                \n            else:\n                raise ValueError(\"Download failed - file not found\")\n                \n        except Exception as e:\n            logger.error(f\"Error downloading PDF {url}: {e}\")\n            download_info['status'] = 'failed'\n            download_info['error'] = str(e)\n            download_info['end_time'] = time.time()\n            self.failed_downloads.append(download_info)\n            \n    def _download_pdf_fallback(self, url: str, output_folder: str, progress_callback: Callable):\n        \"\"\"Fallback PDF download implementation\"\"\"\n        try:\n            os.makedirs(output_folder, exist_ok=True)\n            \n            # Convert arXiv abstract URLs to PDF URLs\n            if 'arxiv.org/abs/' in url:\n                url = url.replace('/abs/', '/pdf/') + '.pdf'\n                logger.info(f\"Converted arXiv abstract URL to PDF URL: {url}\")\n            \n            # Generate filename\n            url_hash = hashlib.md5(url.encode()).hexdigest()[:10]\n            filename = f\"{os.path.basename(url) or 'document'}_{url_hash}.pdf\"\n            if not filename.endswith('.pdf'):\n                filename += '.pdf'\n            \n            file_path = os.path.join(output_folder, filename)\n            \n            # Check if already exists\n            if os.path.exists(file_path):\n                logger.info(f\"PDF already exists: {file_path}\")\n                progress_callback(100, \"File already exists\")\n                return file_path\n            \n            # Download with proper headers\n            headers = {\n                'User-Agent': 'Mozilla/5.0 (compatible; Academic-Scraper/1.0)',\n                'Accept': 'application/pdf,*/*'\n            }\n            \n            progress_callback(10, \"Starting download...\")\n            \n            response = requests.get(url, headers=headers, timeout=60, stream=True)\n            response.raise_for_status()\n            \n            # Get content length for progress tracking\n            content_length = response.headers.get('Content-Length')\n            total_size = int(content_length) if content_length else 0\n            \n            progress_callback(20, \"Downloading content...\")\n            \n            # Write file in chunks\n            downloaded_size = 0\n            with open(file_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        f.write(chunk)\n                        downloaded_size += len(chunk)\n                        \n                        # Update progress\n                        if total_size > 0:\n                            progress = 20 + (downloaded_size / total_size * 70)\n                            progress_callback(progress, f\"Downloaded {downloaded_size // 1024}KB...\")\n            \n            progress_callback(100, \"Download completed\")\n            logger.info(f\"Successfully downloaded PDF: {file_path}\")\n            return file_path\n            \n        except Exception as e:\n            logger.error(f\"Error downloading PDF {url}: {e}\")\n            return None\n            \n    def _process_with_structify(self, file_path: str, output_folder: str, options: Dict):\n        \"\"\"Process PDF with Structify module\"\"\"\n        try:\n            if not structify_module:\n                logger.warning(\"Structify module not available for PDF processing\")\n                return\n                \n            json_filename = f\"{os.path.splitext(os.path.basename(file_path))[0]}_processed.json\"\n            json_path = os.path.join(output_folder, json_filename)\n            \n            # Use existing Structify integration\n            result = structify_module.process_all_files(\n                root_directory=os.path.dirname(file_path),\n                output_file=json_path,\n                file_filter=lambda f: f == file_path\n            )\n            \n            logger.info(f\"PDF processed with Structify: {json_path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error processing PDF with Structify: {e}\")\n\n\ndef download_pdf_enhanced(url: str, output_folder: str, options: Dict = None) -> Dict[str, Any]:\n    \"\"\"\n    Enhanced PDF download function with task management\n    \n    Args:\n        url: PDF URL to download\n        output_folder: Directory to save the PDF\n        options: Download and processing options\n        \n    Returns:\n        Download result with file path and metadata\n    \"\"\"\n    task_id = str(uuid.uuid4())\n    download_task = PdfDownloadTask(task_id)\n    add_task(task_id, download_task)\n    \n    try:\n        download_task.start_download(url, output_folder, options or {})\n        \n        # Get download result\n        if url in download_task.downloads:\n            download_info = download_task.downloads[url]\n            \n            if download_info['status'] == 'completed':\n                return {\n                    'status': 'success',\n                    'file_path': download_info['file_path'],\n                    'download_time': download_info.get('end_time', 0) - download_info.get('start_time', 0),\n                    'task_id': task_id\n                }\n            else:\n                return {\n                    'status': 'failed',\n                    'error': download_info.get('error', 'Unknown error'),\n                    'task_id': task_id\n                }\n        else:\n            return {\n                'status': 'failed',\n                'error': 'Download not found in task',\n                'task_id': task_id\n            }\n            \n    except Exception as e:\n        logger.error(f\"Error in enhanced PDF download: {e}\")\n        return {\n            'status': 'failed',\n            'error': str(e),\n            'task_id': task_id\n        }\n    finally:\n        remove_task(task_id)\n\n\ndef batch_download_pdfs(urls: List[str], output_folder: str, options: Dict = None) -> str:\n    \"\"\"\n    Download multiple PDFs in batch\n    \n    Args:\n        urls: List of PDF URLs to download\n        output_folder: Directory to save PDFs\n        options: Download and processing options\n        \n    Returns:\n        Task ID for tracking batch download progress\n    \"\"\"\n    task_id = str(uuid.uuid4())\n    download_task = PdfDownloadTask(task_id)\n    add_task(task_id, download_task)\n    \n    # Add all URLs to the task\n    for url in urls:\n        download_task.add_download(url)\n    \n    def download_worker():\n        \"\"\"Background worker for batch downloads\"\"\"\n        try:\n            concurrent_downloads = options.get('concurrent_downloads', 3) if options else 3\n            \n            # Process downloads\n            for i, url in enumerate(urls):\n                try:\n                    download_task.start_download(url, output_folder, options)\n                    \n                    # Update overall progress\n                    overall_progress = ((i + 1) / len(urls)) * 100\n                    download_task.emit_progress(\n                        overall_progress, \n                        f\"Downloaded {i + 1}/{len(urls)} PDFs\"\n                    )\n                    \n                except Exception as e:\n                    logger.error(f\"Error downloading {url}: {e}\")\n                    continue\n            \n            # Mark task as completed\n            download_task.status = \"completed\"\n            download_task.emit_completion({\n                'total_downloads': len(urls),\n                'successful_downloads': len(download_task.completed_downloads),\n                'failed_downloads': len(download_task.failed_downloads),\n                'output_folder': output_folder\n            })\n            \n        except Exception as e:\n            logger.error(f\"Error in batch download worker: {e}\")\n            download_task.emit_error(str(e))\n        finally:\n            remove_task(task_id)\n    \n    # Start background worker\n    thread = threading.Thread(target=download_worker)\n    thread.daemon = True\n    thread.start()\n    \n    return task_id\n\n\n@pdf_downloader_bp.route('/download', methods=['POST'])\ndef api_download_pdf():\n    \"\"\"\n    API endpoint to download a single PDF\n    \n    Expected JSON body:\n    {\n        \"url\": \"https://example.com/paper.pdf\",\n        \"output_folder\": \"path/to/output\",\n        \"options\": {\n            \"process_with_structify\": true,\n            \"extract_tables\": true,\n            \"use_ocr\": true,\n            \"timeout\": 60,\n            \"max_file_size_mb\": 100\n        }\n    }\n    \"\"\"\n    data = request.get_json()\n    if not data:\n        return structured_error_response(\"NO_DATA\", \"No JSON data provided.\", 400)\n    \n    url = data.get(\"url\")\n    output_folder = data.get(\"output_folder\", DEFAULT_OUTPUT_FOLDER)\n    options = data.get(\"options\", {})\n    \n    if not url:\n        return structured_error_response(\"URL_REQUIRED\", \"PDF URL is required.\", 400)\n    \n    try:\n        # Ensure output directory exists\n        os.makedirs(output_folder, exist_ok=True)\n        \n        # Download PDF\n        result = download_pdf_enhanced(url, output_folder, options)\n        \n        if result['status'] == 'success':\n            return jsonify({\n                \"status\": \"success\",\n                \"message\": \"PDF downloaded successfully\",\n                \"file_path\": result['file_path'],\n                \"download_time\": result.get('download_time', 0),\n                \"task_id\": result['task_id']\n            })\n        else:\n            return structured_error_response(\n                \"DOWNLOAD_FAILED\", \n                result.get('error', 'Download failed'), \n                400\n            )\n            \n    except Exception as e:\n        logger.error(f\"Error in PDF download API: {e}\")\n        return structured_error_response(\"DOWNLOAD_ERROR\", f\"Error: {str(e)}\", 500)\n\n\n@pdf_downloader_bp.route('/batch-download', methods=['POST'])\ndef api_batch_download():\n    \"\"\"\n    API endpoint to download multiple PDFs\n    \n    Expected JSON body:\n    {\n        \"urls\": [\"url1\", \"url2\", \"url3\"],\n        \"output_folder\": \"path/to/output\",\n        \"options\": {\n            \"concurrent_downloads\": 3,\n            \"process_with_structify\": true,\n            \"extract_tables\": true,\n            \"use_ocr\": true\n        }\n    }\n    \"\"\"\n    data = request.get_json()\n    if not data:\n        return structured_error_response(\"NO_DATA\", \"No JSON data provided.\", 400)\n    \n    urls = data.get(\"urls\", [])\n    output_folder = data.get(\"output_folder\", DEFAULT_OUTPUT_FOLDER)\n    options = data.get(\"options\", {})\n    \n    if not urls or not isinstance(urls, list):\n        return structured_error_response(\"URLS_REQUIRED\", \"List of URLs is required.\", 400)\n    \n    try:\n        # Ensure output directory exists\n        os.makedirs(output_folder, exist_ok=True)\n        \n        # Start batch download\n        task_id = batch_download_pdfs(urls, output_folder, options)\n        \n        return jsonify({\n            \"status\": \"started\",\n            \"message\": f\"Batch download started for {len(urls)} PDFs\",\n            \"task_id\": task_id,\n            \"urls_count\": len(urls),\n            \"output_folder\": output_folder\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error in batch PDF download API: {e}\")\n        return structured_error_response(\"BATCH_DOWNLOAD_ERROR\", f\"Error: {str(e)}\", 500)\n\n\n@pdf_downloader_bp.route('/status/<task_id>', methods=['GET'])\ndef api_download_status(task_id):\n    \"\"\"Get the status of a PDF download task\"\"\"\n    task = get_task(task_id)\n    if not task or not isinstance(task, PdfDownloadTask):\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"PDF download task {task_id} not found.\", 404)\n    \n    return jsonify({\n        \"task_id\": task.task_id,\n        \"status\": task.status,\n        \"progress\": task.progress,\n        \"downloads\": {\n            \"total\": len(task.downloads),\n            \"completed\": len(task.completed_downloads),\n            \"failed\": len(task.failed_downloads),\n            \"in_progress\": len([d for d in task.downloads.values() if d['status'] == 'downloading'])\n        },\n        \"error\": task.error\n    })\n\n\n@pdf_downloader_bp.route('/cancel/<task_id>', methods=['POST'])\ndef api_cancel_download(task_id):\n    \"\"\"Cancel a PDF download task\"\"\"\n    task = get_task(task_id)\n    if not task or not isinstance(task, PdfDownloadTask):\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"PDF download task {task_id} not found.\", 404)\n    \n    task.status = \"cancelled\"\n    remove_task(task_id)\n    \n    return jsonify({\n        \"task_id\": task_id,\n        \"status\": \"cancelled\",\n        \"message\": \"PDF download task cancelled successfully.\"\n    })\n\n\n@pdf_downloader_bp.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check for PDF downloader module\"\"\"\n    return jsonify({\n        \"status\": \"healthy\",\n        \"module\": \"pdf_downloader\",\n        \"version\": \"1.0.0\",\n        \"features\": {\n            \"single_download\": True,\n            \"batch_download\": True,\n            \"progress_tracking\": True,\n            \"structify_integration\": structify_module is not None,\n            \"centralized_download\": centralized_download_available\n        },\n        \"endpoints\": {\n            \"download\": \"/api/pdf/download\",\n            \"batch_download\": \"/api/pdf/batch-download\",\n            \"status\": \"/api/pdf/status/<task_id>\",\n            \"cancel\": \"/api/pdf/cancel/<task_id>\",\n            \"health\": \"/api/pdf/health\"\n        }\n    })","source":"/workspace/modules/blueprints/features/pdf_downloader.py","title":"pdf_downloader.py","language":"en"},{"content":"\"\"\"\nPDF Processor Blueprint\nHandles all PDF processing, extraction, and analysis functionality\n\"\"\"\n\nfrom flask import Blueprint, request, jsonify, send_file, current_app\nimport logging\nimport os\nimport sys\nimport uuid\nimport time\nimport traceback\nfrom typing import Dict, List, Any, Optional\nfrom werkzeug.utils import secure_filename\nimport threading\n\nlogger = logging.getLogger(__name__)\n\n# Constants\nDEFAULT_OUTPUT_FOLDER = \"downloads\"\n\n# Create the blueprint\npdf_processor_bp = Blueprint('pdf_processor', __name__, url_prefix='/api/pdf-process')\n\n# Import necessary modules and functions\nfrom blueprints.core.services import (\n    get_task, add_task, remove_task, active_tasks, tasks_lock,\n    ProcessingTask, require_api_key\n)\nfrom blueprints.core.utils import (\n    sanitize_filename, ensure_temp_directory, get_output_filepath,\n    structured_error_response\n)\nfrom blueprints.core.ocr_config import pdf_extractor, pdf_extractor_available\nfrom blueprints.core.structify_integration import structify_module, structify_available\n\n# Get shared services from app context\ndef get_limiter():\n    \"\"\"Get limiter from app context\"\"\"\n    if hasattr(current_app, 'limiter'):\n        return current_app.limiter\n    else:\n        # Fallback limiter\n        class MockLimiter:\n            def limit(self, rate_limit):\n                def decorator(f):\n                    return f\n                return decorator\n        return MockLimiter()\n\nlimiter = type('MockLimiter', (), {'limit': lambda self, x: lambda f: f})()  # Placeholder\n\n# Initialize when blueprint is registered\ndef init_blueprint(app):\n    \"\"\"Initialize blueprint with app context services\"\"\"\n    global limiter\n    if hasattr(app, 'limiter'):\n        limiter = app.limiter\n    else:\n        limiter = get_limiter()\n\n# Register the initialization\npdf_processor_bp.record(lambda setup_state: init_blueprint(setup_state.app))\n\n# =============================================================================\n# PDF PROCESSING ROUTES\n# =============================================================================\n\n@pdf_processor_bp.route('/process', methods=['POST'])\n@require_api_key\n@limiter.limit(\"20 per minute\")\ndef process_pdf_endpoint():\n    \"\"\"\n    API endpoint to process a PDF file using structured extraction capabilities.\n    \n    Expected JSON input:\n    {\n        \"pdf_path\": Path to PDF file,\n        \"output_dir\": Output directory (optional),\n        \"extract_tables\": Whether to extract tables (default: true),\n        \"use_ocr\": Whether to use OCR for scanned content (default: true)\n    }\n    \"\"\"\n    if not request.is_json:\n        return structured_error_response(\"INVALID_REQUEST\", \"JSON request expected\", 400)\n    \n    data = request.get_json()\n    pdf_path = data.get('pdf_path')\n    output_dir = data.get('output_dir')\n    extract_tables = data.get('extract_tables', True)\n    use_ocr = data.get('use_ocr', True)\n    \n    if not pdf_path:\n        return structured_error_response(\"PATH_REQUIRED\", \"PDF file path is required\", 400)\n    \n    try:\n        # Validate file existence\n        if not os.path.isfile(pdf_path):\n            return structured_error_response(\"FILE_NOT_FOUND\", f\"PDF file not found: {pdf_path}\", 404)\n        \n        # Validate file is actually a PDF\n        if not pdf_path.lower().endswith('.pdf'):\n            return structured_error_response(\"INVALID_FILE\", \"File is not a PDF\", 400)\n            \n        # Generate output path if needed\n        if output_dir:\n            # Ensure output directory exists\n            os.makedirs(output_dir, exist_ok=True)\n            base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n            output_path = os.path.join(output_dir, f\"{base_name}_processed.json\")\n        else:\n            output_path = None  # Will be derived in process_pdf\n            \n        # Generate a task ID for tracking\n        task_id = str(uuid.uuid4())\n        \n        # Process the PDF in a background thread to avoid blocking\n        def process_pdf_thread():\n            try:\n                # Add task to active_tasks for tracking\n                task_data = {\n                    \"type\": \"pdf_processing\",\n                    \"pdf_path\": pdf_path,\n                    \"output_path\": output_path,\n                    \"start_time\": time.time(),\n                    \"status\": \"processing\"\n                }\n                \n                with tasks_lock:\n                    active_tasks[task_id] = task_data\n                \n                # Emit initial status via SocketIO\n                try:\n                    socketio.emit(\"pdf_processing_start\", {\n                        \"task_id\": task_id,\n                        \"file_path\": pdf_path,\n                        \"file_name\": os.path.basename(pdf_path),\n                        \"status\": \"processing\",\n                        \"timestamp\": time.time()\n                    })\n                except Exception as socket_err:\n                    logger.debug(f\"Socket.IO emission failed: {socket_err}\")\n                \n                # Process the PDF using either pdf_extractor or structify_module\n                if pdf_extractor_available:\n                    result = pdf_extractor.process_pdf(\n                        pdf_path=pdf_path,\n                        output_path=output_path,\n                        extract_tables=extract_tables,\n                        use_ocr=use_ocr,\n                        return_data=True\n                    )\n                elif structify_available and hasattr(structify_module, 'process_pdf'):\n                    result = structify_module.process_pdf(\n                        pdf_path=pdf_path,\n                        output_path=output_path,\n                        max_chunk_size=4096,\n                        extract_tables=extract_tables,\n                        use_ocr=use_ocr,\n                        return_data=True\n                    )\n                else:\n                    result = {\n                        \"status\": \"error\",\n                        \"error\": \"No PDF processing module available\"\n                    }\n                \n                # Update task status\n                with tasks_lock:\n                    if task_id in active_tasks:\n                        active_tasks[task_id][\"status\"] = \"completed\" if result and result.get(\"status\") == \"success\" else \"error\"\n                        active_tasks[task_id][\"end_time\"] = time.time()\n                        active_tasks[task_id][\"result\"] = result\n                \n                # Emit completion event via SocketIO\n                try:\n                    completion_data = {\n                        \"task_id\": task_id,\n                        \"status\": \"completed\" if result and result.get(\"status\") == \"success\" else \"error\",\n                        \"file_path\": pdf_path,\n                        \"output_path\": result.get(\"output_file\", output_path) if result else output_path,\n                        \"timestamp\": time.time()\n                    }\n                    \n                    if result:\n                        # Add additional data for UI\n                        completion_data.update({\n                            \"document_type\": result.get(\"document_type\", \"unknown\"),\n                            \"page_count\": result.get(\"page_count\", 0),\n                            \"tables_count\": len(result.get(\"tables\", [])),\n                            \"references_count\": len(result.get(\"references\", [])),\n                            \"chunks_count\": len(result.get(\"chunks\", [])),\n                            \"processing_time\": result.get(\"processing_info\", {}).get(\"elapsed_seconds\", 0)\n                        })\n                        \n                        if result.get(\"status\") != \"success\":\n                            completion_data[\"error\"] = result.get(\"processing_info\", {}).get(\"error\", \"Unknown error\")\n                    \n                    socketio.emit(\"pdf_processing_complete\", completion_data)\n                except Exception as socket_err:\n                    logger.debug(f\"Socket.IO completion emission failed: {socket_err}\")\n                    \n            except Exception as e:\n                logger.error(f\"Error processing PDF {pdf_path}: {e}\", exc_info=True)\n                \n                # Update task status to error\n                with tasks_lock:\n                    if task_id in active_tasks:\n                        active_tasks[task_id][\"status\"] = \"error\"\n                        active_tasks[task_id][\"error\"] = str(e)\n                        active_tasks[task_id][\"end_time\"] = time.time()\n                \n                # Emit error event\n                try:\n                    socketio.emit(\"pdf_processing_error\", {\n                        \"task_id\": task_id,\n                        \"file_path\": pdf_path,\n                        \"error\": str(e),\n                        \"timestamp\": time.time()\n                    })\n                except Exception as socket_err:\n                    logger.debug(f\"Socket.IO error emission failed: {socket_err}\")\n                    \n                # Remove task from active tasks when finished\n                remove_task(task_id)\n        \n        # Start processing thread\n        thread = threading.Thread(target=process_pdf_thread, daemon=True)\n        thread.start()\n        \n        # Return immediate response with task ID\n        return jsonify({\n            \"status\": \"processing\",\n            \"message\": \"PDF processing started\",\n            \"task_id\": task_id,\n            \"pdf_file\": pdf_path,\n            \"file_name\": os.path.basename(pdf_path)\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error initiating PDF processing for {pdf_path}: {e}\", exc_info=True)\n        return structured_error_response(\"SERVER_ERROR\", f\"PDF processing error: {str(e)}\", 500)\n\n@pdf_processor_bp.route('/extract-tables', methods=['POST'])\n@require_api_key\n@limiter.limit(\"20 per minute\")\ndef extract_pdf_tables():\n    \"\"\"\n    API endpoint to extract tables from a PDF file.\n    \n    Expected JSON input:\n    {\n        \"pdf_path\": \"path/to/file.pdf\",\n        \"page_range\": [1, 5]  # Optional\n    }\n    \"\"\"\n    if not request.is_json:\n        return structured_error_response(\"INVALID_REQUEST\", \"JSON request expected\", 400)\n    \n    data = request.get_json()\n    pdf_path = data.get('pdf_path')\n    page_range = data.get('page_range')\n    \n    if not pdf_path:\n        return structured_error_response(\"PATH_REQUIRED\", \"PDF file path is required\", 400)\n    \n    try:\n        # Check if file exists\n        if not os.path.isfile(pdf_path):\n            return structured_error_response(\"FILE_NOT_FOUND\", f\"PDF file not found: {pdf_path}\", 404)\n        \n        # Check if pdf_extractor module is available\n        if 'pdf_extractor' not in sys.modules:\n            try:\n                import pdf_extractor\n                pdf_extractor_available = True\n            except ImportError:\n                return structured_error_response(\"MODULE_ERROR\", \"PDF processing module not available\", 500)\n        \n        # Convert page_range to tuple if provided\n        page_range_tuple = None\n        if page_range and isinstance(page_range, list) and len(page_range) == 2:\n            page_range_tuple = (int(page_range[0]), int(page_range[1]))\n        \n        # Extract tables with progress tracking\n        start_time = time.time()\n        tables = pdf_extractor.extract_tables_from_pdf(pdf_path, page_range=page_range_tuple)\n        processing_time = time.time() - start_time\n        \n        return jsonify({\n            \"status\": \"success\",\n            \"pdf_path\": pdf_path,\n            \"file_name\": os.path.basename(pdf_path),\n            \"tables_count\": len(tables),\n            \"tables\": tables,\n            \"processing_time\": processing_time\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error extracting tables from PDF {pdf_path}: {e}\", exc_info=True)\n        return structured_error_response(\"SERVER_ERROR\", f\"Table extraction error: {str(e)}\", 500)\n\n\n@pdf_processor_bp.route('/detect-type', methods=['POST'])\n@require_api_key\n@limiter.limit(\"20 per minute\")\ndef detect_pdf_type():\n    \"\"\"\n    API endpoint to detect the type of a PDF document.\n    \n    Expected JSON input:\n    {\n        \"pdf_path\": \"path/to/file.pdf\"\n    }\n    \"\"\"\n    if not request.is_json:\n        return structured_error_response(\"INVALID_REQUEST\", \"JSON request expected\", 400)\n    \n    data = request.get_json()\n    pdf_path = data.get('pdf_path')\n    \n    if not pdf_path:\n        return structured_error_response(\"PATH_REQUIRED\", \"PDF file path is required\", 400)\n    \n    try:\n        # Check if file exists\n        if not os.path.isfile(pdf_path):\n            return structured_error_response(\"FILE_NOT_FOUND\", f\"PDF file not found: {pdf_path}\", 404)\n        \n        # Check if pdf_extractor module is available\n        if 'pdf_extractor' not in sys.modules:\n            try:\n                import pdf_extractor\n                pdf_extractor_available = True\n            except ImportError:\n                return structured_error_response(\"MODULE_ERROR\", \"PDF processing module not available\", 500)\n        \n        # Detect document type\n        start_time = time.time()\n        doc_type = pdf_extractor.detect_document_type(pdf_path)\n        processing_time = time.time() - start_time\n        \n        return jsonify({\n            \"status\": \"success\",\n            \"pdf_path\": pdf_path,\n            \"file_name\": os.path.basename(pdf_path),\n            \"document_type\": doc_type,\n            \"is_scanned\": doc_type == \"scan\",\n            \"processing_time\": processing_time\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error detecting PDF type for {pdf_path}: {e}\", exc_info=True)\n        return structured_error_response(\"SERVER_ERROR\", f\"Type detection error: {str(e)}\", 500)\n\n\n@pdf_processor_bp.route('/analyze', methods=['POST'])\n@require_api_key\n@limiter.limit(\"20 per minute\")\ndef analyze_pdf_endpoint():\n    \"\"\"\n    API endpoint to analyze a PDF file and return comprehensive information about it.\n    \n    Expected JSON input:\n    {\n        \"pdf_path\": \"path/to/file.pdf\"\n    }\n    \n    Returns:\n        JSON with comprehensive PDF analysis\n    \"\"\"\n    if not request.is_json:\n        return structured_error_response(\"INVALID_REQUEST\", \"JSON request expected\", 400)\n    \n    data = request.get_json()\n    pdf_path = data.get('pdf_path')\n    \n    if not pdf_path:\n        return structured_error_response(\"PATH_REQUIRED\", \"PDF file path is required\", 400)\n    \n    try:\n        # Check if file exists\n        if not os.path.isfile(pdf_path):\n            return structured_error_response(\"FILE_NOT_FOUND\", f\"PDF file not found: {pdf_path}\", 404)\n        \n        # Check if pdf_extractor module is available\n        if 'pdf_extractor' not in sys.modules:\n            try:\n                import pdf_extractor\n                pdf_extractor_available = True\n            except ImportError:\n                return structured_error_response(\"MODULE_ERROR\", \"PDF processing module not available\", 500)\n        \n        # Generate a task ID for tracking expensive operations\n        task_id = str(uuid.uuid4())\n        \n        # Use pdf_extractor's get_pdf_summary function if available\n        start_time = time.time()\n        \n        if hasattr(pdf_extractor, 'get_pdf_summary'):\n            summary = pdf_extractor.get_pdf_summary(pdf_path)\n            processing_time = time.time() - start_time\n            \n            # Add additional metadata\n            summary.update({\n                \"status\": \"success\",\n                \"file_name\": os.path.basename(pdf_path),\n                \"processing_time\": processing_time,\n                \"task_id\": task_id\n            })\n            \n            return jsonify(summary)\n        else:\n            # Fallback to basic analysis\n            doc_type = pdf_extractor.detect_document_type(pdf_path)\n            basic_data = pdf_extractor.extract_text_from_pdf(pdf_path)\n            \n            analysis = {\n                \"status\": \"success\",\n                \"task_id\": task_id,\n                \"pdf_path\": pdf_path,\n                \"file_name\": os.path.basename(pdf_path),\n                \"file_size\": os.path.getsize(pdf_path),\n                \"file_size_mb\": round(os.path.getsize(pdf_path) / (1024 * 1024), 2),\n                \"analysis_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"document_type\": doc_type,\n                \"metadata\": basic_data.get(\"metadata\", {}),\n                \"page_count\": basic_data.get(\"page_count\", 0),\n                \"has_scanned_content\": basic_data.get(\"has_scanned_content\", False),\n                \"extraction_method\": basic_data.get(\"extraction_method\", \"unknown\"),\n                \"processing_time\": time.time() - start_time\n            }\n            \n            # Add text preview\n            full_text = basic_data.get(\"full_text\", \"\")\n            if full_text:\n                # Get first few paragraphs\n                paragraphs = re.split(r'\\n\\s*\\n', full_text)\n                preview_text = \"\\n\\n\".join(paragraphs[:3]) if paragraphs else \"\"\n                \n                # Limit to a reasonable size\n                if len(preview_text) > 500:\n                    preview_text = preview_text[:497] + \"...\"\n                    \n                analysis[\"preview\"] = preview_text\n                analysis[\"text_length\"] = len(full_text)\n                analysis[\"word_count\"] = len(re.findall(r'\\b\\w+\\b', full_text))\n                analysis[\"estimated_reading_time_mins\"] = max(1, analysis[\"word_count\"] // 200)\n            \n            return jsonify(analysis)\n        \n    except Exception as e:\n        logger.error(f\"Error analyzing PDF {pdf_path}: {e}\", exc_info=True)\n        return structured_error_response(\"SERVER_ERROR\", f\"PDF analysis error: {str(e)}\", 500)\n\n\n@pdf_processor_bp.route('/batch-process', methods=['POST'])\n@require_api_key\n@limiter.limit(\"20 per minute\")\ndef batch_process_pdfs_endpoint():\n    \"\"\"\n    API endpoint to process multiple PDF files in a batch.\n    \n    Expected JSON input:\n    {\n        \"pdf_files\": [\"path/to/file1.pdf\", \"path/to/file2.pdf\"],\n        \"output_folder\": \"/path/to/output\",\n        \"extract_tables\": true,\n        \"use_ocr\": true\n    }\n    \n    Returns:\n        Task ID for tracking the batch operation\n    \"\"\"\n    if not request.is_json:\n        return structured_error_response(\"INVALID_REQUEST\", \"JSON request expected\", 400)\n    \n    data = request.get_json()\n    pdf_files = data.get('pdf_files', [])\n    output_folder = data.get('output_folder', DEFAULT_OUTPUT_FOLDER)\n    extract_tables = data.get('extract_tables', True)\n    use_ocr = data.get('use_ocr', True)\n    \n    if not pdf_files:\n        return structured_error_response(\"FILES_REQUIRED\", \"List of PDF files is required\", 400)\n    \n    try:\n        # Validate files existence\n        non_existent_files = [f for f in pdf_files if not os.path.isfile(f)]\n        if non_existent_files:\n            return structured_error_response(\n                \"FILES_NOT_FOUND\", \n                f\"Some PDF files not found: {', '.join(os.path.basename(f) for f in non_existent_files[:5])}\" +\n                (f\" and {len(non_existent_files) - 5} more\" if len(non_existent_files) > 5 else \"\"),\n                400\n            )\n        \n        # Ensure output directory exists\n        try:\n            os.makedirs(output_folder, exist_ok=True)\n        except Exception as e:\n            logger.error(f\"Error creating output directory: {e}\")\n            return structured_error_response(\"OUTPUT_DIR_ERROR\", f\"Failed to create output directory: {str(e)}\", 500)\n        \n        # Check if pdf_extractor module is available\n        if 'pdf_extractor' not in sys.modules:\n            try:\n                import pdf_extractor\n                pdf_extractor_available = True\n            except ImportError:\n                return structured_error_response(\"MODULE_ERROR\", \"PDF processing module not available\", 500)\n        \n        # Create a unique task ID\n        task_id = str(uuid.uuid4())\n        \n        # Create and register the task\n        task_data = {\n            \"type\": \"batch_processing\",\n            \"pdf_files\": pdf_files,\n            \"output_folder\": output_folder,\n            \"total_files\": len(pdf_files),\n            \"processed_files\": 0,\n            \"failed_files\": 0,\n            \"start_time\": time.time(),\n            \"status\": \"processing\"\n        }\n        \n        with tasks_lock:\n            active_tasks[task_id] = task_data\n        \n        # Emit initial status\n        try:\n            socketio.emit(\"batch_processing_start\", {\n                \"task_id\": task_id,\n                \"total_files\": len(pdf_files),\n                \"output_folder\": output_folder,\n                \"timestamp\": time.time()\n            })\n        except Exception as socket_err:\n            logger.debug(f\"Socket.IO start emission failed: {socket_err}\")\n        \n        # Start batch processing in a background thread\n        def batch_process_thread():\n            try:\n                if hasattr(pdf_extractor, 'batch_process_pdfs'):\n                    # Use pdf_extractor's batch processing function\n                    result = pdf_extractor.batch_process_pdfs(\n                        pdf_files=pdf_files,\n                        output_folder=output_folder,\n                        extract_tables=extract_tables,\n                        use_ocr=use_ocr\n                    )\n                    \n                    # Update task with results\n                    with tasks_lock:\n                        if task_id in active_tasks:\n                            active_tasks[task_id][\"status\"] = \"completed\"\n                            active_tasks[task_id][\"end_time\"] = time.time()\n                            active_tasks[task_id][\"processed_files\"] = result.get(\"processed_files\", 0)\n                            active_tasks[task_id][\"failed_files\"] = result.get(\"failed_files\", 0)\n                            active_tasks[task_id][\"results\"] = result.get(\"results\", [])\n                    \n                    # Emit completion event\n                    try:\n                        socketio.emit(\"batch_processing_complete\", {\n                            \"task_id\": task_id,\n                            \"status\": \"completed\",\n                            \"output_folder\": output_folder,\n                            \"processed_files\": result.get(\"processed_files\", 0),\n                            \"failed_files\": result.get(\"failed_files\", 0),\n                            \"total_files\": result.get(\"total_files\", 0),\n                            \"success_rate\": result.get(\"success_rate\", 0),\n                            \"processing_time\": result.get(\"elapsed_seconds\", 0),\n                            \"timestamp\": time.time()\n                        })\n                    except Exception as socket_err:\n                        logger.debug(f\"Socket.IO completion emission failed: {socket_err}\")\n                else:\n                    # Fallback to manual processing\n                    processed = 0\n                    failed = 0\n                    results = []\n                    \n                    for i, pdf_file in enumerate(pdf_files):\n                        try:\n                            base_name = os.path.splitext(os.path.basename(pdf_file))[0]\n                            output_path = os.path.join(output_folder, f\"{base_name}_processed.json\")\n                            \n                            start_time = time.time()\n                            result = pdf_extractor.process_pdf(\n                                pdf_path=pdf_file,\n                                output_path=output_path,\n                                extract_tables=extract_tables,\n                                use_ocr=use_ocr,\n                                return_data=True\n                            )\n                            elapsed = time.time() - start_time\n                            \n                            if result and result.get(\"status\") == \"success\":\n                                processed += 1\n                                results.append({\n                                    \"pdf_file\": pdf_file,\n                                    \"output_file\": output_path,\n                                    \"success\": True,\n                                    \"document_type\": result.get(\"document_type\", \"unknown\"),\n                                    \"page_count\": result.get(\"page_count\", 0),\n                                    \"elapsed_seconds\": elapsed\n                                })\n                            else:\n                                failed += 1\n                                results.append({\n                                    \"pdf_file\": pdf_file,\n                                    \"output_file\": output_path,\n                                    \"success\": False,\n                                    \"error\": result.get(\"error\", \"Unknown error\") if result else \"Processing failed\",\n                                    \"elapsed_seconds\": elapsed\n                                })\n                            \n                            # Update task progress\n                            with tasks_lock:\n                                if task_id in active_tasks:\n                                    active_tasks[task_id][\"processed_files\"] = processed\n                                    active_tasks[task_id][\"failed_files\"] = failed\n                            \n                            # Update progress\n                            try:\n                                progress = int((i + 1) / len(pdf_files) * 100)\n                                socketio.emit(\"batch_processing_progress\", {\n                                    \"task_id\": task_id,\n                                    \"progress\": progress,\n                                    \"processed\": processed,\n                                    \"failed\": failed,\n                                    \"total\": len(pdf_files),\n                                    \"current_file\": os.path.basename(pdf_file),\n                                    \"timestamp\": time.time()\n                                })\n                            except Exception as socket_err:\n                                logger.debug(f\"Socket.IO progress emission failed: {socket_err}\")\n                                \n                        except Exception as e:\n                            logger.error(f\"Error processing PDF {pdf_file}: {e}\")\n                            failed += 1\n                            results.append({\n                                \"pdf_file\": pdf_file,\n                                \"success\": False,\n                                \"error\": str(e),\n                                \"elapsed_seconds\": time.time() - start_time\n                            })\n                    \n                    # Update task with results\n                    with tasks_lock:\n                        if task_id in active_tasks:\n                            active_tasks[task_id][\"status\"] = \"completed\"\n                            active_tasks[task_id][\"end_time\"] = time.time()\n                            active_tasks[task_id][\"processed_files\"] = processed\n                            active_tasks[task_id][\"failed_files\"] = failed\n                            active_tasks[task_id][\"results\"] = results\n                    \n                    # Calculate processing time\n                    processing_time = time.time() - active_tasks[task_id][\"start_time\"] if task_id in active_tasks else 0\n                    \n                    # Emit completion event\n                    try:\n                        socketio.emit(\"batch_processing_complete\", {\n                            \"task_id\": task_id,\n                            \"status\": \"completed\",\n                            \"output_folder\": output_folder,\n                            \"processed_files\": processed,\n                            \"failed_files\": failed,\n                            \"total_files\": len(pdf_files),\n                            \"success_rate\": (processed / len(pdf_files) * 100) if pdf_files else 0,\n                            \"processing_time\": processing_time,\n                            \"timestamp\": time.time()\n                        })\n                    except Exception as socket_err:\n                        logger.debug(f\"Socket.IO completion emission failed: {socket_err}\")\n                    \n            except Exception as e:\n                logger.error(f\"Error in batch processing: {e}\", exc_info=True)\n                \n                # Update task status to error\n                with tasks_lock:\n                    if task_id in active_tasks:\n                        active_tasks[task_id][\"status\"] = \"error\"\n                        active_tasks[task_id][\"error\"] = str(e)\n                        active_tasks[task_id][\"end_time\"] = time.time()\n                \n                # Emit error event\n                try:\n                    socketio.emit(\"batch_processing_error\", {\n                        \"task_id\": task_id,\n                        \"error\": str(e),\n                        \"timestamp\": time.time()\n                    })\n                except Exception as socket_err:\n                    logger.debug(f\"Socket.IO error emission failed: {socket_err}\")\n            \n            finally:\n                # Remove task from active tasks when finished\n                remove_task(task_id)\n        \n        # Start thread\n        thread = threading.Thread(target=batch_process_thread, daemon=True)\n        thread.start()\n        \n        # Return immediate response\n        return jsonify({\n            \"status\": \"processing\",\n            \"message\": f\"Batch processing started for {len(pdf_files)} PDF files\",\n            \"task_id\": task_id,\n            \"output_folder\": output_folder,\n            \"total_files\": len(pdf_files)\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error initiating batch processing: {e}\", exc_info=True)\n        return structured_error_response(\"SERVER_ERROR\", f\"Batch processing error: {str(e)}\", 500)\n\n@pdf_processor_bp.route(\"/pdf-capabilities\", methods=[\"GET\"])\ndef get_pdf_capabilities():\n    \"\"\"\n    Get PDF processing capabilities of the server.\n    \n    Returns:\n        JSON response with PDF processing capabilities\n    \"\"\"\n    capabilities = {\n        \"pdf_extraction\": pdf_extractor_available,\n        \"ocr\": 'pytesseract' in sys.modules,\n        \"structify\": structify_available,\n        \"pikepdf\": pikepdf_available,\n        \"table_extraction\": pdf_extractor_available and hasattr(pdf_extractor, 'extract_tables_from_pdf'),\n        \"document_detection\": pdf_extractor_available and hasattr(pdf_extractor, 'detect_document_type'),\n        \"max_file_size\": MAX_FILE_SIZE // (1024 * 1024)  # Convert to MB\n    }\n    \n    return jsonify({\n        \"status\": \"success\",\n        \"capabilities\": capabilities\n    })\n\n@pdf_processor_bp.route('/status/<task_id>', methods=['GET'])\n@require_api_key\ndef pdf_processing_status(task_id):\n    \"\"\"\n    API endpoint to get the status of a PDF processing task.\n    \n    Args:\n        task_id: The ID of the task to check\n    \"\"\"\n    task = get_task(task_id)\n    if not task:\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"Task with ID {task_id} not found\", 404)\n    \n    # Calculate processing time\n    processing_time = time.time() - task.get(\"start_time\", time.time())\n    if \"end_time\" in task:\n        processing_time = task[\"end_time\"] - task[\"start_time\"]\n    \n    # Build response based on task type\n    response = {\n        \"task_id\": task_id,\n        \"status\": task.get(\"status\", \"unknown\"),\n        \"type\": task.get(\"type\", \"unknown\"),\n        \"processing_time\": processing_time\n    }\n    \n    # Add type-specific details\n    if task.get(\"type\") == \"pdf_processing\":\n        response.update({\n            \"pdf_path\": task.get(\"pdf_path\", \"\"),\n            \"output_path\": task.get(\"output_path\", \"\"),\n            \"file_name\": os.path.basename(task.get(\"pdf_path\", \"\"))\n        })\n        \n        # Add result details if available\n        if \"result\" in task:\n            result = task[\"result\"]\n            response.update({\n                \"document_type\": result.get(\"document_type\", \"unknown\"),\n                \"page_count\": result.get(\"page_count\", 0),\n                \"tables_count\": len(result.get(\"tables\", [])),\n                \"references_count\": len(result.get(\"references\", [])),\n                \"chunks_count\": len(result.get(\"chunks\", []))\n            })\n    \n    elif task.get(\"type\") == \"batch_processing\":\n        response.update({\n            \"total_files\": task.get(\"total_files\", 0),\n            \"processed_files\": task.get(\"processed_files\", 0),\n            \"failed_files\": task.get(\"failed_files\", 0),\n            \"output_folder\": task.get(\"output_folder\", \"\"),\n            \"progress\": int((task.get(\"processed_files\", 0) + task.get(\"failed_files\", 0)) / \n                           max(1, task.get(\"total_files\", 1)) * 100)\n        })\n    \n    # Add error if present\n    if \"error\" in task:\n        response[\"error\"] = task[\"error\"]\n    \n    return jsonify(response)\n\n\n@pdf_processor_bp.route('/cancel/<task_id>', methods=['POST'])\n@require_api_key\ndef cancel_pdf_task(task_id):\n    \"\"\"\n    API endpoint to cancel a running PDF processing task.\n    \n    Args:\n        task_id: The ID of the task to cancel\n    \"\"\"\n    task = get_task(task_id)\n    if not task:\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"Task with ID {task_id} not found\", 404)\n    \n    # Check if task is cancellable (not already completed/failed)\n    if task.get(\"status\") in [\"completed\", \"failed\", \"cancelled\"]:\n        return structured_error_response(\"TASK_ALREADY_FINISHED\", \n                                         f\"Task already in state: {task.get('status')}\", 400)\n    \n    # Update task status\n    with tasks_lock:\n        task[\"status\"] = \"cancelled\"\n        task[\"end_time\"] = time.time()\n    \n    # Emit cancellation event\n    try:\n        event_name = \"pdf_processing_cancelled\" if task.get(\"type\") == \"pdf_processing\" else \"batch_processing_cancelled\"\n        socketio.emit(event_name, {\n            \"task_id\": task_id,\n            \"timestamp\": time.time()\n        })\n    except Exception as socket_err:\n        logger.debug(f\"Socket.IO cancellation emission failed: {socket_err}\")\n    \n    # Note: The task will continue running in the background, but we'll ignore its results\n    \n    return jsonify({\n        \"status\": \"success\",\n        \"message\": f\"Task {task_id} cancelled\",\n        \"task_type\": task.get(\"type\", \"unknown\")\n    })\n\n\n@pdf_processor_bp.route('/capabilities', methods=['GET'])\ndef get_capabilities():\n    \"\"\"\n    Get PDF processing capabilities of the server.\n    \n    Returns:\n        JSON response with PDF processing capabilities\n    \"\"\"\n    capabilities = {\n        \"pdf_extraction\": pdf_extractor_available,\n        \"ocr\": 'pytesseract' in sys.modules,\n        \"structify\": structify_available,\n        \"pikepdf\": pikepdf_available,\n        \"table_extraction\": pdf_extractor_available and hasattr(pdf_extractor, 'extract_tables_from_pdf'),\n        \"document_detection\": pdf_extractor_available and hasattr(pdf_extractor, 'detect_document_type'),\n        \"max_file_size\": MAX_FILE_SIZE // (1024 * 1024)  # Convert to MB\n    }\n    \n    return jsonify({\n        \"status\": \"success\",\n        \"capabilities\": capabilities\n    })\n\n\n# =============================================================================\n# HELPER FUNCTIONS\n# =============================================================================\n# ----------------------------------------------------------------------------\n# Enhanced download_pdf Function\n# ----------------------------------------------------------------------------\ndef enhanced_download_pdf(url: str, save_path: str = DEFAULT_OUTPUT_FOLDER, \n                         task_id: Optional[str] = None, \n                         progress_callback: Optional[callable] = None,\n                         timeout: int = 60,\n                         max_file_size_mb: int = 100,\n                         max_retries: int = 3) -> str:\n    \"\"\"\n    Centralized, enhanced PDF download function with all features consolidated.\n    \n    Args:\n        url (str): The URL to download from\n        save_path (str): Directory where the PDF will be saved\n        task_id (Optional[str]): Task ID for progress tracking\n        progress_callback (Optional[callable]): Callback function for progress updates\n        timeout (int): Download timeout in seconds (default: 60)\n        max_file_size_mb (int): Maximum file size in MB (default: 100)\n        max_retries (int): Maximum retry attempts (default: 3)\n        \n    Returns:\n        str: The path to the downloaded PDF file\n        \n    Raises:\n        ValueError: If the download fails\n    \"\"\"\n    # Import requests here to ensure availability\n    try:\n        import requests\n        import hashlib\n    except ImportError:\n        raise ValueError(\"Required libraries (requests, hashlib) not available. Cannot download PDF.\")\n    \n    logger.info(f\"Starting enhanced PDF download: {url}\")\n    if task_id:\n        logger.info(f\"Task ID: {task_id}\")\n    \n    def call_progress(progress: float, message: str):\n        \"\"\"Helper to call progress callback safely\"\"\"\n        if progress_callback:\n            try:\n                progress_callback(progress, message)\n            except Exception as e:\n                logger.debug(f\"Progress callback error: {e}\")\n    \n    call_progress(0, \"Starting PDF download...\")\n    \n    # Convert arXiv abstract links to PDF links if needed\n    if \"arxiv.org/abs/\" in url:\n        pdf_url = url.replace(\"arxiv.org/abs/\", \"arxiv.org/pdf/\")\n        if not pdf_url.lower().endswith(\".pdf\"):\n            pdf_url += \".pdf\"\n        logger.info(f\"Converted arXiv abstract URL to PDF URL: {pdf_url}\")\n    else:\n        pdf_url = url\n    \n    # Ensure the save directory exists\n    try:\n        os.makedirs(save_path, exist_ok=True)\n        logger.info(f\"Ensured download directory exists: {save_path}\")\n    except Exception as e:\n        logger.error(f\"Failed to create download directory {save_path}: {e}\")\n        raise ValueError(f\"Cannot create download directory: {e}\")\n    \n    # Generate a unique filename based on URL\n    url_hash = hashlib.md5(pdf_url.encode()).hexdigest()[:10]\n    filename = pdf_url.split(\"/\")[-1] or \"document.pdf\"\n    if not filename.lower().endswith(\".pdf\"):\n        filename += \".pdf\"\n    # Sanitize the filename and add hash to make it unique\n    filename = sanitize_filename(f\"{os.path.splitext(filename)[0]}_{url_hash}.pdf\")\n    file_path = os.path.join(save_path, filename)\n    \n    # If file already exists, return the path without downloading\n    if os.path.exists(file_path) and os.path.getsize(file_path) > 1000:  # Ensure it's not an empty file\n        logger.info(f\"PDF already exists: {file_path}\")\n        return file_path\n    \n    # Download with retries\n    max_retries = 3\n    for attempt in range(max_retries):\n        try:\n            # Use streaming to handle large files efficiently\n            response = session.get(pdf_url, stream=True, timeout=30, \n                                  headers={\n                                      \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n                                      \"Accept\": \"application/pdf,*/*\",\n                                      \"Connection\": \"keep-alive\"\n                                  })\n            response.raise_for_status()\n            \n            # Check if content type indicates it's a PDF (if available)\n            content_type = response.headers.get('Content-Type', '').lower()\n            if content_type and 'application/pdf' not in content_type and not pdf_url.lower().endswith('.pdf'):\n                logger.warning(f\"Content type not PDF: {content_type}. Checking content...\")\n                # Check first few bytes to see if it starts with %PDF\n                first_chunk = next(response.iter_content(256), None)\n                if not first_chunk or not first_chunk.startswith(b'%PDF-'):\n                    raise ValueError(f\"Content at {pdf_url} is not a PDF\")\n            \n            # Stream content to file\n            with open(file_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=16384):\n                    if chunk:\n                        f.write(chunk)\n            \n            # Verify the file is a valid PDF and has content\n            if not os.path.exists(file_path) or os.path.getsize(file_path) < 1000:\n                if os.path.exists(file_path):\n                    os.remove(file_path)\n                raise ValueError(\"Downloaded file is not a valid PDF (too small)\")\n                \n            logger.info(f\"PDF successfully downloaded to: {file_path}\")\n            return file_path\n            \n        except Exception as e:\n            logger.warning(f\"Attempt {attempt+1}/{max_retries} failed: {e}\")\n            if attempt < max_retries - 1:\n                # Exponential backoff\n                delay = (2 ** attempt) * 1.5\n                logger.info(f\"Retrying in {delay:.1f} seconds...\")\n                time.sleep(delay)\n            else:\n                logger.error(f\"Failed to download PDF after {max_retries} attempts: {e}\")\n                raise ValueError(f\"Failed to download PDF from {pdf_url}: {e}\")\n\ndef analyze_pdf_structure(pdf_file: str) -> Dict[str, Any]:\n    \"\"\"\n    Analyze a PDF file's structure and return a summary of its content.\n    \n    Args:\n        pdf_file: Path to the PDF file\n        \n    Returns:\n        Dict with PDF structure information\n    \"\"\"\n    if not structify_module:\n        return {\"error\": \"Claude module not available for PDF analysis\"}\n    \n    try:\n        summary = {}\n        \n        # Detect document type\n        if hasattr(structify_module, 'detect_document_type'):\n            try:\n                summary[\"document_type\"] = structify_module.detect_document_type(pdf_file)\n            except Exception as e:\n                logger.warning(f\"Error detecting document type: {e}\")\n                summary[\"document_type\"] = \"unknown\"\n        \n        # Extract metadata using PyMuPDF if available\n        if hasattr(structify_module, 'extract_text_from_pdf'):\n            try:\n                pdf_data = structify_module.extract_text_from_pdf(pdf_file)\n                if pdf_data:\n                    summary[\"metadata\"] = pdf_data.get(\"metadata\", {})\n                    summary[\"page_count\"] = pdf_data.get(\"page_count\", 0)\n                    summary[\"has_scanned_content\"] = pdf_data.get(\"has_scanned_content\", False)\n            except Exception as e:\n                logger.warning(f\"Error extracting PDF metadata: {e}\")\n        \n        # Extract tables if document type suggests it might have tables\n        tables = []\n        if hasattr(structify_module, 'extract_tables_from_pdf') and summary.get(\"document_type\") in [\"academic_paper\", \"report\", \"book\"]:\n            try:\n                tables = structify_module.extract_tables_from_pdf(pdf_file)\n                summary[\"tables_count\"] = len(tables)\n                if tables:\n                    # Just include count and page location of tables, not full content\n                    summary[\"tables_info\"] = [\n                        {\"table_id\": t.get(\"table_id\"), \"page\": t.get(\"page\"), \"rows\": t.get(\"rows\"), \"columns\": len(t.get(\"columns\", []))}\n                        for t in tables[:10]  # Limit to first 10 tables\n                    ]\n            except Exception as e:\n                logger.warning(f\"Error extracting tables: {e}\")\n                summary[\"tables_count\"] = 0\n        \n        # Extract structure if available\n        if hasattr(structify_module, 'identify_document_structure') and pdf_data.get(\"full_text\"):\n            try:\n                structure = structify_module.identify_document_structure(\n                    pdf_data[\"full_text\"],\n                    pdf_data.get(\"structure\", {}).get(\"headings\", [])\n                )\n                if structure:\n                    summary[\"sections_count\"] = len(structure.get(\"sections\", []))\n                    # Include section titles for the first few sections\n                    summary[\"section_titles\"] = [\n                        s.get(\"clean_title\", s.get(\"title\", \"Untitled Section\"))\n                        for s in structure.get(\"sections\", [])[:5]  # Limit to first 5 sections\n                    ]\n            except Exception as e:\n                logger.warning(f\"Error identifying document structure: {e}\")\n        \n        # File stats\n        try:\n            file_size = os.path.getsize(pdf_file)\n            summary[\"file_size_bytes\"] = file_size\n            summary[\"file_size_mb\"] = round(file_size / (1024 * 1024), 2)\n        except Exception as e:\n            logger.warning(f\"Error getting file stats: {e}\")\n        \n        return summary\n        \n    except Exception as e:\n        logger.error(f\"Error analyzing PDF structure: {e}\")\n        return {\"error\": str(e)}\n\ndef extract_pdf_preview(pdf_file: str, max_preview_length: int = 2000) -> Dict[str, Any]:\n    \"\"\"\n    Extract a preview of PDF content for display in the UI.\n    \n    Args:\n        pdf_file: Path to the PDF file\n        max_preview_length: Maximum length of text preview\n        \n    Returns:\n        Dict with PDF preview information\n    \"\"\"\n    if not structify_module:\n        return {\"error\": \"Claude module not available for PDF preview\"}\n    \n    try:\n        preview = {}\n        \n        # Extract basic text using PyMuPDF if available\n        if hasattr(structify_module, 'extract_text_from_pdf'):\n            pdf_data = structify_module.extract_text_from_pdf(pdf_file)\n            if pdf_data and pdf_data.get(\"full_text\"):\n                text = pdf_data[\"full_text\"]\n                preview[\"title\"] = pdf_data.get(\"metadata\", {}).get(\"title\", os.path.basename(pdf_file))\n                preview[\"author\"] = pdf_data.get(\"metadata\", {}).get(\"author\", \"Unknown\")\n                preview[\"page_count\"] = pdf_data.get(\"page_count\", 0)\n                \n                # Create a short text preview\n                if len(text) > max_preview_length:\n                    preview[\"text_preview\"] = text[:max_preview_length] + \"...\"\n                else:\n                    preview[\"text_preview\"] = text\n                \n                # Extract first few headings if available\n                if \"structure\" in pdf_data and \"headings\" in pdf_data[\"structure\"]:\n                    preview[\"headings\"] = pdf_data[\"structure\"][\"headings\"][:10]  # First 10 headings\n                \n                return preview\n        \n        # Fallback to simple metadata only if text extraction failed\n        preview[\"title\"] = os.path.basename(pdf_file)\n        preview[\"text_preview\"] = \"PDF preview not available\"\n        \n        return preview\n        \n    except Exception as e:\n        logger.error(f\"Error extracting PDF preview: {e}\")\n        return {\"error\": str(e), \"text_preview\": \"Error generating preview\"}\n\ndef validate_pdf_file(file):\n    \"\"\"Validate uploaded PDF file\"\"\"\n    if not file:\n        return False, \"No file provided\"\n        \n    if file.filename == '':\n        return False, \"No file selected\"\n        \n    if not file.filename.lower().endswith('.pdf'):\n        return False, \"File must be a PDF\"\n        \n    # Check file size (limit to 100MB)\n    file.seek(0, os.SEEK_END)\n    file_size = file.tell()\n    file.seek(0)\n    \n    if file_size > 100 * 1024 * 1024:\n        return False, \"File size exceeds 100MB limit\"\n        \n    return True, None\n\n\ndef save_processing_result(result, original_filename, task_type='processed'):\n    \"\"\"Save processing result to file\"\"\"\n    try:\n        output_path = get_output_filepath(original_filename, task_type)\n        \n        if isinstance(result, dict):\n            # Save as JSON\n            import json\n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(result, f, indent=2, ensure_ascii=False)\n        elif isinstance(result, str):\n            # Save as text\n            with open(output_path, 'w', encoding='utf-8') as f:\n                f.write(result)\n        else:\n            # Save as binary\n            with open(output_path, 'wb') as f:\n                f.write(result)\n                \n        return output_path\n    except Exception as e:\n        logger.error(f\"Error saving result: {e}\")\n        return None\n\ndef process_file(file_path, output_path=None, max_chunk_size=4096, extract_tables=True, use_ocr=True):\n    \"\"\"\n    Process a file using claude.py's enhanced capabilities\n    \n    Args:\n        file_path: Path to the file\n        output_path: Output JSON path (if None, derives from input filename)\n        max_chunk_size: Maximum chunk size for text processing\n        extract_tables: Whether to extract tables (for PDFs)\n        use_ocr: Whether to use OCR for scanned content\n        \n    Returns:\n        Dictionary with success status and processing details\n    \"\"\"\n    if not structify_module:\n        return {\"status\": \"error\", \"error\": \"Claude module not available\"}\n    \n    try:\n        # Check if file exists\n        if not os.path.isfile(file_path):\n            return {\"status\": \"error\", \"error\": f\"File not found: {file_path}\"}\n            \n        # For PDF files, use the specialized PDF handling\n        if file_path.lower().endswith('.pdf'):\n            logger.info(f\"Processing PDF file: {file_path}\")\n            \n            # If output_path is not specified, create a default one\n            if not output_path:\n                base_name = os.path.splitext(os.path.basename(file_path))[0]\n                output_path = os.path.join(os.path.dirname(file_path), f\"{base_name}_processed.json\")\n            \n            # First try direct PDF processing with enhanced features\n            if hasattr(structify_module, 'process_pdf'):\n                try:\n                    # Detect document type to apply proper processing\n                    doc_type = None\n                    if hasattr(structify_module, 'detect_document_type'):\n                        try:\n                            doc_type = structify_module.detect_document_type(file_path)\n                            logger.info(f\"Detected document type: {doc_type}\")\n                        except Exception as type_err:\n                            logger.warning(f\"Error detecting document type: {type_err}\")\n                    \n                    # Apply OCR only if document type is scan or use_ocr is explicitly True\n                    apply_ocr = use_ocr or (doc_type == \"scan\")\n                    \n                    result = structify_module.process_pdf(\n                        pdf_path=file_path, \n                        output_path=output_path,\n                        max_chunk_size=max_chunk_size,\n                        extract_tables=extract_tables,\n                        use_ocr=apply_ocr,\n                        return_data=True\n                    )\n                    \n                    if result:\n                        return {\n                            \"status\": \"success\",\n                            \"file_path\": file_path,\n                            \"output_path\": output_path,\n                            \"data\": result,\n                            \"document_type\": doc_type\n                        }\n                except Exception as pdf_err:\n                    logger.warning(f\"Direct PDF processing failed, falling back: {pdf_err}\")\n            \n            # Fallback to general processing\n            result = structify_module.process_all_files(\n                root_directory=os.path.dirname(file_path),\n                output_file=output_path,\n                max_chunk_size=max_chunk_size,\n                file_filter=lambda f: f == file_path,\n                include_binary_detection=False  # PDFs should not be treated as binary\n            )\n            \n            if result:\n                return {\n                    \"status\": \"success\",\n                    \"file_path\": file_path,\n                    \"output_path\": output_path,\n                    \"data\": result\n                }\n            else:\n                return {\"status\": \"error\", \"error\": \"PDF processing failed\"}\n                \n        else:\n            # For non-PDF files, use the general processing capability\n            logger.info(f\"Processing file: {file_path}\")\n            \n            # If output_path is not specified, create a default one\n            if not output_path:\n                base_name = os.path.splitext(os.path.basename(file_path))[0]\n                output_path = os.path.join(os.path.dirname(file_path), f\"{base_name}_processed.json\")\n            \n            # Use claude.py's general document processing if available\n            if hasattr(structify_module, 'process_document'):\n                result = structify_module.process_document(\n                    file_path=file_path,\n                    output_path=output_path,\n                    max_chunk_size=max_chunk_size,\n                    return_data=True\n                )\n            else:\n                # Fallback to general processing\n                result = structify_module.process_all_files(\n                    root_directory=os.path.dirname(file_path),\n                    output_file=output_path,\n                    max_chunk_size=max_chunk_size,\n                    file_filter=lambda f: f == file_path\n                )\n            \n            if result:\n                return {\n                    \"status\": \"success\",\n                    \"file_path\": file_path,\n                    \"output_path\": output_path,\n                    \"data\": result\n                }\n            else:\n                return {\"status\": \"error\", \"error\": \"File processing failed\"}\n    \n    except Exception as e:\n        logger.error(f\"Error processing file {file_path}: {e}\", exc_info=True)\n        return {\"status\": \"error\", \"error\": str(e)}\n           \n\n\ndef download_pdf(url: str, save_path: str = DEFAULT_OUTPUT_FOLDER) -> str:\n    \"\"\"\n    Download a PDF from the given URL using streaming to save memory.\n    Returns the local file path on success.\n    \n    Args:\n        url (str): The URL to download from\n        save_path (str): Directory where the PDF will be saved\n        \n    Returns:\n        str: The path to the downloaded PDF file\n        \n    Raises:\n        ValueError: If the download fails\n    \"\"\"\n    logger.info(f\"Downloading PDF: {url}\")\n    \n    # Convert arXiv abstract links to PDF links if needed\n    if \"arxiv.org/abs/\" in url:\n        pdf_url = url.replace(\"arxiv.org/abs/\", \"arxiv.org/pdf/\")\n        if not pdf_url.lower().endswith(\".pdf\"):\n            pdf_url += \".pdf\"\n        logger.info(f\"Converted arXiv abstract URL to PDF URL: {pdf_url}\")\n    else:\n        pdf_url = url\n    \n    # Ensure the save directory exists\n    try:\n        os.makedirs(save_path, exist_ok=True)\n        logger.info(f\"Ensured download directory exists: {save_path}\")\n    except Exception as e:\n        logger.error(f\"Failed to create download directory {save_path}: {e}\")\n        raise ValueError(f\"Cannot create download directory: {e}\")\n    \n    # Generate a unique filename based on URL\n    url_hash = hashlib.md5(pdf_url.encode()).hexdigest()[:10]\n    filename = pdf_url.split(\"/\")[-1] or \"document.pdf\"\n    if not filename.lower().endswith(\".pdf\"):\n        filename += \".pdf\"\n    # Sanitize the filename and add hash to make it unique\n    filename = sanitize_filename(f\"{os.path.splitext(filename)[0]}_{url_hash}.pdf\")\n    file_path = os.path.join(save_path, filename)\n    \n    # If file already exists, return the path without downloading\n    if os.path.exists(file_path) and os.path.getsize(file_path) > 1000:  # Ensure it's not an empty file\n        logger.info(f\"PDF already exists: {file_path}\")\n        return file_path\n    \n    # Create a requests session with retries\n    session = requests.Session()\n    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])\n    session.mount('http://', HTTPAdapter(max_retries=retries))\n    session.mount('https://', HTTPAdapter(max_retries=retries))\n    \n    # Download with retries\n    max_retries = 3\n    for attempt in range(max_retries):\n        try:\n            # Use streaming to handle large files efficiently\n            response = session.get(pdf_url, stream=True, timeout=30, \n                                  headers={\n                                      \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n                                      \"Accept\": \"application/pdf,*/*\",\n                                      \"Connection\": \"keep-alive\"\n                                  })\n            response.raise_for_status()\n            \n            # Check if content type indicates it's a PDF (if available)\n            content_type = response.headers.get('Content-Type', '').lower()\n            if content_type and 'application/pdf' not in content_type and not pdf_url.lower().endswith('.pdf'):\n                logger.warning(f\"Content type not PDF: {content_type}. Checking content...\")\n                # Check first few bytes to see if it starts with %PDF\n                first_chunk = next(response.iter_content(256), None)\n                if not first_chunk or not first_chunk.startswith(b'%PDF-'):\n                    raise ValueError(f\"Content at {pdf_url} is not a PDF\")\n            \n            # Stream content to file\n            with open(file_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=16384):\n                    if chunk:\n                        f.write(chunk)\n            \n            # Verify the file is a valid PDF and has content\n            if not os.path.exists(file_path) or os.path.getsize(file_path) < 1000:\n                if os.path.exists(file_path):\n                    os.remove(file_path)\n                raise ValueError(\"Downloaded file is not a valid PDF (too small)\")\n                \n            logger.info(f\"PDF successfully downloaded to: {file_path}\")\n            \n            # Emit Socket.IO event for PDF download progress if using Socket.IO\n            try:\n                socketio.emit(\"pdf_download_progress\", {\n                    \"url\": url,\n                    \"progress\": 100,\n                    \"status\": \"success\",\n                    \"file_path\": file_path\n                })\n            except Exception as socket_err:\n                logger.debug(f\"Socket.IO event emission failed: {socket_err}\")\n            \n            return file_path\n            \n        except Exception as e:\n            logger.warning(f\"Attempt {attempt+1}/{max_retries} failed: {e}\")\n            if attempt < max_retries - 1:\n                # Exponential backoff\n                delay = (2 ** attempt) * 1.5\n                logger.info(f\"Retrying in {delay:.1f} seconds...\")\n                time.sleep(delay)\n            else:\n                logger.error(f\"Failed to download PDF after {max_retries} attempts: {e}\")\n                raise ValueError(f\"Failed to download PDF from {pdf_url}: {e}\")\n\ndef handle_pdf_processing_error(pdf_file, error, output_folder=None):\n    \"\"\"\n    Handle PDF processing errors with recovery options\n    \n    Args:\n        pdf_file: Path to the problematic PDF\n        error: The exception or error message\n        output_folder: Optional output folder to save error report\n        \n    Returns:\n        Dict with error information and recovery status\n    \"\"\"\n    error_type = type(error).__name__\n    error_msg = str(error)\n    \n    logger.error(f\"PDF processing error ({error_type}): {error_msg}\")\n    \n    # Attempt to classify the error\n    if \"memory\" in error_msg.lower() or \"allocation\" in error_msg.lower():\n        error_category = \"memory\"\n    elif \"timeout\" in error_msg.lower() or \"timed out\" in error_msg.lower():\n        error_category = \"timeout\" \n    elif \"permission\" in error_msg.lower() or \"access\" in error_msg.lower():\n        error_category = \"permissions\"\n    elif \"corrupt\" in error_msg.lower() or \"invalid\" in error_msg.lower():\n        error_category = \"corrupt_file\"\n    else:\n        error_category = \"general\"\n    \n    # Try recovery based on error type\n    recovery_successful = False\n    recovery_method = None\n    \n    try:\n        if error_category == \"memory\":\n            # Try processing with reduced memory usage\n            logger.info(\"Attempting memory-optimized processing...\")\n            recovery_successful = process_pdf_with_reduced_memory(pdf_file, output_folder)\n            recovery_method = \"reduced_memory\"\n            \n        elif error_category == \"corrupt_file\":\n            # Try PDF repair methods\n            logger.info(\"Attempting PDF repair...\")\n            recovery_successful = attempt_pdf_repair(pdf_file, output_folder)\n            recovery_method = \"file_repair\"\n            \n        elif error_category == \"timeout\":\n            # Try processing with extended timeout\n            logger.info(\"Attempting processing with extended timeout...\")\n            recovery_successful = process_pdf_with_extended_timeout(pdf_file, output_folder)\n            recovery_method = \"extended_timeout\"\n    except Exception as recovery_error:\n        logger.error(f\"Recovery attempt failed: {recovery_error}\")\n    \n    # Create error report\n    result = {\n        \"status\": \"error\",\n        \"error_type\": error_type,\n        \"error_message\": error_msg,\n        \"error_category\": error_category,\n        \"recovery_attempted\": True,\n        \"recovery_successful\": recovery_successful,\n        \"recovery_method\": recovery_method\n    }\n    \n    # Save error report if output folder provided\n    if output_folder:\n        try:\n            report_path = os.path.join(\n                output_folder,\n                f\"error_report_{os.path.basename(pdf_file)}.json\"\n            )\n            \n            with open(report_path, 'w', encoding='utf-8') as f:\n                json.dump(result, f, indent=2)\n                \n            result[\"error_report\"] = report_path\n        except Exception as e:\n            logger.warning(f\"Failed to save error report: {e}\")\n    \n    return result\n\ndef process_pdf_with_reduced_memory(pdf_file, output_folder):\n    \"\"\"Process PDF with reduced memory usage\"\"\"\n    try:\n        # Generate output path\n        file_name = os.path.basename(pdf_file)\n        json_filename = os.path.splitext(file_name)[0] + \"_processed.json\"\n        json_path = os.path.join(output_folder, json_filename)\n        \n        # Process in chunks to reduce memory usage\n        if hasattr(structify_module, 'process_pdf'):\n            structify_module.process_pdf(\n                pdf_path=pdf_file,\n                output_path=json_path,\n                max_chunk_size=2048,  # Smaller chunks\n                extract_tables=False,  # Skip tables to save memory\n                use_ocr=False,  # Skip OCR to save memory\n                return_data=False  # Don't keep data in memory\n            )\n            return True\n        return False\n    except Exception as e:\n        logger.error(f\"Reduced memory processing failed: {e}\")\n        return False\n\ndef attempt_pdf_repair(pdf_file, output_folder):\n    \"\"\"Attempt to repair corrupted PDF file\"\"\"\n    if not pikepdf_available:\n        logger.warning(\"pikepdf not available for PDF repair\")\n        return False\n        \n    try:\n        # Now use pikepdf knowing it's available\n        import pikepdf\n        # Rest of your function...\n    except Exception as e:\n        logger.error(f\"PDF repair failed: {e}\")\n        return False\n\ndef validate_pdf(pdf_path):\n    \"\"\"\n    Validate a PDF file and detect its features.\n    \n    Args:\n        pdf_path: Path to the PDF file\n        \n    Returns:\n        dict: Validation results and detected features\n    \"\"\"\n    if not os.path.exists(pdf_path):\n        return {\"valid\": False, \"error\": \"File not found\"}\n        \n    if not pdf_path.lower().endswith('.pdf'):\n        return {\"valid\": False, \"error\": \"Not a PDF file\"}\n    \n    # Check if file is readable\n    try:\n        with open(pdf_path, 'rb') as f:\n            header = f.read(5)\n            if header != b'%PDF-':\n                return {\"valid\": False, \"error\": \"Invalid PDF format\"}\n    except Exception as e:\n        return {\"valid\": False, \"error\": f\"Error reading file: {str(e)}\"}\n    \n    # Try to extract basic features\n    features = {\"valid\": True, \"encrypted\": False, \"page_count\": 0, \"scanned\": False}\n    \n    try:\n        if pdf_extractor_available:\n            # Use pdf_extractor for feature detection\n            doc_type = pdf_extractor.detect_document_type(pdf_path)\n            features[\"document_type\"] = doc_type\n            features[\"scanned\"] = doc_type == \"scan\"\n            \n            # Get more metadata\n            metadata = pdf_extractor.extract_text_from_pdf(pdf_path)\n            if metadata:\n                features[\"page_count\"] = metadata.get(\"page_count\", 0)\n                features[\"has_text\"] = bool(metadata.get(\"full_text\"))\n                features[\"metadata\"] = metadata.get(\"metadata\", {})\n        elif pikepdf_available:\n            # Use pikepdf as fallback\n            with pikepdf.Pdf.open(pdf_path) as pdf:\n                features[\"page_count\"] = len(pdf.pages)\n                features[\"encrypted\"] = pdf.is_encrypted\n                features[\"version\"] = f\"{pdf.pdf_version.major}.{pdf.pdf_version.minor}\"\n    except Exception as e:\n        # Don't fail validation if feature detection fails\n        features[\"feature_error\"] = str(e)\n    \n    return features\n# Export the blueprint\n__all__ = [\n    'pdf_processor_bp',\n    'init_blueprint',\n    'get_limiter'\n]","source":"/workspace/modules/blueprints/features/pdf_processor.py","title":"pdf_processor.py","language":"en"},{"content":"\"\"\"\nPlaylist Downloader Blueprint\nHandles YouTube playlist downloading functionality\n\"\"\"\n\nfrom flask import Blueprint, request, jsonify\nfrom flask_socketio import emit\nfrom werkzeug.utils import secure_filename\nimport logging\nimport uuid\nimport time\nimport os\n\n# Import necessary modules and functions\nfrom blueprints.core.services import (\n    get_task, add_task, remove_task, active_tasks, tasks_lock,\n    ProcessingTask, PlaylistTask\n)\nfrom blueprints.core.utils import (\n    sanitize_filename, ensure_temp_directory, get_output_filepath,\n    structured_error_response, normalize_path\n)\n\n# Get YouTube API key from environment\nYOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY', '')\n\nlogger = logging.getLogger(__name__)\n\n# Create the blueprint\nplaylist_downloader_bp = Blueprint('playlist_downloader', __name__, url_prefix='/api')\n\n# Export the blueprint and utility functions\n__all__ = ['playlist_downloader_bp', 'emit_download_progress', 'emit_download_completed', 'emit_download_error']\n\n@playlist_downloader_bp.route('/start-playlists', methods=['POST'])\ndef start_playlists():\n    \"\"\"\n    Enhanced handler for starting playlist downloads with improved validation,\n    error handling, and path resolution.\n    \n    The route delegates output path resolution to the PlaylistTask class\n    for consistency and cleaner separation of concerns.\n    \n    Returns:\n        JSON response with task details or error information\n    \"\"\"\n    # Check if YouTube API key is configured\n    if not YOUTUBE_API_KEY:\n        logger.error(\"YouTube API key not configured\")\n        return structured_error_response(\n            \"API_KEY_MISSING\", \n            \"YouTube API key is not configured. Please set YOUTUBE_API_KEY in your .env file.\",\n            500\n        )\n    \n    # Get and validate request JSON\n    try:\n        data = request.get_json()\n        if not data:\n            return structured_error_response(\"NO_DATA\", \"No JSON data provided.\", 400)\n    except Exception as e:\n        logger.error(f\"Invalid JSON in request: {str(e)}\")\n        return structured_error_response(\"INVALID_JSON\", f\"Invalid JSON format: {str(e)}\", 400)\n    \n    # Extract and validate required parameters\n    raw_playlists = data.get(\"playlists\")\n    root_directory = data.get(\"root_directory\")\n    output_file = data.get(\"output_file\")\n    \n    # Validate playlist URLs\n    if not raw_playlists or not isinstance(raw_playlists, list):\n        return structured_error_response(\"PLAYLISTS_REQUIRED\", \"A list of playlist URLs is required.\", 400)\n    \n    # Validate each playlist URL format\n    invalid_urls = [url for url in raw_playlists if not url or 'list=' not in url]\n    if invalid_urls:\n        return structured_error_response(\n            \"INVALID_PLAYLIST_URLS\", \n            f\"Found {len(invalid_urls)} invalid playlist URLs. Each URL must contain 'list=' parameter.\",\n            400,\n            details={\"invalid_urls\": invalid_urls[:5]}  # Show up to 5 invalid URLs\n        )\n    \n    # Validate root directory\n    if not root_directory:\n        return structured_error_response(\"ROOT_DIR_REQUIRED\", \"Root directory is required.\", 400)\n    \n    # Validate output file\n    if not output_file:\n        return structured_error_response(\"OUTPUT_FILE_REQUIRED\", \"Output file is required.\", 400)\n    \n    # Normalize root directory path\n    try:\n        root_directory = normalize_path(root_directory)\n    except Exception as e:\n        logger.error(f\"Failed to normalize root directory path: {str(e)}\")\n        return structured_error_response(\n            \"INVALID_ROOT_DIR\", \n            f\"Invalid root directory path: {str(e)}\", \n            400\n        )\n    \n    # Create playlist configurations with sanitized folder names\n    try:\n        playlists = []\n        for idx, url in enumerate(raw_playlists):\n            # Create a folder name based on playlist index and extract playlist ID if possible\n            playlist_id = None\n            if 'list=' in url:\n                try:\n                    playlist_id = url.split('list=')[1].split('&')[0]\n                    playlist_folder = f\"playlist_{idx+1}_{playlist_id}\"\n                except:\n                    playlist_folder = f\"playlist_{idx+1}\"\n            else:\n                playlist_folder = f\"playlist_{idx+1}\"\n            \n            # Sanitize the folder name and create full path\n            sanitized_folder = secure_filename(playlist_folder)\n            full_folder_path = os.path.join(root_directory, sanitized_folder)\n            \n            playlists.append({\n                \"url\": url,\n                \"folder\": full_folder_path,\n                \"playlist_id\": playlist_id  # Store playlist ID for reference\n            })\n            \n        logger.debug(f\"Created {len(playlists)} playlist configurations\")\n    except Exception as e:\n        logger.error(f\"Failed to create playlist configurations: {str(e)}\")\n        return structured_error_response(\n            \"CONFIG_ERROR\", \n            f\"Failed to create playlist configurations: {str(e)}\", \n            500\n        )\n    \n    # Create task ID and instantiate playlist task\n    task_id = str(uuid.uuid4())\n    \n    try:\n        # Create task and register it in task manager\n        playlist_task = PlaylistTask(task_id)\n        add_task(task_id, playlist_task)\n        logger.info(f\"Created playlist task with ID: {task_id}\")\n        \n        # Try to create root directory (PlaylistTask will handle playlist folders)\n        try:\n            os.makedirs(root_directory, exist_ok=True)\n            logger.debug(f\"Ensured root directory exists: {root_directory}\")\n        except Exception as dir_err:\n            logger.error(f\"Failed to create root directory: {str(dir_err)}\")\n            remove_task(task_id)  # Clean up task on failure\n            return structured_error_response(\n                \"ROOT_DIR_CREATION_ERROR\", \n                f\"Failed to create root directory: {str(dir_err)}\", \n                500\n            )\n        \n        # Start the playlist task with the original output file parameter\n        # The task will handle path resolution for consistency\n        start_result = playlist_task.start(playlists, root_directory, output_file)\n        \n        # If task start returns an error status, clean up and return the error\n        if start_result.get(\"status\") == \"failed\":\n            logger.error(f\"Task start failed: {start_result.get('error')}\")\n            remove_task(task_id)\n            return structured_error_response(\n                \"TASK_START_ERROR\", \n                start_result.get(\"error\", \"Unknown task start error\"), \n                500\n            )\n        \n        # Include task info in response for client use\n        response_data = {\n            \"task_id\": task_id,\n            \"status\": \"processing\",\n            \"message\": \"Playlist processing started\",\n            \"playlists_count\": len(playlists),\n            \"root_directory\": root_directory,\n            \"output_file\": start_result.get(\"output_file\", \"\")\n        }\n        \n        # Emit task creation event via Socket.IO for real-time updates\n        try:\n            # Use enhanced Socket.IO function if available\n            if 'emit_task_started' in globals():\n                emit_task_started(\n                    task_id=task_id,\n                    task_type=\"playlist_download\",\n                    message=f\"Starting download of {len(playlists)} playlists\",\n                    details={\"playlists_count\": len(playlists)}\n                )\n            # Fallback to direct socketio emission\n            else:\n                socketio.emit('task_started', {\n                    'task_id': task_id,\n                    'task_type': \"playlist_download\",\n                    'status': 'processing',\n                    'message': f\"Starting download of {len(playlists)} playlists\",\n                    'timestamp': time.time()\n                })\n        except Exception as socketio_err:\n            # Log but don't fail if Socket.IO emission fails\n            logger.error(f\"Failed to emit task_started event: {str(socketio_err)}\")\n        \n        return jsonify(response_data)\n        \n    except Exception as e:\n        logger.error(f\"Failed to start playlist task: {str(e)}\", exc_info=True)\n        # Ensure task is removed from task manager on failure\n        remove_task(task_id)\n        return structured_error_response(\n            \"TASK_CREATION_ERROR\", \n            f\"Failed to create and start playlist task: {str(e)}\", \n            500\n        )\n\n@playlist_downloader_bp.route('/cancel-playlists/<task_id>', methods=['POST'])\ndef cancel_playlists(task_id):\n    \"\"\"\n    Cancel a running playlist download task.\n    \n    Args:\n        task_id: The ID of the playlist task to cancel\n        \n    Returns:\n        JSON response indicating cancellation status\n    \"\"\"\n    try:\n        # Get the task from active tasks\n        task = get_task(task_id)\n        \n        if not task:\n            return structured_error_response(\n                \"TASK_NOT_FOUND\", \n                f\"Playlist task with ID {task_id} not found\", \n                404\n            )\n        \n        # Check if it's a playlist task\n        if not isinstance(task, PlaylistTask):\n            return structured_error_response(\n                \"INVALID_TASK_TYPE\", \n                f\"Task {task_id} is not a playlist download task\", \n                400\n            )\n        \n        # Check if task is already in a terminal state\n        if hasattr(task, 'status') and task.status in ['completed', 'failed', 'cancelled']:\n            return jsonify({\n                \"status\": \"already_finished\",\n                \"message\": f\"Task {task_id} already {task.status}\",\n                \"task_id\": task_id,\n                \"task_status\": task.status\n            })\n        \n        # Cancel the task\n        try:\n            # Set cancellation flag if available\n            if hasattr(task, 'cancel'):\n                task.cancel()\n            elif hasattr(task, 'is_cancelled_flag'):\n                task.is_cancelled_flag = True\n            \n            # Update task status\n            if hasattr(task, 'status'):\n                task.status = 'cancelled'\n            \n            # Remove from active tasks\n            remove_task(task_id)\n            \n            # Emit cancellation event\n            try:\n                emit_download_error(\n                    task_id=task_id,\n                    error_message=\"Task cancelled by user request\"\n                )\n            except Exception as e:\n                logger.warning(f\"Failed to emit cancellation event: {e}\")\n            \n            logger.info(f\"Successfully cancelled playlist task {task_id}\")\n            \n            return jsonify({\n                \"status\": \"success\",\n                \"message\": f\"Playlist task {task_id} cancelled successfully\",\n                \"task_id\": task_id,\n                \"cancelled_at\": time.time()\n            })\n            \n        except Exception as cancel_error:\n            logger.error(f\"Error during task cancellation: {cancel_error}\")\n            return structured_error_response(\n                \"CANCELLATION_ERROR\", \n                f\"Failed to cancel task: {str(cancel_error)}\", \n                500\n            )\n        \n    except Exception as e:\n        logger.error(f\"Error cancelling playlist task {task_id}: {e}\")\n        return structured_error_response(\n            \"CANCEL_REQUEST_ERROR\", \n            f\"Error processing cancellation request: {str(e)}\", \n            500\n        )\n\ndef structured_error_response(error_code, error_message, status_code=400, details=None):\n    \"\"\"\n    Create a structured error response with consistent format.\n    \n    Args:\n        error_code: String code for machine-readable error identification\n        error_message: Human-readable error description\n        status_code: HTTP status code to return\n        details: Optional dict with additional error context\n        \n    Returns:\n        Flask response with JSON error data\n    \"\"\"\n    error_data = {\n        \"error\": {\n            \"code\": error_code,\n            \"message\": error_message,\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        },\n        \"status\": \"error\"\n    }\n    \n    # Add details if provided\n    if details:\n        error_data[\"error\"][\"details\"] = details\n    \n    return jsonify(error_data), status_code\n    \ndef format_output_path(root_directory, output_file):\n    \"\"\"\n    Properly format the output path by ensuring we don't create paths with multiple drive letters.\n    \n    Args:\n        root_directory (str): Root directory for the playlist download\n        output_file (str): The target output filename (with or without path)\n        \n    Returns:\n        str: A correctly formatted absolute path\n    \"\"\"\n    # If output_file already has a drive letter, use it as is\n    if re.match(r'^[A-Za-z]:', output_file):\n        return output_file\n        \n    # Otherwise join with root directory\n    return os.path.join(root_directory, os.path.basename(output_file))\n\n\n# Socket.IO events for playlist downloader\ndef emit_download_progress(task_id, progress, current_video=None, downloaded=0, total=0):\n    \"\"\"Emit download progress update\"\"\"\n    try:\n        payload = {\n            'task_id': task_id,\n            'progress': progress,\n            'status': 'downloading',\n            'current_video': current_video,\n            'downloaded': downloaded,\n            'total': total,\n            'timestamp': time.time()\n        }\n        \n        emit('download_progress', payload, broadcast=True)\n        logger.debug(f\"Emitted download progress for task {task_id}: {progress}%\")\n        \n    except Exception as e:\n        logger.error(f\"Error emitting download progress: {str(e)}\")\n\n\ndef emit_download_completed(task_id, downloaded_files=None, stats=None):\n    \"\"\"Emit download completion event\"\"\"\n    try:\n        payload = {\n            'task_id': task_id,\n            'status': 'completed',\n            'downloaded_files': downloaded_files or [],\n            'stats': stats or {},\n            'timestamp': time.time()\n        }\n        \n        emit('download_completed', payload, broadcast=True)\n        logger.info(f\"Emitted download completion for task {task_id}\")\n        \n    except Exception as e:\n        logger.error(f\"Error emitting download completion: {str(e)}\")\n\n\ndef emit_download_error(task_id, error_message, current_video=None):\n    \"\"\"Emit download error event\"\"\"\n    try:\n        payload = {\n            'task_id': task_id,\n            'status': 'error',\n            'error': error_message,\n            'current_video': current_video,\n            'timestamp': time.time()\n        }\n        \n        emit('download_error', payload, broadcast=True)\n        logger.error(f\"Emitted download error for task {task_id}: {error_message}\")\n        \n    except Exception as e:\n        logger.error(f\"Error emitting download error: {str(e)}\")","source":"/workspace/modules/blueprints/features/playlist_downloader.py","title":"playlist_downloader.py","language":"en"},{"content":"\"\"\"\nWeb Crawler Module\nImplements recursive web crawling functionality for the Web Scraper blueprint\n\"\"\"\n\nimport logging\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse, urlunparse\nfrom urllib.robotparser import RobotFileParser\nimport time\nfrom typing import Set, List, Dict, Optional, Callable\nfrom collections import deque\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport hashlib\n\nlogger = logging.getLogger(__name__)\n\n\nclass WebCrawler:\n    \"\"\"\n    Advanced web crawler with recursive crawling capabilities.\n    \n    Features:\n    - Depth-first and breadth-first crawling\n    - Robots.txt compliance\n    - Domain restriction options\n    - Rate limiting and throttling\n    - Duplicate URL detection\n    - PDF link extraction\n    - Progress tracking\n    \"\"\"\n    \n    def __init__(self, \n                 max_depth: int = 3,\n                 max_pages: int = 100,\n                 respect_robots: bool = True,\n                 follow_redirects: bool = True,\n                 request_delay: float = 0.5,\n                 timeout: int = 30,\n                 max_workers: int = 5):\n        \"\"\"\n        Initialize the web crawler.\n        \n        Args:\n            max_depth: Maximum crawling depth\n            max_pages: Maximum pages to crawl per domain\n            respect_robots: Whether to respect robots.txt\n            follow_redirects: Whether to follow HTTP redirects\n            request_delay: Delay between requests to same domain (seconds)\n            timeout: Request timeout in seconds\n            max_workers: Maximum concurrent workers\n        \"\"\"\n        self.max_depth = max_depth\n        self.max_pages = max_pages\n        self.respect_robots = respect_robots\n        self.follow_redirects = follow_redirects\n        self.request_delay = request_delay\n        self.timeout = timeout\n        self.max_workers = max_workers\n        \n        # Crawling state\n        self.visited_urls: Set[str] = set()\n        self.url_queue: deque = deque()\n        self.pdf_links: List[Dict[str, str]] = []\n        self.scraped_data: Dict[str, Dict] = {}\n        self.domain_last_access: Dict[str, float] = {}\n        self.robots_cache: Dict[str, RobotFileParser] = {}\n        \n        # Statistics\n        self.stats = {\n            'pages_crawled': 0,\n            'pdfs_found': 0,\n            'errors': 0,\n            'total_bytes': 0,\n            'start_time': None,\n            'end_time': None\n        }\n        \n        # Control flags\n        self.is_cancelled = False\n        self.lock = threading.RLock()\n        \n        # Session for connection pooling\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'NeuroGenBot/1.0 (+https://neurogen.ai/bot)'\n        })\n    \n    def crawl(self, \n              start_url: str,\n              progress_callback: Optional[Callable] = None,\n              pdf_callback: Optional[Callable] = None,\n              stay_in_domain: bool = True) -> Dict[str, any]:\n        \"\"\"\n        Start crawling from the given URL.\n        \n        Args:\n            start_url: URL to start crawling from\n            progress_callback: Function to call with progress updates\n            pdf_callback: Function to call when PDF is found\n            stay_in_domain: Whether to restrict crawling to start domain\n            \n        Returns:\n            Dictionary with crawling results\n        \"\"\"\n        self.stats['start_time'] = time.time()\n        start_domain = urlparse(start_url).netloc\n        \n        # Initialize queue with start URL\n        self.url_queue.append((start_url, 0))  # (url, depth)\n        \n        try:\n            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n                futures = []\n                \n                while self.url_queue and not self.is_cancelled:\n                    if self.stats['pages_crawled'] >= self.max_pages:\n                        logger.info(f\"Reached maximum pages limit: {self.max_pages}\")\n                        break\n                    \n                    # Get next URL to crawl\n                    url, depth = self._get_next_url()\n                    if not url:\n                        continue\n                    \n                    # Check domain restriction\n                    if stay_in_domain and urlparse(url).netloc != start_domain:\n                        continue\n                    \n                    # Submit crawling task\n                    future = executor.submit(\n                        self._crawl_page, \n                        url, \n                        depth, \n                        stay_in_domain, \n                        start_domain,\n                        progress_callback,\n                        pdf_callback\n                    )\n                    futures.append(future)\n                    \n                    # Process completed futures\n                    for completed in list(futures):\n                        if completed.done():\n                            try:\n                                completed.result()\n                            except Exception as e:\n                                logger.error(f\"Crawling error: {e}\")\n                                self.stats['errors'] += 1\n                            futures.remove(completed)\n                \n                # Wait for remaining futures\n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception as e:\n                        logger.error(f\"Crawling error: {e}\")\n                        self.stats['errors'] += 1\n                        \n        except Exception as e:\n            logger.error(f\"Critical crawling error: {e}\")\n            \n        finally:\n            self.stats['end_time'] = time.time()\n            self.session.close()\n        \n        return self._get_results()\n    \n    def _get_next_url(self) -> Optional[tuple]:\n        \"\"\"Get next URL from queue with thread safety.\"\"\"\n        with self.lock:\n            if self.url_queue:\n                return self.url_queue.popleft()\n            return None, None\n    \n    def _crawl_page(self, \n                    url: str, \n                    depth: int,\n                    stay_in_domain: bool,\n                    start_domain: str,\n                    progress_callback: Optional[Callable],\n                    pdf_callback: Optional[Callable]) -> None:\n        \"\"\"\n        Crawl a single page.\n        \n        Args:\n            url: URL to crawl\n            depth: Current crawling depth\n            stay_in_domain: Whether to stay in start domain\n            start_domain: Original domain\n            progress_callback: Progress callback function\n            pdf_callback: PDF discovery callback function\n        \"\"\"\n        # Check if already visited\n        with self.lock:\n            if url in self.visited_urls:\n                return\n            self.visited_urls.add(url)\n        \n        # Check robots.txt\n        if self.respect_robots and not self._can_fetch(url):\n            logger.info(f\"Robots.txt disallows: {url}\")\n            return\n        \n        # Rate limiting\n        self._apply_rate_limit(url)\n        \n        try:\n            # Fetch the page\n            response = self.session.get(\n                url, \n                timeout=self.timeout,\n                allow_redirects=self.follow_redirects\n            )\n            response.raise_for_status()\n            \n            # Update statistics\n            with self.lock:\n                self.stats['pages_crawled'] += 1\n                self.stats['total_bytes'] += len(response.content)\n            \n            # Parse the page\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            # Extract page data\n            page_data = self._extract_page_data(url, soup, response)\n            with self.lock:\n                self.scraped_data[url] = page_data\n            \n            # Find and process links\n            if depth < self.max_depth:\n                links = self._extract_links(soup, url)\n                \n                for link in links:\n                    # Check if it's a PDF\n                    if self._is_pdf_link(link):\n                        pdf_info = {\n                            'url': link,\n                            'source_page': url,\n                            'title': self._extract_link_title(soup, link),\n                            'depth': depth\n                        }\n                        \n                        with self.lock:\n                            self.pdf_links.append(pdf_info)\n                            self.stats['pdfs_found'] += 1\n                        \n                        if pdf_callback:\n                            pdf_callback(pdf_info)\n                    else:\n                        # Add to crawl queue if not visited\n                        with self.lock:\n                            if link not in self.visited_urls:\n                                # Check domain restriction\n                                if not stay_in_domain or urlparse(link).netloc == start_domain:\n                                    self.url_queue.append((link, depth + 1))\n            \n            # Progress callback\n            if progress_callback:\n                progress_callback({\n                    'url': url,\n                    'depth': depth,\n                    'pages_crawled': self.stats['pages_crawled'],\n                    'pdfs_found': self.stats['pdfs_found'],\n                    'queue_size': len(self.url_queue)\n                })\n                \n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Error crawling {url}: {e}\")\n            with self.lock:\n                self.stats['errors'] += 1\n    \n    def _can_fetch(self, url: str) -> bool:\n        \"\"\"Check if URL can be fetched according to robots.txt.\"\"\"\n        try:\n            parsed = urlparse(url)\n            robot_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n            \n            # Check cache\n            if robot_url in self.robots_cache:\n                rp = self.robots_cache[robot_url]\n            else:\n                # Fetch and parse robots.txt\n                rp = RobotFileParser()\n                rp.set_url(robot_url)\n                rp.read()\n                self.robots_cache[robot_url] = rp\n            \n            return rp.can_fetch(self.session.headers['User-Agent'], url)\n            \n        except Exception as e:\n            logger.warning(f\"Error checking robots.txt for {url}: {e}\")\n            return True  # Allow if can't check\n    \n    def _apply_rate_limit(self, url: str) -> None:\n        \"\"\"Apply rate limiting for the domain.\"\"\"\n        domain = urlparse(url).netloc\n        \n        with self.lock:\n            if domain in self.domain_last_access:\n                elapsed = time.time() - self.domain_last_access[domain]\n                if elapsed < self.request_delay:\n                    time.sleep(self.request_delay - elapsed)\n            \n            self.domain_last_access[domain] = time.time()\n    \n    def _extract_page_data(self, url: str, soup: BeautifulSoup, response: requests.Response) -> Dict:\n        \"\"\"Extract relevant data from the page.\"\"\"\n        # Extract title\n        title = \"\"\n        if soup.title:\n            title = soup.title.string.strip() if soup.title.string else \"\"\n        \n        # Extract meta description\n        description = \"\"\n        meta_desc = soup.find('meta', attrs={'name': 'description'})\n        if meta_desc:\n            description = meta_desc.get('content', '')\n        \n        # Extract text content (limited)\n        text_content = soup.get_text(separator=' ', strip=True)[:5000]\n        \n        return {\n            'url': url,\n            'title': title,\n            'description': description,\n            'content_preview': text_content,\n            'content_type': response.headers.get('Content-Type', ''),\n            'status_code': response.status_code,\n            'size': len(response.content),\n            'timestamp': time.time()\n        }\n    \n    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n        \"\"\"Extract all links from the page.\"\"\"\n        links = []\n        \n        for tag in soup.find_all(['a', 'link']):\n            href = tag.get('href')\n            if href:\n                # Make absolute URL\n                absolute_url = urljoin(base_url, href)\n                \n                # Clean URL (remove fragments)\n                parsed = urlparse(absolute_url)\n                clean_url = urlunparse(parsed._replace(fragment=''))\n                \n                # Filter out non-HTTP(S) URLs\n                if parsed.scheme in ['http', 'https']:\n                    links.append(clean_url)\n        \n        return list(set(links))  # Remove duplicates\n    \n    def _is_pdf_link(self, url: str) -> bool:\n        \"\"\"Check if URL points to a PDF file.\"\"\"\n        # Check file extension\n        if url.lower().endswith('.pdf'):\n            return True\n        \n        # Check for PDF in path\n        if '/pdf/' in url.lower() or 'pdf' in urlparse(url).path.lower():\n            return True\n        \n        # Check for common academic PDF patterns\n        patterns = [\n            'arxiv.org/pdf/',\n            'doi.org/',\n            '/fulltext.pdf',\n            '/download/pdf',\n            'type=pdf'\n        ]\n        \n        return any(pattern in url.lower() for pattern in patterns)\n    \n    def _extract_link_title(self, soup: BeautifulSoup, link_url: str) -> str:\n        \"\"\"Extract title for a link.\"\"\"\n        # Find the anchor tag with this href\n        for a in soup.find_all('a', href=True):\n            if urljoin(soup.get('url', ''), a['href']) == link_url:\n                # Get link text\n                text = a.get_text(strip=True)\n                if text:\n                    return text\n                \n                # Check title attribute\n                if a.get('title'):\n                    return a['title']\n        \n        # Default to filename\n        return urlparse(link_url).path.split('/')[-1] or 'Untitled'\n    \n    def _get_results(self) -> Dict[str, any]:\n        \"\"\"Get crawling results.\"\"\"\n        duration = self.stats['end_time'] - self.stats['start_time'] if self.stats['end_time'] else 0\n        \n        return {\n            'stats': {\n                **self.stats,\n                'duration': duration,\n                'pages_per_second': self.stats['pages_crawled'] / max(duration, 1)\n            },\n            'pdf_links': self.pdf_links,\n            'scraped_pages': len(self.scraped_data),\n            'total_links_found': len(self.visited_urls)\n        }\n    \n    def cancel(self) -> None:\n        \"\"\"Cancel the crawling operation.\"\"\"\n        self.is_cancelled = True\n        logger.info(\"Crawling cancelled by user\")\n\n\ndef crawl_website(url: str, \n                  max_depth: int = 3,\n                  max_pages: int = 100,\n                  progress_callback: Optional[Callable] = None,\n                  pdf_callback: Optional[Callable] = None,\n                  **kwargs) -> Dict[str, any]:\n    \"\"\"\n    Convenience function to crawl a website.\n    \n    Args:\n        url: Starting URL\n        max_depth: Maximum crawling depth\n        max_pages: Maximum pages to crawl\n        progress_callback: Progress callback function\n        pdf_callback: PDF discovery callback function\n        **kwargs: Additional crawler parameters\n        \n    Returns:\n        Crawling results dictionary\n    \"\"\"\n    crawler = WebCrawler(\n        max_depth=max_depth,\n        max_pages=max_pages,\n        **kwargs\n    )\n    \n    return crawler.crawl(\n        url,\n        progress_callback=progress_callback,\n        pdf_callback=pdf_callback\n    )","source":"/workspace/modules/blueprints/features/web_crawler.py","title":"web_crawler.py","language":"en"},{"content":"\"\"\"\nWeb Scraper Blueprint\nHandles all web scraping related routes and functionality\n\"\"\"\n\nfrom flask import Blueprint, request, jsonify, send_from_directory, abort\nfrom flask_socketio import emit\nimport logging\nimport uuid\nimport time\nimport os\nimport threading\nimport tempfile\nimport hashlib\nimport requests\nfrom typing import Dict, Any, List, Optional, Set\nfrom pathlib import Path\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse, urlunparse\nfrom urllib.robotparser import RobotFileParser\nimport re\nfrom collections import deque\n\n# Optional dependencies (graceful fallback if not available)\ntry:\n    import trafilatura\n    TRAFILATURA_AVAILABLE = True\nexcept ImportError:\n    TRAFILATURA_AVAILABLE = False\n\n# Import necessary modules and utilities\nfrom blueprints.core.services import (\n    add_task, get_task, remove_task,\n    structured_error_response, emit_task_error,\n    ScraperTask\n)\nfrom blueprints.core.utils import get_output_filepath, sanitize_filename\nfrom blueprints.core.structify_integration import structify_module\nfrom blueprints.features.pdf_processor import download_pdf, analyze_pdf_structure\n\n# Try to import web_scraper module\ntry:\n    import web_scraper\n    web_scraper_available = True\nexcept ImportError:\n    web_scraper_available = False\n\n# Try to import python-magic for file type detection\ntry:\n    import magic\n    magic_available = True\nexcept ImportError:\n    magic_available = False\n\n# Default settings\nDEFAULT_OUTPUT_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'downloads')\n\nlogger = logging.getLogger(__name__)\n\n# Initialize logger if needed\nif not logger.handlers:\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n\n# Create the blueprint\nweb_scraper_bp = Blueprint('web_scraper', __name__, url_prefix='/api')\n\n# Export the blueprint and utility functions\n__all__ = ['web_scraper_bp', 'emit_scraping_progress', 'emit_scraping_completed', 'emit_scraping_error']\n\nclass EnhancedWebScraper:\n    \"\"\"Enhanced Web Scraper with 2 powerful options integrated into main file\"\"\"\n    \n    def __init__(self):\n        self.crawl_config = {\n            'max_depth': 3,\n            'max_pages': 200,\n            'respect_robots': True,\n            'follow_redirects': True,\n            'concurrent_requests': 8,\n            'request_delay': 1000,  # milliseconds\n            'timeout': 30000,\n            'retry_attempts': 3,\n            'user_agent_rotation': True\n        }\n        \n        self.content_config = {\n            'extract_clean_content': True,\n            'remove_navigation': True,\n            'remove_ads': True,\n            'preserve_code_blocks': True,\n            'convert_to_markdown': True,\n            'extract_images': True,\n            'follow_internal_links': True\n        }\n        \n        # State management\n        self.visited_urls: Set[str] = set()\n        self.url_queue: deque = deque()\n        self.pdf_urls: List[Dict] = []\n        self.results: Dict = {}\n        self.errors: List[Dict] = []\n        \n    def detect_site_type(self, url: str, html: str) -> Dict[str, Any]:\n        \"\"\"Detect if site is documentation, blog, news, etc.\"\"\"\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        detection = {\n            'type': 'general',\n            'platform': 'unknown',\n            'features': []\n        }\n        \n        # Check for documentation patterns\n        if any(keyword in url.lower() for keyword in ['docs', 'documentation', 'api', 'guide']):\n            detection['type'] = 'documentation'\n            detection['features'].append('docs_url')\n            \n        # GitBook detection\n        if 'gitbook' in html.lower() or soup.find('meta', {'name': 'generator', 'content': lambda x: x and 'gitbook' in x.lower()}):\n            detection['platform'] = 'gitbook'\n            detection['features'].append('gitbook')\n            \n        # ReadTheDocs detection\n        if 'readthedocs' in url or soup.find('div', class_='rst-content'):\n            detection['platform'] = 'readthedocs'\n            detection['features'].append('sphinx')\n            \n        # Navigation patterns\n        if soup.find('nav') or soup.find('div', class_=re.compile(r'nav|sidebar|menu', re.I)):\n            detection['features'].append('navigation')\n            \n        # Table of contents\n        if soup.find('div', class_=re.compile(r'toc|table-of-contents', re.I)):\n            detection['features'].append('toc')\n            \n        return detection\n        \n    def extract_navigation_links(self, html: str, base_url: str) -> List[str]:\n        \"\"\"Extract navigation and documentation links\"\"\"\n        soup = BeautifulSoup(html, 'html.parser')\n        links = []\n        \n        # Find navigation areas\n        nav_selectors = [\n            'nav a',\n            '.nav a', \n            '.navbar a',\n            '.sidebar a',\n            '.menu a',\n            '.toc a',\n            '.table-of-contents a',\n            '[class*=\"nav\"] a',\n            '[class*=\"menu\"] a'\n        ]\n        \n        for selector in nav_selectors:\n            nav_links = soup.select(selector)\n            for link in nav_links:\n                href = link.get('href')\n                if href:\n                    absolute_url = urljoin(base_url, href)\n                    if self.is_same_domain(base_url, absolute_url):\n                        links.append(absolute_url)\n                        \n        # Also look for \"next\" and pagination links\n        pagination_links = soup.find_all('a', text=re.compile(r'next|continue|more', re.I))\n        for link in pagination_links:\n            href = link.get('href')\n            if href:\n                absolute_url = urljoin(base_url, href)\n                if self.is_same_domain(base_url, absolute_url):\n                    links.append(absolute_url)\n                    \n        return list(set(links))  # Remove duplicates\n        \n    def extract_clean_content(self, html: str, url: str) -> Dict[str, Any]:\n        \"\"\"Extract clean content optimized for LLM training\"\"\"\n        try:\n            if TRAFILATURA_AVAILABLE:\n                # Use trafilatura for clean content extraction\n                extracted = trafilatura.extract(\n                    html,\n                    include_comments=False,\n                    include_tables=True,\n                    include_links=True,\n                    output_format='json',\n                    config=trafilatura.settings.use_config()\n                )\n                \n                if extracted:\n                    import json\n                    content_data = json.loads(extracted)\n                    \n                    return {\n                        'title': content_data.get('title', ''),\n                        'content': content_data.get('text', ''),\n                        'markdown': self.html_to_markdown(content_data.get('text', '')),\n                        'metadata': {\n                            'url': url,\n                            'extracted_at': time.time(),\n                            'word_count': len(content_data.get('text', '').split()),\n                            'language': content_data.get('language', 'en'),\n                            'extraction_method': 'trafilatura'\n                        }\n                    }\n            \n            # Fallback to BeautifulSoup if trafilatura not available\n            return self.extract_with_beautifulsoup(html, url)\n                \n        except Exception as e:\n            logger.warning(f\"Content extraction failed for {url}: {e}\")\n            return self.extract_with_beautifulsoup(html, url)\n            \n    def extract_with_beautifulsoup(self, html: str, url: str) -> Dict[str, Any]:\n        \"\"\"Fallback content extraction with BeautifulSoup\"\"\"\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        # Remove unwanted elements\n        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n            element.decompose()\n            \n        # Find main content area\n        main_content = (\n            soup.find('main') or \n            soup.find('article') or \n            soup.find('div', class_=re.compile(r'content|main|body', re.I)) or\n            soup.find('body')\n        )\n        \n        if main_content:\n            # Get title\n            title = soup.find('title')\n            title_text = title.get_text().strip() if title else ''\n            \n            # Extract text content\n            content = main_content.get_text(separator='\\n', strip=True)\n            \n            return {\n                'title': title_text,\n                'content': content,\n                'markdown': self.html_to_markdown(content),\n                'metadata': {\n                    'url': url,\n                    'extracted_at': time.time(),\n                    'word_count': len(content.split()),\n                    'extraction_method': 'beautifulsoup'\n                }\n            }\n        \n        return {\n            'title': '',\n            'content': '',\n            'markdown': '',\n            'metadata': {'url': url, 'error': 'No content found'}\n        }\n        \n    def html_to_markdown(self, text: str) -> str:\n        \"\"\"Convert HTML text to clean markdown\"\"\"\n        markdown = text\n        \n        # Basic formatting preservation\n        markdown = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', markdown)  # Normalize spacing\n        markdown = re.sub(r'^\\s+', '', markdown, flags=re.MULTILINE)  # Remove leading spaces\n        \n        return markdown.strip()\n        \n    def is_same_domain(self, base_url: str, check_url: str) -> bool:\n        \"\"\"Check if URLs are from the same domain\"\"\"\n        base_domain = urlparse(base_url).netloc.lower()\n        check_domain = urlparse(check_url).netloc.lower()\n        return base_domain == check_domain\n        \n    def discover_pdfs_on_page(self, html: str, base_url: str) -> List[Dict[str, str]]:\n        \"\"\"Discover PDF links on a single page (depth 0)\"\"\"\n        soup = BeautifulSoup(html, 'html.parser')\n        pdf_links = []\n        \n        # Find all links\n        for link in soup.find_all('a', href=True):\n            href = link['href']\n            \n            # Check if it's a PDF link\n            if href.lower().endswith('.pdf') or 'pdf' in href.lower():\n                absolute_url = urljoin(base_url, href)\n                \n                # Get link text or title for metadata\n                title = (\n                    link.get_text(strip=True) or \n                    link.get('title', '') or \n                    os.path.basename(href)\n                )\n                \n                pdf_links.append({\n                    'url': absolute_url,\n                    'title': title,\n                    'found_on': base_url\n                })\n                \n        return pdf_links\n\n    def check_robots_txt(self, base_url: str) -> bool:\n        \"\"\"Check if robots.txt allows crawling\"\"\"\n        try:\n            parsed_url = urlparse(base_url)\n            robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n            \n            rp = RobotFileParser()\n            rp.set_url(robots_url)\n            rp.read()\n            \n            return rp.can_fetch('*', base_url)\n        except Exception:\n            # If can't read robots.txt, assume allowed\n            return True\n\ndef download_pdf_fixed(url: str, save_directory: str) -> Optional[str]:\n    \"\"\"Fixed version of PDF download without os.fsync bug\"\"\"\n    try:\n        os.makedirs(save_directory, exist_ok=True)\n        \n        # Convert arXiv abstract URLs to PDF URLs\n        if 'arxiv.org/abs/' in url:\n            url = url.replace('/abs/', '/pdf/') + '.pdf'\n            logger.info(f\"Converted arXiv abstract URL to PDF URL: {url}\")\n        \n        # Generate filename\n        url_hash = hashlib.md5(url.encode()).hexdigest()[:10]\n        filename = f\"{os.path.basename(urlparse(url).path) or 'document'}_{url_hash}.pdf\"\n        if not filename.endswith('.pdf'):\n            filename += '.pdf'\n        \n        file_path = os.path.join(save_directory, filename)\n        \n        # Check if already exists\n        if os.path.exists(file_path):\n            logger.info(f\"PDF already exists: {file_path}\")\n            return file_path\n        \n        # Download with proper headers\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (compatible; Academic-Scraper/1.0)',\n            'Accept': 'application/pdf,*/*'\n        }\n        \n        response = requests.get(url, headers=headers, timeout=60, stream=True)\n        response.raise_for_status()\n        \n        # Write file in chunks without the fsync bug\n        with open(file_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n        \n        logger.info(f\"Successfully downloaded PDF: {file_path}\")\n        return file_path\n        \n    except Exception as e:\n        logger.error(f\"Error downloading PDF {url}: {e}\")\n        return None\n\ndef crawl_website_recursive(scraper: EnhancedWebScraper, start_url: str, \n                          output_dir: str, output_format: str) -> Dict[str, Any]:\n    \"\"\"Recursively crawl website and extract content\"\"\"\n    \n    scraper.url_queue.append((start_url, 0))  # (url, depth)\n    pages_crawled = 0\n    pdfs_found = 0\n    site_map = {}\n    \n    # Create output subdirectories\n    pages_dir = os.path.join(output_dir, \"pages\")\n    pdfs_dir = os.path.join(output_dir, \"pdfs\")\n    os.makedirs(pages_dir, exist_ok=True)\n    os.makedirs(pdfs_dir, exist_ok=True)\n    \n    while scraper.url_queue and pages_crawled < scraper.crawl_config['max_pages']:\n        current_url, depth = scraper.url_queue.popleft()\n        \n        # Skip if already visited or exceeded depth\n        if current_url in scraper.visited_urls or depth > scraper.crawl_config['max_depth']:\n            continue\n            \n        scraper.visited_urls.add(current_url)\n        \n        try:\n            logger.info(f\"Crawling [{depth}]: {current_url}\")\n            \n            response = requests.get(current_url, timeout=30)\n            response.raise_for_status()\n            \n            # Detect site type and extract content\n            site_info = scraper.detect_site_type(current_url, response.text)\n            content_data = scraper.extract_clean_content(response.text, current_url)\n            \n            # Save page content\n            page_filename = f\"page_{pages_crawled:04d}_{sanitize_filename(urlparse(current_url).path or 'index')}\"\n            \n            if output_format == \"markdown\":\n                page_file = os.path.join(pages_dir, f\"{page_filename}.md\")\n                with open(page_file, 'w', encoding='utf-8') as f:\n                    f.write(f\"# {content_data['title']}\\n\\n\")\n                    f.write(f\"**URL:** {current_url}\\n\\n\")\n                    f.write(content_data['markdown'])\n                    \n            elif output_format == \"json\":\n                page_file = os.path.join(pages_dir, f\"{page_filename}.json\")\n                with open(page_file, 'w', encoding='utf-8') as f:\n                    import json\n                    json.dump({\n                        'url': current_url,\n                        'title': content_data['title'],\n                        'content': content_data['content'],\n                        'markdown': content_data['markdown'],\n                        'site_info': site_info,\n                        'metadata': content_data['metadata']\n                    }, f, indent=2)\n            \n            # Discover PDFs on this page\n            page_pdfs = scraper.discover_pdfs_on_page(response.text, current_url)\n            for pdf_info in page_pdfs:\n                pdf_file = download_pdf_fixed(pdf_info['url'], pdfs_dir)\n                if pdf_file:\n                    pdfs_found += 1\n            \n            # Find more links to crawl (if not at max depth)\n            if depth < scraper.crawl_config['max_depth']:\n                nav_links = scraper.extract_navigation_links(response.text, current_url)\n                for link in nav_links:\n                    if link not in scraper.visited_urls:\n                        scraper.url_queue.append((link, depth + 1))\n            \n            # Update site map\n            site_map[current_url] = {\n                'depth': depth,\n                'title': content_data['title'],\n                'content_length': len(content_data['content']),\n                'site_type': site_info['type'],\n                'pdfs_found': len(page_pdfs)\n            }\n            \n            pages_crawled += 1\n            \n            # Polite delay\n            time.sleep(scraper.crawl_config['request_delay'] / 1000.0)\n            \n        except Exception as e:\n            logger.error(f\"Error crawling {current_url}: {e}\")\n            continue\n    \n    # Create summary\n    summary = {\n        'start_url': start_url,\n        'pages_crawled': pages_crawled,\n        'pdfs_found': pdfs_found,\n        'max_depth_reached': max(site_map[url]['depth'] for url in site_map) if site_map else 0,\n        'output_format': output_format,\n        'directories': {\n            'pages': pages_dir,\n            'pdfs': pdfs_dir\n        }\n    }\n    \n    # Save summary\n    summary_file = os.path.join(output_dir, \"crawl_summary.json\")\n    with open(summary_file, 'w', encoding='utf-8') as f:\n        import json\n        json.dump(summary, f, indent=2)\n    \n    return {\n        'pages_crawled': pages_crawled,\n        'pdfs_found': pdfs_found,\n        'site_map': site_map,\n        'summary': summary\n    }\n\ndef handle_enhanced_scraping(data: Dict[str, Any], scrape_mode: str) -> jsonify:\n    \"\"\"\n    Handle the new 2-option enhanced scraping system\n    \"\"\"\n    try:\n        # Use the local enhanced scraper class and functions\n        \n        url = data.get(\"url\")\n        download_directory = data.get(\"download_directory\")\n        \n        if not url or not download_directory:\n            return structured_error_response(\"MISSING_PARAMS\", \"URL and download directory required.\", 400)\n        \n        # Ensure download directory exists\n        os.makedirs(download_directory, exist_ok=True)\n        task_id = str(uuid.uuid4())\n        \n        if scrape_mode == \"smart_pdf\":\n            return handle_smart_pdf_mode(data, task_id, url, download_directory)\n        elif scrape_mode == \"full_website\":\n            return handle_full_website_mode(data, task_id, url, download_directory)\n        else:\n            return structured_error_response(\"INVALID_MODE\", f\"Unknown scrape mode: {scrape_mode}\", 400)\n            \n    except Exception as e:\n        logger.error(f\"Enhanced scraping failed: {e}\")\n        return structured_error_response(\"ENHANCED_SCRAPING_ERROR\", f\"Error: {str(e)}\", 500)\n\ndef handle_smart_pdf_mode(data: Dict[str, Any], task_id: str, url: str, download_directory: str) -> jsonify:\n    \"\"\"Handle Smart PDF Discovery & Processing mode\"\"\"\n    # Use local classes and functions\n    \n    # Get PDF processing options\n    pdf_options = data.get(\"pdf_options\", {})\n    process_pdfs = pdf_options.get(\"process_pdfs\", True)\n    extract_tables = pdf_options.get(\"extract_tables\", True)\n    use_ocr = pdf_options.get(\"use_ocr\", True)\n    \n    scraper = EnhancedWebScraper()\n    \n    try:\n        # Check if URL is direct PDF\n        if url.lower().endswith('.pdf'):\n            logger.info(f\"Direct PDF URL detected: {url}\")\n            \n            # Download and process single PDF\n            pdf_file = download_pdf_fixed(url, download_directory)\n            \n            if pdf_file and os.path.exists(pdf_file):\n                # Process with Structify\n                if structify_module and process_pdfs:\n                    json_filename = f\"{os.path.splitext(os.path.basename(pdf_file))[0]}_processed.json\"\n                    json_path = os.path.join(download_directory, json_filename)\n                    \n                    # Use existing Structify integration\n                    result = structify_module.process_all_files(\n                        root_directory=os.path.dirname(pdf_file),\n                        output_file=json_path,\n                        file_filter=lambda f: f == pdf_file\n                    )\n                    \n                    return jsonify({\n                        \"task_id\": task_id,\n                        \"status\": \"completed\",\n                        \"mode\": \"smart_pdf\",\n                        \"message\": \"Direct PDF downloaded and processed\",\n                        \"pdfs_found\": 1,\n                        \"pdfs_processed\": 1,\n                        \"results\": [{\n                            \"url\": url,\n                            \"pdf_file\": pdf_file,\n                            \"json_file\": json_path\n                        }]\n                    })\n                else:\n                    return jsonify({\n                        \"task_id\": task_id,\n                        \"status\": \"completed\",\n                        \"mode\": \"smart_pdf\",\n                        \"message\": \"Direct PDF downloaded (processing disabled)\",\n                        \"pdfs_found\": 1,\n                        \"pdfs_processed\": 0,\n                        \"results\": [{\"url\": url, \"pdf_file\": pdf_file}]\n                    })\n        \n        else:\n            logger.info(f\"HTML page detected, discovering PDFs: {url}\")\n            \n            # Fetch the HTML page\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            \n            # Discover PDFs on this page (depth 0)\n            pdf_links = scraper.discover_pdfs_on_page(response.text, url)\n            \n            if not pdf_links:\n                return jsonify({\n                    \"task_id\": task_id,\n                    \"status\": \"completed\", \n                    \"mode\": \"smart_pdf\",\n                    \"message\": \"No PDFs found on page\",\n                    \"pdfs_found\": 0,\n                    \"pdfs_processed\": 0\n                })\n            \n            # Download and process all discovered PDFs\n            processed_pdfs = []\n            for pdf_info in pdf_links:\n                try:\n                    pdf_file = download_pdf_fixed(pdf_info['url'], download_directory)\n                    \n                    if pdf_file and os.path.exists(pdf_file):\n                        result_info = {\n                            \"url\": pdf_info['url'],\n                            \"title\": pdf_info['title'],\n                            \"pdf_file\": pdf_file\n                        }\n                        \n                        # Process with Structify if enabled\n                        if structify_module and process_pdfs:\n                            json_filename = f\"{os.path.splitext(os.path.basename(pdf_file))[0]}_processed.json\"\n                            json_path = os.path.join(download_directory, json_filename)\n                            \n                            structify_module.process_all_files(\n                                root_directory=os.path.dirname(pdf_file),\n                                output_file=json_path,\n                                file_filter=lambda f: f == pdf_file\n                            )\n                            \n                            result_info[\"json_file\"] = json_path\n                        \n                        processed_pdfs.append(result_info)\n                        \n                except Exception as e:\n                    logger.error(f\"Error processing PDF {pdf_info['url']}: {e}\")\n                    continue\n            \n            return jsonify({\n                \"task_id\": task_id,\n                \"status\": \"completed\",\n                \"mode\": \"smart_pdf\", \n                \"message\": f\"Discovered and processed {len(processed_pdfs)} PDFs from page\",\n                \"pdfs_found\": len(pdf_links),\n                \"pdfs_processed\": len(processed_pdfs),\n                \"results\": processed_pdfs\n            })\n            \n    except Exception as e:\n        logger.error(f\"Smart PDF mode failed: {e}\")\n        return structured_error_response(\"SMART_PDF_ERROR\", f\"Error: {str(e)}\", 500)\n\ndef handle_full_website_mode(data: Dict[str, Any], task_id: str, url: str, download_directory: str) -> jsonify:\n    \"\"\"Handle Full Website & Documentation Crawler mode\"\"\"\n    # Use local classes and functions\n    \n    # Get crawling options\n    max_depth = min(data.get(\"max_depth\", 3), 10)  # Cap at 10 levels\n    max_pages = min(data.get(\"max_pages\", 200), 1000)  # Cap at 1000 pages\n    respect_robots = data.get(\"respect_robots\", True)\n    output_format = data.get(\"output_format\", \"markdown\")  # markdown, html, json\n    \n    scraper = EnhancedWebScraper()\n    scraper.crawl_config['max_depth'] = max_depth\n    scraper.crawl_config['max_pages'] = max_pages\n    scraper.crawl_config['respect_robots'] = respect_robots\n    \n    try:\n        # Check robots.txt if requested\n        if respect_robots and not scraper.check_robots_txt(url):\n            return structured_error_response(\"ROBOTS_BLOCKED\", \"Crawling blocked by robots.txt\", 403)\n        \n        # Start crawling\n        result = crawl_website_recursive(scraper, url, download_directory, output_format)\n        \n        return jsonify({\n            \"task_id\": task_id,\n            \"status\": \"completed\",\n            \"mode\": \"full_website\",\n            \"message\": f\"Crawled {result['pages_crawled']} pages, found {result['pdfs_found']} PDFs\",\n            \"pages_crawled\": result[\"pages_crawled\"],\n            \"pdfs_found\": result[\"pdfs_found\"],\n            \"output_directory\": download_directory,\n            \"max_depth_reached\": result[\"summary\"][\"max_depth_reached\"],\n            \"output_format\": output_format,\n            \"summary\": result[\"summary\"]\n        })\n        \n    except Exception as e:\n        logger.error(f\"Full website mode failed: {e}\")\n        return structured_error_response(\"FULL_WEBSITE_ERROR\", f\"Error: {str(e)}\", 500)\n\n@web_scraper_bp.route('/health-enhanced', methods=['GET'])\ndef health_check_enhanced():\n    \"\"\"Health check for enhanced web scraper features\"\"\"\n    return jsonify({\n        \"status\": \"healthy\",\n        \"version\": \"2.0_consolidated\",\n        \"features\": {\n            \"smart_pdf\": \"Smart PDF Discovery & Processing\",\n            \"full_website\": \"Full Website & Documentation Crawler\"\n        },\n        \"dependencies\": {\n            \"structify\": structify_module is not None,\n            \"trafilatura\": TRAFILATURA_AVAILABLE,\n            \"beautifulsoup\": True,\n            \"requests\": True,\n            \"urllib_robotparser\": True\n        },\n        \"legacy_compatibility\": \"Maintained for old 5-option system\",\n        \"endpoints\": {\n            \"enhanced\": \"/api/scrape2 (with scrape_mode parameter)\",\n            \"legacy\": \"/api/scrape2 (with urls parameter)\",\n            \"health\": \"/api/health-enhanced\"\n        }\n    })\n\n    \n@web_scraper_bp.route('/scrape2', methods=['POST'])\ndef scrape2():\n    \"\"\"\n    Enhanced endpoint supporting 2 powerful scraping modes:\n    - smart_pdf: Smart PDF Discovery & Processing\n    - full_website: Full Website & Documentation Crawler\n    \n    Maintains backward compatibility with old 5-option system\n    \"\"\"\n    data = request.get_json()\n    if not data:\n        return structured_error_response(\"NO_DATA\", \"No JSON data provided.\", 400)\n    \n    # Check for new 2-option system\n    scrape_mode = data.get(\"scrape_mode\")\n    if scrape_mode in [\"smart_pdf\", \"full_website\"]:\n        return handle_enhanced_scraping(data, scrape_mode)\n    \n    # Legacy support for old 5-option system\n    url_configs = data.get(\"urls\")\n    download_directory = data.get(\"download_directory\")\n    output_filename = data.get(\"outputFilename\", \"\").strip()\n    \n    # Get enhanced PDF options\n    pdf_options = data.get(\"pdf_options\", {})\n    process_pdfs = pdf_options.get(\"process_pdfs\", True)\n    extract_tables = pdf_options.get(\"extract_tables\", True)\n    use_ocr = pdf_options.get(\"use_ocr\", True)\n    extract_structure = pdf_options.get(\"extract_structure\", True)\n    chunk_size = pdf_options.get(\"chunk_size\", 4096)\n    max_downloads = pdf_options.get(\"max_downloads\", 10)  # Default to 10 PDFs\n    \n    if not url_configs or not isinstance(url_configs, list):\n        return structured_error_response(\"URLS_REQUIRED\", \"A list of URLs is required.\", 400)\n    \n    if not download_directory:\n        return structured_error_response(\"ROOT_DIR_REQUIRED\", \"Download directory is required.\", 400)\n    \n    if not output_filename:\n        return structured_error_response(\"OUTPUT_FILE_REQUIRED\", \"Output filename is required.\", 400)\n    \n    # Ensure output file has proper extension\n    if not output_filename.lower().endswith('.json'):\n        output_filename += '.json'\n    \n    # Convert to absolute path\n    download_directory = os.path.abspath(download_directory)\n    \n    # Get properly formatted output path\n    final_json = get_output_filepath(output_filename, user_defined_dir=download_directory)\n    \n    # Validate and create download directory if it doesn't exist\n    if not os.path.isdir(download_directory):\n        try:\n            os.makedirs(download_directory, exist_ok=True)\n            logger.info(f\"Created download directory: {download_directory}\")\n        except Exception as e:\n            return structured_error_response(\"DIR_CREATION_FAILED\", f\"Could not create download directory: {e}\", 500)\n    \n    # Log the request\n    logger.info(f\"Starting web scraping with {len(url_configs)} URLs to {download_directory}\")\n    logger.info(f\"Output JSON will be saved to: {final_json}\")\n    logger.info(f\"PDF options: process={process_pdfs}, tables={extract_tables}, ocr={use_ocr}, structure={extract_structure}, chunk_size={chunk_size}, max_downloads={max_downloads}\")\n    \n    # Create and start the scraper task with enhanced options\n    task_id = str(uuid.uuid4())\n    scraper_task = ScraperTask(task_id)\n    add_task(task_id, scraper_task)\n    \n    # Pass the enhanced options to the task\n    scraper_task.pdf_options = {\n        \"process_pdfs\": process_pdfs,\n        \"extract_tables\": extract_tables,\n        \"use_ocr\": use_ocr,\n        \"extract_structure\": extract_structure,\n        \"chunk_size\": chunk_size,\n        \"max_downloads\": max_downloads\n    }\n    \n    # Start the task with parameters\n    scraper_task.start(\n        url_configs=url_configs,\n        root_scrape_directory=download_directory,\n        output_json_file=output_filename,\n        pdf_options=scraper_task.pdf_options\n    )\n    \n    return jsonify({\n        \"task_id\": task_id,\n        \"status\": \"processing\",\n        \"message\": \"Scraping started\",\n        \"root_directory\": download_directory,\n        \"output_file\": final_json\n    })\n    \n\n\n@web_scraper_bp.route('/scrape2/status/<task_id>', methods=['GET'])\ndef scrape2_status(task_id):\n    \"\"\"Get the status of a scraping task with PDF download information.\"\"\"\n    task = get_task(task_id)\n    if not task or not isinstance(task, ScraperTask):\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"ScraperTask with ID {task_id} not found.\", 404)\n    \n    # Build response with PDF downloads information\n    response = {\n        \"task_id\": task.task_id,\n        \"status\": task.status,\n        \"progress\": task.progress,\n        \"stats\": task.stats,\n        \"error\": task.error,\n        \"output_file\": task.output_file,\n        \"output_folder\": task.root_scrape_directory if hasattr(task, 'root_scrape_directory') else None\n    }\n    \n    # Include PDF downloads information if available\n    if hasattr(task, 'pdf_downloads') and task.pdf_downloads:\n        response[\"pdf_downloads\"] = task.pdf_downloads\n    \n    return jsonify(response)\n\n@web_scraper_bp.route(\"/download-pdf\", methods=[\"POST\"])\ndef api_download_pdf():\n    \"\"\"\n    Enhanced API endpoint to download a PDF file from a URL to a user-specified folder.\n    \n    Expected JSON body:\n    {\n        \"url\": \"https://example.com/paper.pdf\",\n        \"outputFolder\": User-selected download directory,\n        \"outputFilename\": User-specified filename (without extension),\n        \"processFile\": true,  # Whether to process the PDF to JSON\n        \"extractTables\": true,  # Whether to extract tables\n        \"useOcr\": true  # Whether to use OCR for scanned content\n    }\n    \n    Returns:\n        JSON response with download status, file path, etc.\n    \"\"\"\n    data = request.get_json()\n    if not data:\n        return structured_error_response(\"NO_DATA\", \"No JSON data provided.\", 400)\n    \n    url = data.get(\"url\")\n    output_folder = data.get(\"outputFolder\", DEFAULT_OUTPUT_FOLDER)\n    output_filename = data.get(\"outputFilename\")\n    process_file = data.get(\"processFile\", True)\n    extract_tables = data.get(\"extractTables\", True)\n    use_ocr = data.get(\"useOcr\", True)\n    \n    if not url:\n        return structured_error_response(\"URL_REQUIRED\", \"PDF URL is required.\", 400)\n    \n    # Ensure output directory exists\n    try:\n        os.makedirs(output_folder, exist_ok=True)\n    except Exception as e:\n        logger.error(f\"Error creating output directory: {e}\")\n        return structured_error_response(\"OUTPUT_DIR_ERROR\", f\"Failed to create output directory: {str(e)}\", 500)\n    \n    # Create a unique task ID for tracking this download\n    download_id = str(uuid.uuid4())\n    \n    try:\n        # Download the PDF using enhanced function\n        logger.info(f\"Starting PDF download from {url} to {output_folder}\")\n        \n        # Use the enhanced download_pdf function from web_scraper\n        pdf_file = download_pdf(url, output_folder)\n        \n        if pdf_file and os.path.exists(pdf_file):\n            # Get file size and other metadata\n            file_size = os.path.getsize(pdf_file)\n            file_name = os.path.basename(pdf_file)\n            \n            response_data = {\n                \"status\": \"success\",\n                \"message\": \"PDF downloaded successfully\",\n                \"download_id\": download_id,\n                \"url\": url,\n                \"filePath\": pdf_file,\n                \"fileName\": file_name,\n                \"fileSize\": file_size,\n                \"outputFolder\": output_folder\n            }\n            \n            # Process the PDF to JSON if requested\n            if process_file and structify_module:\n                json_file = None\n                try:\n                    # Generate a JSON filename based on user preference or PDF name\n                    if output_filename:\n                        json_filename = f\"{output_filename}.json\"\n                    else:\n                        json_filename = os.path.splitext(file_name)[0] + \"_processed.json\"\n                        \n                    json_path = os.path.join(output_folder, json_filename)\n                    \n                    # Detect document type to determine if OCR is needed\n                    doc_type = None\n                    if hasattr(structify_module, 'detect_document_type'):\n                        try:\n                            doc_type = structify_module.detect_document_type(pdf_file)\n                            response_data[\"documentType\"] = doc_type\n                        except Exception as e:\n                            logger.warning(f\"Error detecting document type: {e}\")\n                    \n                    # Apply OCR only if document type is scan or use_ocr is explicitly True\n                    apply_ocr = use_ocr or (doc_type == \"scan\")\n                    \n                    # Process with process_pdf if available\n                    if hasattr(structify_module, 'process_pdf'):\n                        result = structify_module.process_pdf(\n                            pdf_path=pdf_file,\n                            output_path=json_path,\n                            max_chunk_size=4096,\n                            extract_tables=extract_tables,\n                            use_ocr=apply_ocr,\n                            return_data=True\n                        )\n                        \n                        json_file = json_path\n                        \n                        # Add summary metrics to response\n                        if result:\n                            response_data[\"processingDetails\"] = {\n                                \"tablesExtracted\": len(result.get(\"tables\", [])),\n                                \"referencesExtracted\": len(result.get(\"references\", [])),\n                                \"pageCount\": result.get(\"page_count\", 0),\n                                \"chunksCreated\": len(result.get(\"chunks\", []))\n                            }\n                            \n                    else:\n                        # Fallback to process_all_files\n                        structify_module.process_all_files(\n                            root_directory=os.path.dirname(pdf_file),\n                            output_file=json_path,\n                            max_chunk_size=4096,\n                            executor_type=\"thread\",\n                            max_workers=None,\n                            stop_words=structify_module.DEFAULT_STOP_WORDS if hasattr(structify_module, 'DEFAULT_STOP_WORDS') else set(),\n                            use_cache=False,\n                            valid_extensions=[\".pdf\"],\n                            ignore_dirs=\"venv,node_modules,.git,__pycache__,dist,build\",\n                            stats_only=False,\n                            include_binary_detection=False,\n                            file_filter=lambda f: f == pdf_file\n                        )\n                        \n                        json_file = json_path\n                    \n                    # Add JSON file info to response\n                    if json_file and os.path.exists(json_file):\n                        response_data[\"jsonFile\"] = json_file\n                        logger.info(f\"PDF processed to JSON: {json_file}\")\n                        \n                        # Generate a quick PDF structure summary\n                        summary = analyze_pdf_structure(pdf_file)\n                        if summary and \"error\" not in summary:\n                            response_data[\"pdfStructure\"] = summary\n                    \n                except Exception as e:\n                    logger.error(f\"Error processing PDF to JSON: {e}\")\n                    response_data[\"processingError\"] = str(e)\n            \n            return jsonify(response_data)\n        else:\n            return structured_error_response(\"DOWNLOAD_FAILED\", \"Failed to download PDF file.\", 400)\n            \n    except Exception as e:\n       logger.error(f\"Error downloading PDF: {e}\", exc_info=True)\n       return structured_error_response(\"DOWNLOAD_ERROR\", f\"Error downloading PDF: {str(e)}\", 500)\n\n@web_scraper_bp.route(\"/download-pdf/<path:pdf_path>\")\ndef download_pdf_file(pdf_path):\n    \"\"\"\n    Download or view a specific PDF file with enhanced security checks.\n    \n    Args:\n        pdf_path: The path to the PDF file.\n        \n    Returns:\n        The PDF file for download or viewing.\n    \"\"\"\n    try:\n        # For security, ensure the path is within allowed directories\n        abs_path = os.path.abspath(pdf_path)\n        \n        # Define allowed directories (can be expanded based on application needs)\n        allowed_dirs = [\n            DEFAULT_OUTPUT_FOLDER,\n            os.path.join(os.path.expanduser(\"~\"), \"Documents\"),\n            app.config.get(\"UPLOAD_FOLDER\", tempfile.mkdtemp())\n        ]\n        \n        # Check if the path is within any allowed directory\n        is_allowed = any(os.path.commonpath([abs_path, allowed_dir]) == allowed_dir \n                        for allowed_dir in allowed_dirs if os.path.exists(allowed_dir))\n        \n        if not is_allowed:\n            logger.warning(f\"Attempted to access file outside allowed directories: {abs_path}\")\n            abort(403)  # Forbidden\n        \n        # Check if file exists\n        if not os.path.exists(abs_path):\n            logger.warning(f\"PDF file not found: {abs_path}\")\n            abort(404)\n        \n        # Verify file is a PDF (optional but adds security)\n        if not abs_path.lower().endswith('.pdf') and magic_available:\n            mime = magic.from_file(abs_path, mime=True)\n            if 'application/pdf' not in mime:\n                logger.warning(f\"File is not a PDF: {abs_path}, mime: {mime}\")\n                abort(400)  # Bad request\n        \n        # Get directory and filename\n        directory = os.path.dirname(abs_path)\n        filename = os.path.basename(abs_path)\n        \n        # Set response headers for PDF content\n        response = send_from_directory(\n            directory,\n            filename,\n            mimetype='application/pdf',\n            as_attachment=False  # Display in browser instead of downloading\n        )\n        \n        # Add additional security headers\n        response.headers['Content-Security-Policy'] = \"default-src 'self'\"\n        response.headers['X-Content-Type-Options'] = 'nosniff'\n        \n        logger.info(f\"Successfully served PDF file: {filename}\")\n        return response\n        \n    except Exception as e:\n        logger.error(f\"Error serving PDF file: {e}\")\n        abort(500)\n\n@web_scraper_bp.route(\"/download-file/<path:file_path>\")\ndef download_file_attachment(file_path):\n    \"\"\"\n    Force download of a specific file.\n    \n    Args:\n        file_path: The path to the file.\n        \n    Returns:\n        The file as an attachment for download.\n    \"\"\"\n    try:\n        # For security, ensure the path is within allowed directories\n        abs_path = os.path.abspath(file_path)\n        \n        # Check if file exists\n        if not os.path.exists(abs_path):\n            abort(404)\n        \n        # Get directory and filename\n        directory = os.path.dirname(abs_path)\n        filename = os.path.basename(abs_path)\n        \n        # Set response headers for attachment download\n        return send_from_directory(\n            directory, \n            filename,\n            as_attachment=True,  # Force download instead of displaying\n            download_name=filename\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error serving file for download: {e}\")\n        abort(500)\n@web_scraper_bp.route('/scrape2/cancel/<task_id>', methods=['POST'])\ndef cancel_scrape2(task_id):\n    \"\"\"Cancel a scraping task.\"\"\"\n    task = get_task(task_id)\n    if not task or not isinstance(task, ScraperTask):\n        return structured_error_response(\"TASK_NOT_FOUND\", f\"ScraperTask with ID {task_id} not found.\", 404)\n    \n    task.status = \"cancelled\"\n    remove_task(task_id)\n    \n    return jsonify({\n        \"task_id\": task_id,\n        \"status\": \"cancelled\",\n        \"message\": \"ScraperTask cancelled successfully.\"\n    })\n\n##########################\n# Helper Functions\n##########################\ndef scrape_and_download_pdfs(url: str, output_folder: str = DEFAULT_OUTPUT_FOLDER) -> Dict[str, Any]:\n    \"\"\"\n    Scrape a webpage for PDF links and download them.\n    \n    Args:\n        url (str): URL of the webpage to scrape\n        output_folder (str): Folder to save PDFs\n        \n    Returns:\n        Dict[str, Any]: Results of the scraping and downloading\n    \"\"\"\n    logger.info(f\"Scraping for PDFs from: {url}\")\n    \n    try:\n        # Ensure output folder exists\n        os.makedirs(output_folder, exist_ok=True)\n        \n        # Get PDF links from the page\n        pdf_links = fetch_pdf_links(url)\n        \n        if not pdf_links:\n            logger.info(f\"No PDF links found on {url}\")\n            return {\n                \"status\": \"completed\",\n                \"url\": url,\n                \"message\": \"No PDF links found\",\n                \"pdfs_found\": 0,\n                \"pdfs_downloaded\": 0\n            }\n        \n        # Download each PDF\n        downloaded_pdfs = []\n        failed_pdfs = []\n        \n        for pdf_info in pdf_links:\n            pdf_url = pdf_info[\"url\"]\n            try:\n                # Download the PDF\n                pdf_path = download_pdf(pdf_url, output_folder)\n                \n                # Process the PDF if download was successful\n                if pdf_path and os.path.exists(pdf_path):\n                    # Generate JSON output filename\n                    pdf_filename = os.path.basename(pdf_path)\n                    json_filename = f\"{os.path.splitext(pdf_filename)[0]}_processed.json\"\n                    json_path = os.path.join(output_folder, json_filename)\n                    \n                    # Process PDF to JSON if module is available\n                    if structify_module:\n                        try:\n                            structify_module.process_all_files(\n                                root_directory=output_folder,\n                                output_file=json_path,\n                                file_filter=lambda f: f == pdf_path\n                            )\n                            downloaded_pdfs.append({\n                                \"url\": pdf_url,\n                                \"file_path\": pdf_path,\n                                \"json_path\": json_path,\n                                \"title\": pdf_info.get(\"title\", \"\")\n                            })\n                        except Exception as e:\n                            logger.error(f\"Error processing PDF to JSON: {e}\")\n                            downloaded_pdfs.append({\n                                \"url\": pdf_url,\n                                \"file_path\": pdf_path,\n                                \"title\": pdf_info.get(\"title\", \"\")\n                            })\n                    else:\n                        downloaded_pdfs.append({\n                            \"url\": pdf_url,\n                            \"file_path\": pdf_path,\n                            \"title\": pdf_info.get(\"title\", \"\")\n                        })\n            except Exception as e:\n                logger.error(f\"Error downloading PDF from {pdf_url}: {e}\")\n                failed_pdfs.append({\n                    \"url\": pdf_url,\n                    \"error\": str(e),\n                    \"title\": pdf_info.get(\"title\", \"\")\n                })\n        \n        return {\n            \"status\": \"completed\",\n            \"url\": url,\n            \"pdfs_found\": len(pdf_links),\n            \"pdfs_downloaded\": len(downloaded_pdfs),\n            \"pdfs_failed\": len(failed_pdfs),\n            \"downloaded_pdfs\": downloaded_pdfs,\n            \"failed_pdfs\": failed_pdfs,\n            \"output_folder\": output_folder\n        }\n    \n    except Exception as e:\n        logger.error(f\"Error scraping PDFs from {url}: {e}\")\n        return {\n            \"status\": \"error\",\n            \"url\": url,\n            \"error\": str(e)\n        }\ndef fetch_pdf_links(url: str) -> List[Dict[str, str]]:\n    \"\"\"\n    Extract PDF links from a webpage.\n    \n    Args:\n        url: URL of the webpage to scrape\n        \n    Returns:\n        List of dictionaries containing PDF URLs and titles\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=30)\n        response.raise_for_status()\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n        pdf_links = []\n        \n        # Find all links that point to PDFs\n        for link in soup.find_all('a', href=True):\n            href = link['href']\n            if href.lower().endswith('.pdf') or 'pdf' in href.lower():\n                # Make absolute URL\n                pdf_url = urljoin(url, href)\n                \n                # Get link text or title\n                title = link.get_text(strip=True) or link.get('title', '') or os.path.basename(href)\n                \n                pdf_links.append({\n                    'url': pdf_url,\n                    'title': title\n                })\n        \n        return pdf_links\n        \n    except Exception as e:\n        logger.error(f\"Error fetching PDF links from {url}: {e}\")\n        return []\n\n# Add this function before process_url_with_settings\ndef process_url(url: str, setting: str, keyword: str = \"\", output_folder: str = DEFAULT_OUTPUT_FOLDER) -> Dict[str, Any]:\n    \"\"\"\n    Process a URL based on the specified setting.\n    \n    Args:\n        url (str): The URL to process\n        setting (str): One of 'full', 'metadata', 'title', 'keyword', 'pdf'\n        keyword (str): Optional keyword for keyword search mode\n        output_folder (str): Directory where outputs should be saved\n        \n    Returns:\n        Dict[str, Any]: Results of the processing\n    \"\"\"\n    # Ensure output folder exists\n    os.makedirs(output_folder, exist_ok=True)\n    \n    try:\n        # If web_scraper module is available, use it\n        if web_scraper_available and hasattr(web_scraper, 'process_url'):\n            return web_scraper.process_url(url, setting, keyword, output_folder)\n        \n        # Otherwise, provide a basic implementation\n        import requests\n        from bs4 import BeautifulSoup\n        \n        result = {\"url\": url, \"setting\": setting}\n        \n        # Download the page\n        response = requests.get(url, timeout=30)\n        response.raise_for_status()\n        \n        if setting == 'title':\n            # Extract just the title\n            soup = BeautifulSoup(response.text, 'html.parser')\n            title = soup.find('title')\n            result['title'] = title.text.strip() if title else 'No title found'\n            \n        elif setting == 'metadata':\n            # Extract metadata\n            soup = BeautifulSoup(response.text, 'html.parser')\n            metadata = {}\n            \n            # Get title\n            title = soup.find('title')\n            metadata['title'] = title.text.strip() if title else ''\n            \n            # Get meta tags\n            for meta in soup.find_all('meta'):\n                name = meta.get('name') or meta.get('property', '')\n                content = meta.get('content', '')\n                if name and content:\n                    metadata[name] = content\n            \n            result['metadata'] = metadata\n            \n        elif setting == 'keyword' and keyword:\n            # Search for keyword\n            soup = BeautifulSoup(response.text, 'html.parser')\n            text = soup.get_text()\n            occurrences = text.lower().count(keyword.lower())\n            result['keyword'] = keyword\n            result['occurrences'] = occurrences\n            result['found'] = occurrences > 0\n            \n        elif setting == 'pdf':\n            # Find PDF links\n            soup = BeautifulSoup(response.text, 'html.parser')\n            pdf_links = []\n            for link in soup.find_all('a', href=True):\n                href = link['href']\n                if href.lower().endswith('.pdf'):\n                    # Make absolute URL\n                    from urllib.parse import urljoin\n                    pdf_url = urljoin(url, href)\n                    pdf_links.append(pdf_url)\n            result['pdf_links'] = pdf_links\n            result['pdf_count'] = len(pdf_links)\n            \n        else:  # 'full' or default\n            # Save full content\n            output_file = os.path.join(output_folder, f\"scraped_{int(time.time())}.html\")\n            with open(output_file, 'w', encoding='utf-8') as f:\n                f.write(response.text)\n            result['output_file'] = output_file\n            result['content_length'] = len(response.text)\n        \n        result['status'] = 'success'\n        return result\n        \n    except Exception as e:\n        logger.error(f\"Error processing URL {url}: {e}\")\n        return {\"error\": str(e), \"url\": url, \"status\": \"error\"}        \ndef process_url_with_settings(url, setting, keyword, output_folder):\n    \"\"\"\n    Process a URL based on the specified setting, using the imported web_scraper functions.\n    \n    Args:\n        url: URL to process\n        setting: Processing setting ('full', 'metadata', 'title', 'keyword', 'pdf')\n        keyword: Optional keyword for keyword search\n        output_folder: Output directory for results\n        \n    Returns:\n        Processing result dictionary\n    \"\"\"\n    # Ensure output folder exists\n    os.makedirs(output_folder, exist_ok=True)\n    \n    if web_scraper_available:\n        # If web_scraper is available, use its process_url function\n        return web_scraper.process_url(url, setting, keyword, output_folder)\n    else:\n        # Fallback implementation if web_scraper is not available\n        if setting.lower() == \"pdf\":\n            try:\n                # Download the PDF file\n                pdf_file = download_pdf(url, save_path=output_folder)\n                \n                # Get just the filename without the path\n                pdf_filename = os.path.basename(pdf_file)\n                output_json_name = os.path.splitext(pdf_filename)[0] + \"_processed\"\n                \n                # Create a unique JSON output filename\n                json_output = get_output_filepath(output_json_name, user_defined_dir=output_folder)\n                \n                # Process the downloaded PDF using Structify (claude.py)\n                if structify_module:\n                    single_result = structify_module.process_all_files(\n                        root_directory=os.path.dirname(pdf_file),\n                        output_file=json_output,\n                        file_filter=lambda f: f == pdf_file  # Only process our specific PDF file\n                    )\n                \n                return {\n                    \"status\": \"PDF downloaded and processed\",\n                    \"url\": url,\n                    \"pdf_file\": pdf_file,\n                    \"json_file\": json_output,\n                    \"output_folder\": output_folder\n                }\n            except Exception as e:\n                return {\n                    \"status\": \"error\",\n                    \"url\": url,\n                    \"error\": str(e)\n                }\n        else:\n            # For all other settings, use the process_url function (placeholder if web_scraper not available)\n            return process_url(url, setting, keyword, output_folder)\n\n\ndef emit_pdf_download_progress(task_id, url, progress, status, file_path=None, error=None, details=None):\n    \"\"\"Emit PDF download progress update\"\"\"\n    try:\n        payload = {\n            'task_id': task_id,\n            'url': url,\n            'progress': progress,\n            'status': status,\n            'file_path': file_path,\n            'error': error,\n            'details': details or {},\n            'timestamp': time.time()\n        }\n        \n        emit('pdf_download_progress', payload, broadcast=True)\n        logger.debug(f\"Emitted PDF download progress for task {task_id}: {status}\")\n        \n    except Exception as e:\n        logger.error(f\"Error emitting PDF download progress: {str(e)}\")\n\n# Socket.IO events for web scraper\ndef emit_scraping_progress(task_id, progress, current_url=None, pages_scraped=0, total_pages=0):\n    \"\"\"Emit scraping progress update\"\"\"\n    try:\n        payload = {\n            'task_id': task_id,\n            'progress': progress,\n            'status': 'scraping',\n            'current_url': current_url,\n            'pages_scraped': pages_scraped,\n            'total_pages': total_pages,\n            'timestamp': time.time()\n        }\n        \n        emit('scraping_progress', payload, broadcast=True)\n        logger.debug(f\"Emitted scraping progress for task {task_id}: {progress}%\")\n        \n    except Exception as e:\n        logger.error(f\"Error emitting scraping progress: {str(e)}\")\n\n\ndef emit_scraping_completed(task_id, result_data=None, stats=None):\n    \"\"\"Emit scraping completion event\"\"\"\n    try:\n        payload = {\n            'task_id': task_id,\n            'status': 'completed',\n            'result_data': result_data,\n            'stats': stats or {},\n            'timestamp': time.time()\n        }\n        \n        emit('scraping_completed', payload, broadcast=True)\n        logger.info(f\"Emitted scraping completion for task {task_id}\")\n        \n    except Exception as e:\n        logger.error(f\"Error emitting scraping completion: {str(e)}\")\n\n\ndef emit_scraping_error(task_id, error_message, current_url=None):\n    \"\"\"Emit scraping error event\"\"\"\n    try:\n        payload = {\n            'task_id': task_id,\n            'status': 'error',\n            'error': error_message,\n            'current_url': current_url,\n            'timestamp': time.time()\n        }\n        \n        emit('scraping_error', payload, broadcast=True)\n        logger.error(f\"Emitted scraping error for task {task_id}: {error_message}\")\n        \n    except Exception as e:\n        logger.error(f\"Error emitting scraping error: {str(e)}\")","source":"/workspace/modules/blueprints/features/web_scraper.py","title":"web_scraper.py","language":"en"},{"content":"# Feature blueprints","source":"/workspace/modules/blueprints/features/__init__.py","title":"__init__.py","language":"en"}]}}